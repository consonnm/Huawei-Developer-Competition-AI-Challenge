{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3520b141-ff03-4512-bbae-ba10778ed270",
   "metadata": {},
   "source": [
    "# 代码准备\n",
    "\n",
    "首先准备baseline微调代码与预训练模型，**注意**，在下面所有环节涉及到`/cache`目录，都是为了节约工作目录空间。`/cache`目录会在每次重启后清空，若想要持久化存储，请首先对notebook扩容，具体方式可以参考[文档](https://support.huaweicloud.com/devtool-modelarts/modelarts_30_0040.html)，并将`/cache`路径改写为`/home/ma-user/work`。\n",
    "\n",
    "下载代码与预训练模型大约需要几分钟时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fb5b78-4c6b-4a90-a7e1-0cfa37d8aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载代码的本地路径\n",
    "chatglm2_6b_lora_path = \"/home/ma-user/work/chatglm2_6b_lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499da0c0-c667-4cbb-81bd-a33293613efc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'moxing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35792\\656332112.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 下载微调代码\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmoxing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"obs://dtse-models-guiyang1/competition/glm2/chatglm2_6b_lora/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchatglm2_6b_lora_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'moxing'"
     ]
    }
   ],
   "source": [
    "# 下载微调代码\n",
    "\n",
    "import moxing as mox\n",
    "\n",
    "mox.file.copy_parallel(\"obs://dtse-models-guiyang1/competition/glm2/chatglm2_6b_lora/\", chatglm2_6b_lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b055ef1f-7be3-43f8-bb3f-b2de40d45cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载预训练模型\n",
    "\n",
    "import moxing as mox\n",
    "\n",
    "mox.file.copy(\"obs://llm-mindspore-ei/wxp/chatglm2-6b/glm2_6b_ms.ckpt\", \"/home/ma-user/work/glm2_6b.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9c374-b232-4f18-9c34-a45afd427efb",
   "metadata": {},
   "source": [
    "安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd4875b-c7bd-46c9-be77-fe5a4f4a31bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Requirement already satisfied: numpy in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.97 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 2)) (0.1.99)\n",
      "Requirement already satisfied: ftfy>=6.1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 3)) (6.1.1)\n",
      "Requirement already satisfied: regex>=2022.10.31 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 4)) (2023.10.3)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 5)) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 6)) (6.0.1)\n",
      "Requirement already satisfied: jieba>=0.42.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 7)) (0.42.1)\n",
      "Requirement already satisfied: rouge_chinese>=1.0.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 8)) (1.0.3)\n",
      "Requirement already satisfied: nltk>=2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 9)) (3.8.1)\n",
      "Collecting mindpet==1.0.2\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/mindpet/1.0.2/mindpet-1.0.2-py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydantic==1.10.11 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 11)) (1.10.11)\n",
      "Requirement already satisfied: mdtex2html in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: gradio in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (3.50.2)\n",
      "Requirement already satisfied: opencv-python-headless in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 14)) (4.8.1.78)\n",
      "Requirement already satisfied: click in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindpet==1.0.2->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 10)) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pydantic==1.10.11->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 11)) (4.8.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from ftfy>=6.1.1->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 3)) (0.2.9)\n",
      "Requirement already satisfied: joblib in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from nltk>=2.0->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: six in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from rouge_chinese>=1.0.3->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: gradio-client==0.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.6.1)\n",
      "Requirement already satisfied: ffmpy in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.18.0)\n",
      "Requirement already satisfied: pydub in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.0.6)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (3.5.1)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (3.9.10)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2.1.3)\n",
      "Requirement already satisfied: requests~=2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2.27.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (3.1.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.24.0.post1)\n",
      "Requirement already satisfied: httpx in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.25.1)\n",
      "Requirement already satisfied: fastapi in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.104.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (6.1.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (1.2.5)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (11.0.3)\n",
      "Requirement already satisfied: packaging in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (23.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (23.2.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2.10.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (5.1.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (10.0.1)\n",
      "Requirement already satisfied: fsspec in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio-client==0.6.1->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2023.10.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (4.19.2)\n",
      "Requirement already satisfied: toolz in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (3.13.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (3.17.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (23.1.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.12.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.30.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (1.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (4.44.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (3.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests~=2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests~=2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests~=2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests~=2.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (2.10)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from uvicorn>=0.14.0->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.14.0)\n",
      "Requirement already satisfied: markdown in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mdtex2html->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 12)) (3.5.1)\n",
      "Requirement already satisfied: latex2mathml in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mdtex2html->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 12)) (3.76.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from fastapi->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from fastapi->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (0.27.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (1.1.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: httpcore in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from httpx->gradio->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 13)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from markdown->mdtex2html->-r /home/ma-user/work/chatglm2_6b_lora/mindformers/requirements.txt (line 12)) (6.8.0)\n",
      "Installing collected packages: mindpet\n",
      "  Attempting uninstall: mindpet\n",
      "    Found existing installation: mindpet 1.0.1\n",
      "    Uninstalling mindpet-1.0.1:\n",
      "      Successfully uninstalled mindpet-1.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mindformers 0.8.0 requires mindpet==1.0.1, but you have mindpet 1.0.2 which is incompatible.\u001b[0m\n",
      "Successfully installed mindpet-1.0.2\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r $chatglm2_6b_lora_path/mindformers/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17dcc3-3b68-4a75-87b6-54052bf85aa0",
   "metadata": {},
   "source": [
    "进入工作目录，准备训练配置文件与环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3dfedbe-06c8-4d50-ab7a-b088021afe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd $chatglm2_6b_lora_path/mindformers/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b70ab-039b-47c8-8c00-53405e13b4b6",
   "metadata": {},
   "source": [
    "拷贝配置文件、驱动脚本等文件至mf_standalone，方便修改为个性化版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b2855f-358a-4be9-93df-7ecb84ee9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./mf_standalone\n",
    "!mkdir ./mf_standalone\n",
    "!cp ../*.py ./mf_standalone\n",
    "!cp -r ../configs ./mf_standalone\n",
    "!cp -r ../mindformers ./mf_standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e95c41-3923-4bad-8251-9978fc698109",
   "metadata": {},
   "source": [
    "# 微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68ca03-4c4f-4cf8-8002-7463ad595fae",
   "metadata": {},
   "source": [
    "进入工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b756bd9-9f5b-40a2-97ea-a516ae054c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone\n"
     ]
    }
   ],
   "source": [
    "%cd mf_standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f69478-80cb-4706-84d6-daa2169ba3d3",
   "metadata": {},
   "source": [
    "修改微调配置文件，我们使用的配置文件路径为：`$chatglm2_6b_lora_path/mindformers/scripts/mf_standalone/configs/glm2/run_glm2_6b_lora_910b.yaml`\n",
    "\n",
    "主要需要修改的字段：\n",
    "\n",
    "```yaml\n",
    "output_dir: '/home/ma-user/work/output'\n",
    "load_checkpoint: '/home/ma-user/work/glm2_6b.ckpt'  # 修改为下载的预训练模型路径\n",
    "# 必须，配置为obs路径，微调结果会直接上传至配置的obs桶中\n",
    "remote_save_url: \"obs://ai-l/competition/glm2/output-test/\"\n",
    "\n",
    "train_dataset: &train_dataset\n",
    "  data_loader:\n",
    "    dataset_dir: \"/home/ma-user/work/chatglm2_6b_lora/train.jsonl\"  # 修改为实际训练数据路径\n",
    "    origin_columns: [\"input\", \"output\"]  # 根据实际训练数据使用的字段进行修改\n",
    "  tokenizer:\n",
    "    type: ChatGLM2Tokenizer\n",
    "    vocab_file: \"/home/ma-user/work/chatglm2_6b_lora/tokenizer.model\"  # 实际使用的tokenizer.model路径，我们在微调代码$chatglm2_6b_lora_path中预置了一个tokenizer.model，如需使用自定义tokenizer model也可配置相关参数\n",
    "\n",
    "# 如需训练同时验证，需同步修改验证数据相关字段\n",
    "eval_dataset: &eval_dataset\n",
    "```\n",
    "\n",
    "修改后，运行微调命令，出现` .........Training Over!.............`日志即代表微调成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a085cba9-2fcb-4742-b521-872b882d9205",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \\L\n",
      "  if not dirpath.find(\"AppData\\Local\\Temp\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \\B\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(__version__) >= LooseVersion(\"1.17.0\"):\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n",
      "/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/mindformers/wrapper/wrapper.py:52: DeprecationWarning: invalid escape sequence \\l\n",
      "  \"\"\"TrainOneStep For MindFormer.\n",
      "/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/mindformers/wrapper/wrapper.py:154: DeprecationWarning: invalid escape sequence \\*\n",
      "  \"\"\"\n",
      "2023-11-17 09:39:15,158 - mindformers[run_mindformer.py:93] - INFO - .........Build context config..........\n",
      "2023-11-17 09:39:15,159 - mindformers[parallel_config.py:38] - INFO - initial moe_config from dict: {'expert_num': 1, 'capacity_factor': 1.05, 'aux_loss_factor': 0.05, 'num_experts_chosen': 1}\n",
      "2023-11-17 09:39:15,159 - mindformers[parallel_config.py:44] - INFO - initial recompute_config from dict: {'recompute': True, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}\n",
      "2023-11-17 09:39:15,159 - mindformers[parallel_config.py:50] - INFO - initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'expert_parallel': 1, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}\n",
      "2023-11-17 09:39:15,160 - mindformers[run_mindformer.py:100] - INFO - context config is: [ParallelConfig]\n",
      "_recompute:[ParallelConfig]\n",
      "_recompute:True\n",
      "_select_recompute:False\n",
      "_parallel_optimizer_comm_recompute:False\n",
      "_mp_comm_recompute:True\n",
      "_recompute_slice_activation:True\n",
      "\n",
      "select_recompute:False\n",
      "use_seq_parallel:False\n",
      "_gradient_aggregation_group:4\n",
      "_embed_dp_mp_config:[ParallelConfig]\n",
      "_dp_mp_config:[ParallelConfig]\n",
      "_data_parallel:8\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_vocab_emb_dp:True\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_pp_config:[ParallelConfig]\n",
      "_pipeline_stage:1\n",
      "_micro_batch_num:1\n",
      "\n",
      "_moe_config:[ParallelConfig]\n",
      "_dpmp:[ParallelConfig]\n",
      "_data_parallel:8\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_expert_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "\n",
      "2023-11-17 09:39:15,160 - mindformers[run_mindformer.py:101] - INFO - moe config is: <mindformers.modules.transformer.moe.MoEConfig object at 0xfffe9b87b2b0>\n",
      "2023-11-17 09:39:15,161 - mindformers[run_mindformer.py:117] - INFO - remote_save_url is obs://lxy-guiyang1-output/glm2-2.2-output, the output file will be uploaded to here.\n",
      "2023-11-17 09:39:15,161 - mindformers[base_trainer.py:78] - INFO - Now Running Task is: text_generation, Model is: glm2_6b_lora\n",
      "2023-11-17 09:39:15,161 - mindformers[base_trainer.py:215] - INFO - The current parallel mode is stand_alone, batch size per card will not be changed: batch_size_per_card = 8\n",
      "2023-11-17 09:39:15,162 - mindformers[base_trainer.py:218] - INFO - global_batch_size = batch_size_per_card * device_num = 8 * 1 = 8\n",
      "2023-11-17 09:39:15,162 - mindformers[base_trainer.py:228] - INFO - parallel_config will be change to default config: [ParallelConfig]\n",
      "_recompute:[ParallelConfig]\n",
      "_recompute:True\n",
      "_select_recompute:False\n",
      "_parallel_optimizer_comm_recompute:False\n",
      "_mp_comm_recompute:True\n",
      "_recompute_slice_activation:True\n",
      "\n",
      "select_recompute:False\n",
      "use_seq_parallel:False\n",
      "_gradient_aggregation_group:4\n",
      "_embed_dp_mp_config:[ParallelConfig]\n",
      "_dp_mp_config:[ParallelConfig]\n",
      "_data_parallel:1\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_vocab_emb_dp:True\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_pp_config:[ParallelConfig]\n",
      "_pipeline_stage:1\n",
      "_micro_batch_num:1\n",
      "\n",
      "_moe_config:[ParallelConfig]\n",
      "_dpmp:[ParallelConfig]\n",
      "_data_parallel:1\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_expert_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      ".\n",
      "2023-11-17 09:39:15,162 - mindformers[base_trainer.py:553] - INFO - .........Build Dataset For Train..........\n",
      "2023-11-17 09:39:15,162 - mindformers[base_trainer.py:286] - INFO - .........Build Dataset From Config..........\n",
      "2023-11-17 09:39:15,163 - mindformers[keyword_gen_dataset.py:63] - INFO - Now Create Keyword Generation Dataset.\n",
      "2023-11-17 09:39:15,644 - mindformers[adgen_dataloader.py:145] - INFO - Loading 29999 data success.\n",
      "2023-11-17 09:39:15,644 - mindformers[adgen_dataloader.py:82] - INFO - [DATASET] shuffle status is True, phase is train.\n",
      "2023-11-17 09:39:15,658 - mindformers[keyword_gen_dataset.py:103] - INFO - Start tokenize on the dataset using tokenizer: {'type': 'ChatGLM2Tokenizer', 'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}\n",
      "2023-11-17 09:39:15,666 - mindformers[utils.py:144] - INFO - Will be Training epochs:1, sink_size:4\n",
      "2023-11-17 09:39:15,666 - mindformers[utils.py:146] - INFO - Create training dataset finish, dataset size:3750\n",
      "2023-11-17 09:39:15,670 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "[WARNING] DEVICE(17499,ffff9167b0b0,python):2023-11-17-09:39:18.882.036 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_memory_adapter.cc:103] Initialize] Reserved memory size for other components(1073741824) is less than recommend size(4006978304), It may lead to Out Of Memory in HCCL or other components, Please double check context key 'variable_memory_max_size'/'max_device_memory'\n",
      "[WARNING] DEVICE(17499,ffff9167b0b0,python):2023-11-17-09:39:19.020.687 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_runtime_manager.cc:46] GetAscendRuntime] No ascend runtime creator for AscendVM with device id 0\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "[WARNING] DEVICE(17499,ffff9167b0b0,python):2023-11-17-09:39:39.086.157 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_graph_executor.cc:1258] RunInitGraph] Can not find init_subgraph.kernel_graph_0 sub graph, don't need data init subgraph in INFER mode.\n",
      "2023-11-17 09:39:39,175 - mindformers[base_trainer.py:566] - INFO - .........Build Net For Train..........\n",
      "2023-11-17 09:39:39,177 - mindformers[base_trainer.py:307] - INFO - .........Build Network From Config..........\n",
      "2023-11-17 09:40:58,659 - mindformers[base_model.py:121] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:12.695.990 [mindspore/train/serialization.py:172] The type of transformer.embedding.embedding_table:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:14.908.719 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.0.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:15.287.32 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.0.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:15.560.702 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.1.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:15.671.794 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.1.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:16.192.725 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.2.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:16.303.951 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.2.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:16.827.356 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.3.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:16.938.824 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.3.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:17.451.329 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.4.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:17.563.799 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.4.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:18.109.150 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.5.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:18.219.563 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.5.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:18.747.241 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.6.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:18.858.400 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.6.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:19.374.512 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.7.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:19.496.845 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.7.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:20.162.293 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.8.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:20.277.416 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.8.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:21.238.640 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.9.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:21.456.377 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.9.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:22.388.552 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.10.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:22.471.797 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.10.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:22.859.861 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.11.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:22.942.728 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.11.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:23.329.237 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.12.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:23.412.273 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.12.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:24.716.06 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.13.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:24.275.663 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.13.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:25.238.458 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.14.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:25.427.445 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.14.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:26.310.475 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.15.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:26.537.232 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.15.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:27.429.433 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.16.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:27.619.534 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.16.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:28.306.193 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.17.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:28.506.311 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.17.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:29.403.616 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.18.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:29.601.146 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.18.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:30.388.080 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.19.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:30.498.997 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.19.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:31.200.51 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.20.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:31.130.974 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.20.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:31.661.442 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.21.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:31.780.399 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.21.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:32.296.747 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.22.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:32.416.006 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.22.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:32.930.512 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.23.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:33.495.18 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.23.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:33.592.602 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.24.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:33.708.586 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.24.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:34.232.637 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.25.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:34.353.184 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.25.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:34.864.645 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.26.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:34.979.619 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.26.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:35.495.893 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.27.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:35.606.112 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.27.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:36.127.923 [mindspore/train/serialization.py:172] The type of transformer.encoder.final_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.319.689 [mindspore/train/serialization.py:1317] For 'load_param_into_net', 56 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.319.916 [mindspore/train/serialization.py:1322] transformer.encoder.layers.0.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.319.997 [mindspore/train/serialization.py:1322] transformer.encoder.layers.0.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.060 [mindspore/train/serialization.py:1322] transformer.encoder.layers.1.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.121 [mindspore/train/serialization.py:1322] transformer.encoder.layers.1.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.179 [mindspore/train/serialization.py:1322] transformer.encoder.layers.2.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.235 [mindspore/train/serialization.py:1322] transformer.encoder.layers.2.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.291 [mindspore/train/serialization.py:1322] transformer.encoder.layers.3.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.347 [mindspore/train/serialization.py:1322] transformer.encoder.layers.3.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.401 [mindspore/train/serialization.py:1322] transformer.encoder.layers.4.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.454 [mindspore/train/serialization.py:1322] transformer.encoder.layers.4.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.508 [mindspore/train/serialization.py:1322] transformer.encoder.layers.5.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.562 [mindspore/train/serialization.py:1322] transformer.encoder.layers.5.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.614 [mindspore/train/serialization.py:1322] transformer.encoder.layers.6.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.666 [mindspore/train/serialization.py:1322] transformer.encoder.layers.6.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.719 [mindspore/train/serialization.py:1322] transformer.encoder.layers.7.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.771 [mindspore/train/serialization.py:1322] transformer.encoder.layers.7.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.823 [mindspore/train/serialization.py:1322] transformer.encoder.layers.8.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.875 [mindspore/train/serialization.py:1322] transformer.encoder.layers.8.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.928 [mindspore/train/serialization.py:1322] transformer.encoder.layers.9.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.320.978 [mindspore/train/serialization.py:1322] transformer.encoder.layers.9.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.030 [mindspore/train/serialization.py:1322] transformer.encoder.layers.10.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.082 [mindspore/train/serialization.py:1322] transformer.encoder.layers.10.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.134 [mindspore/train/serialization.py:1322] transformer.encoder.layers.11.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.185 [mindspore/train/serialization.py:1322] transformer.encoder.layers.11.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.237 [mindspore/train/serialization.py:1322] transformer.encoder.layers.12.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.289 [mindspore/train/serialization.py:1322] transformer.encoder.layers.12.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.339 [mindspore/train/serialization.py:1322] transformer.encoder.layers.13.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.390 [mindspore/train/serialization.py:1322] transformer.encoder.layers.13.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.441 [mindspore/train/serialization.py:1322] transformer.encoder.layers.14.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.492 [mindspore/train/serialization.py:1322] transformer.encoder.layers.14.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.544 [mindspore/train/serialization.py:1322] transformer.encoder.layers.15.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.596 [mindspore/train/serialization.py:1322] transformer.encoder.layers.15.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.648 [mindspore/train/serialization.py:1322] transformer.encoder.layers.16.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.699 [mindspore/train/serialization.py:1322] transformer.encoder.layers.16.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.751 [mindspore/train/serialization.py:1322] transformer.encoder.layers.17.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.802 [mindspore/train/serialization.py:1322] transformer.encoder.layers.17.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.853 [mindspore/train/serialization.py:1322] transformer.encoder.layers.18.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.902 [mindspore/train/serialization.py:1322] transformer.encoder.layers.18.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.321.953 [mindspore/train/serialization.py:1322] transformer.encoder.layers.19.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.004 [mindspore/train/serialization.py:1322] transformer.encoder.layers.19.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.054 [mindspore/train/serialization.py:1322] transformer.encoder.layers.20.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.104 [mindspore/train/serialization.py:1322] transformer.encoder.layers.20.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.156 [mindspore/train/serialization.py:1322] transformer.encoder.layers.21.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.207 [mindspore/train/serialization.py:1322] transformer.encoder.layers.21.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.270 [mindspore/train/serialization.py:1322] transformer.encoder.layers.22.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.326 [mindspore/train/serialization.py:1322] transformer.encoder.layers.22.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.379 [mindspore/train/serialization.py:1322] transformer.encoder.layers.23.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.430 [mindspore/train/serialization.py:1322] transformer.encoder.layers.23.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.482 [mindspore/train/serialization.py:1322] transformer.encoder.layers.24.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.533 [mindspore/train/serialization.py:1322] transformer.encoder.layers.24.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.583 [mindspore/train/serialization.py:1322] transformer.encoder.layers.25.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.635 [mindspore/train/serialization.py:1322] transformer.encoder.layers.25.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.686 [mindspore/train/serialization.py:1322] transformer.encoder.layers.26.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.736 [mindspore/train/serialization.py:1322] transformer.encoder.layers.26.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.786 [mindspore/train/serialization.py:1322] transformer.encoder.layers.27.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:37.322.838 [mindspore/train/serialization.py:1322] transformer.encoder.layers.27.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "2023-11-17 09:42:37,322 - mindformers[base_model.py:116] - INFO - weights in /cache/glm2_6b.ckpt are loaded\n",
      "[INFO] 2023-11-17 09:42:37,326 [17499] [SDK] : Start to freeze model for delta, mode: lora, include list: None, exclude list: None\n",
      "[INFO] 2023-11-17 09:42:37,327 [17499] [SDK] : Start to freeze model, include list: ['*'], exclude list: ['*mindpet_delta_lora*']\n",
      "[INFO] 2023-11-17 09:42:37,333 [17499] [SDK] : End to freeze model.\n",
      "[INFO] 2023-11-17 09:42:37,333 [17499] [SDK] : End to freeze model for delta.\n",
      "2023-11-17 09:42:37,344 - mindformers[base_trainer.py:464] - INFO - Network Parameters: 1 M.\n",
      "2023-11-17 09:42:37,344 - mindformers[base_trainer.py:593] - INFO - .........Build Optimizer For Train..........\n",
      "2023-11-17 09:42:37,345 - mindformers[base_trainer.py:360] - INFO - .........Build Optimizer From Config..........\n",
      "2023-11-17 09:42:37,345 - mindformers[base_trainer.py:393] - INFO - .........Build LR Schedule From Config..........\n",
      "2023-11-17 09:42:37,349 - mindformers[optimizer_grouped_parameters.py:74] - WARNING - dynamic_lr_schedule will be reset and invalid when layer_scale is False.\n",
      "2023-11-17 09:42:37,352 - mindformers[optimizer_grouped_parameters.py:113] - INFO - Param groups = {\n",
      "  \"decay\": {\n",
      "    \"weight_decay\": 0.1,\n",
      "    \"params\": [\n",
      "      \"transformer.encoder.layers.0.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.0.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.1.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.1.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.2.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.2.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.3.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.3.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.4.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.4.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.5.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.5.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.6.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.6.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.7.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.7.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.8.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.8.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.9.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.9.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.10.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.10.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.11.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.11.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.12.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.12.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.13.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.13.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.14.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.14.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.15.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.15.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.16.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.16.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.17.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.17.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.18.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.18.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.19.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.19.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.20.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.20.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.21.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.21.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.22.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.22.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.23.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.23.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.24.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.24.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.25.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.25.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.26.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.26.self_attention.query_key_value.mindpet_delta_lora_b\",\n",
      "      \"transformer.encoder.layers.27.self_attention.query_key_value.mindpet_delta_lora_a\",\n",
      "      \"transformer.encoder.layers.27.self_attention.query_key_value.mindpet_delta_lora_b\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "2023-11-17 09:42:38,798 - mindformers[base_trainer.py:599] - INFO - .........Build Running Wrapper From Config For Train..........\n",
      "2023-11-17 09:42:38,798 - mindformers[base_trainer.py:430] - INFO - .........Build Model Wrapper for Train From Config..........\n",
      "2023-11-17 09:42:38,807 - mindformers[base_trainer.py:606] - INFO - .........Build Callbacks For Train..........\n",
      "2023-11-17 09:42:38,807 - mindformers[base_trainer.py:439] - INFO - .........Build Callbacks for Train From Config..........\n",
      "2023-11-17 09:42:38,809 - mindformers[base_trainer.py:623] - INFO - .........Starting Init Train Model..........\n",
      "2023-11-17 09:42:38,810 - mindformers[base_trainer.py:658] - INFO - .........Starting Training Model..........\n",
      "{'auto_trans_ckpt': False,\n",
      " 'auto_tune': False,\n",
      " 'autotune_per_step': 10,\n",
      " 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),\n",
      "               OrderedDict([('type', 'CheckpointMointor'),\n",
      "                            ('prefix', 'glm2-6b-lora'),\n",
      "                            ('save_checkpoint_steps', 1000),\n",
      "                            ('keep_checkpoint_max', 1),\n",
      "                            ('integrated_save', False),\n",
      "                            ('async_save', False)]),\n",
      "               OrderedDict([('type', 'ObsMonitor'), ('keep_last', False)])],\n",
      " 'context': {'device_id': 0,\n",
      "             'device_target': 'Ascend',\n",
      "             'enable_graph_kernel': False,\n",
      "             'graph_kernel_flags': '--disable_expand_ops=Softmax,Dropout '\n",
      "                                   '--enable_parallel_fusion=true '\n",
      "                                   '--reduce_fuse_depth=8 '\n",
      "                                   '--enable_auto_tensor_inplace=true',\n",
      "             'max_call_depth': 10000,\n",
      "             'save_graphs': False},\n",
      " 'data_size': 3750,\n",
      " 'device_num': 1,\n",
      " 'do_eval': False,\n",
      " 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor'),\n",
      "                                 ('keep_last', False)])],\n",
      " 'eval_dataset': {'auto_tune': False,\n",
      "                  'autotune_per_step': 10,\n",
      "                  'batch_size': 8,\n",
      "                  'data_loader': {'dataset_dir': '/path/to/AdvertiseGen/dev.json',\n",
      "                                  'origin_columns': ['content', 'summary'],\n",
      "                                  'phase': 'eval',\n",
      "                                  'shuffle': False,\n",
      "                                  'type': 'ADGenDataLoader',\n",
      "                                  'version': 2},\n",
      "                  'do_eval': True,\n",
      "                  'drop_remainder': True,\n",
      "                  'filepath_prefix': './autotune',\n",
      "                  'ignore_pad_token_for_loss': True,\n",
      "                  'input_columns': ['input_ids', 'labels'],\n",
      "                  'max_source_length': 256,\n",
      "                  'max_target_length': 256,\n",
      "                  'num_parallel_workers': 8,\n",
      "                  'numa_enable': False,\n",
      "                  'prefetch_size': 1,\n",
      "                  'profile': False,\n",
      "                  'python_multiprocessing': False,\n",
      "                  'repeat': 1,\n",
      "                  'seed': 0,\n",
      "                  'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                'vocab_file': '/path/to/tokenizer.model'}},\n",
      " 'eval_dataset_task': {'dataset_config': {'auto_tune': False,\n",
      "                                          'autotune_per_step': 10,\n",
      "                                          'batch_size': 8,\n",
      "                                          'data_loader': {'dataset_dir': '/path/to/AdvertiseGen/dev.json',\n",
      "                                                          'origin_columns': ['content',\n",
      "                                                                             'summary'],\n",
      "                                                          'phase': 'eval',\n",
      "                                                          'shuffle': False,\n",
      "                                                          'type': 'ADGenDataLoader',\n",
      "                                                          'version': 2},\n",
      "                                          'do_eval': True,\n",
      "                                          'drop_remainder': True,\n",
      "                                          'filepath_prefix': './autotune',\n",
      "                                          'ignore_pad_token_for_loss': True,\n",
      "                                          'input_columns': ['input_ids',\n",
      "                                                            'labels'],\n",
      "                                          'max_source_length': 256,\n",
      "                                          'max_target_length': 256,\n",
      "                                          'num_parallel_workers': 8,\n",
      "                                          'numa_enable': False,\n",
      "                                          'prefetch_size': 1,\n",
      "                                          'profile': False,\n",
      "                                          'python_multiprocessing': False,\n",
      "                                          'repeat': 1,\n",
      "                                          'seed': 0,\n",
      "                                          'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                                        'vocab_file': '/path/to/tokenizer.model'}},\n",
      "                       'type': 'KeyWordGenDataset'},\n",
      " 'eval_epoch_interval': -1,\n",
      " 'eval_step_interval': 500,\n",
      " 'filepath_prefix': './autotune',\n",
      " 'init_start_profile': True,\n",
      " 'layer_decay': 0.65,\n",
      " 'layer_scale': False,\n",
      " 'load_checkpoint': None,\n",
      " 'local_rank': 0,\n",
      " 'lr_scale': False,\n",
      " 'lr_scale_factor': 256,\n",
      " 'lr_schedule': {'learning_rate': 5e-05,\n",
      "                 'lr_end': 1e-06,\n",
      "                 'total_steps': 3748,\n",
      "                 'type': 'polynomial',\n",
      "                 'warmup_steps': 0},\n",
      " 'metric': {'type': 'ADGENMetric'},\n",
      " 'micro_batch_interleave_num': 1,\n",
      " 'model': {'arch': {'type': 'ChatGLM2ForConditionalGeneration'},\n",
      "           'model_config': {'add_bias_linear': False,\n",
      "                            'add_qkv_bias': True,\n",
      "                            'apply_query_key_layer_scaling': True,\n",
      "                            'apply_residual_connection_post_layernorm': False,\n",
      "                            'attention_dropout': 0.0,\n",
      "                            'attention_softmax_in_fp32': True,\n",
      "                            'batch_size': 1,\n",
      "                            'bias_dropout_fusion': True,\n",
      "                            'checkpoint_name_or_path': None,\n",
      "                            'compute_dtype': 'float16',\n",
      "                            'do_sample': True,\n",
      "                            'eos_token_id': 2,\n",
      "                            'ffn_hidden_size': 13696,\n",
      "                            'fp32_residual_connection': False,\n",
      "                            'hidden_dropout': 0.0,\n",
      "                            'hidden_size': 4096,\n",
      "                            'kv_channels': 128,\n",
      "                            'layernorm_compute_type': 'float32',\n",
      "                            'layernorm_epsilon': '1e-5',\n",
      "                            'max_decode_length': 256,\n",
      "                            'multi_query_attention': True,\n",
      "                            'multi_query_group_num': 2,\n",
      "                            'num_attention_heads': 32,\n",
      "                            'num_layers': 28,\n",
      "                            'pad_token_id': 0,\n",
      "                            'padded_vocab_size': 65024,\n",
      "                            'param_init_type': 'float16',\n",
      "                            'pet_config': {'lora_alpha': 32,\n",
      "                                           'lora_dropout': 0.1,\n",
      "                                           'lora_rank': 8,\n",
      "                                           'pet_type': 'lora',\n",
      "                                           'target_modules': '.*query_key_value*'},\n",
      "                            'post_layer_norm': True,\n",
      "                            'pre_seq_len': 'None',\n",
      "                            'prefix_projection': False,\n",
      "                            'quantization_bit': 0,\n",
      "                            'repetition_penalty': 1.0,\n",
      "                            'rmsnorm': True,\n",
      "                            'seq_length': 193,\n",
      "                            'top_k': 1,\n",
      "                            'top_p': 1,\n",
      "                            'type': 'ChatGLM2Config',\n",
      "                            'use_past': False}},\n",
      " 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0xfffe9b87b2b0>,\n",
      " 'only_save_strategy': False,\n",
      " 'optimizer': {'beta1': 0.9,\n",
      "               'beta2': 0.95,\n",
      "               'eps': 1e-08,\n",
      "               'type': 'FP32StateAdamWeightDecay',\n",
      "               'weight_decay': 0.1},\n",
      " 'output_dir': '/cache/output',\n",
      " 'parallel': {'enable_alltoall': False,\n",
      "              'enable_parallel_optimizer': True,\n",
      "              'full_batch': True,\n",
      "              'gradients_mean': False,\n",
      "              'loss_repeated_mean': True,\n",
      "              'parallel_mode': 1,\n",
      "              'search_mode': 'sharding_propagation',\n",
      "              'strategy_ckpt_config': {'only_trainable_params': False,\n",
      "                                       'save_file': './ckpt_strategy.ckpt'},\n",
      "              'strategy_ckpt_save_file': '/cache/ma-user-work/strategy/ckpt_strategy_rank_0.ckpt'},\n",
      " 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0xfffe9b87b280>,\n",
      " 'processor': {'return_tensors': 'ms',\n",
      "               'tokenizer': {'bos_token': '<sop>',\n",
      "                             'end_token': '</s>',\n",
      "                             'eos_token': '<eop>',\n",
      "                             'gmask_token': '[gMASK]',\n",
      "                             'mask_token': '[MASK]',\n",
      "                             'pad_token': '<pad>',\n",
      "                             'type': 'ChatGLM2Tokenizer',\n",
      "                             'unk_token': '<unk>'},\n",
      "               'type': 'GLMProcessor'},\n",
      " 'profile': False,\n",
      " 'profile_communication': True,\n",
      " 'profile_memory': True,\n",
      " 'profile_start_step': 1,\n",
      " 'profile_stop_step': 10,\n",
      " 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0xfffe9b87b610>,\n",
      " 'remote_save_url': 'obs://lxy-guiyang1-output/glm2-2.2-output',\n",
      " 'resume_training': False,\n",
      " 'run_mode': 'finetune',\n",
      " 'runner_config': {'batch_size': 8,\n",
      "                   'epochs': 937,\n",
      "                   'initial_epoch': 0,\n",
      "                   'origin_epochs': 1,\n",
      "                   'sink_mode': True,\n",
      "                   'sink_size': 4},\n",
      " 'runner_wrapper': {'scale_sense': DynamicLossScaleUpdateCell<>,\n",
      "                    'type': 'MFTrainOneStepCell',\n",
      "                    'use_clip_grad': True},\n",
      " 'seed': 0,\n",
      " 'train_dataset': {'auto_tune': False,\n",
      "                   'autotune_per_step': 10,\n",
      "                   'batch_size': 8,\n",
      "                   'data_loader': {'origin_columns': ['questions', 'answers'],\n",
      "                                   'phase': 'train',\n",
      "                                   'shuffle': True,\n",
      "                                   'type': 'ADGenDataLoader',\n",
      "                                   'version': 2},\n",
      "                   'device_num': 1,\n",
      "                   'do_eval': False,\n",
      "                   'drop_remainder': True,\n",
      "                   'filepath_prefix': './autotune',\n",
      "                   'ignore_pad_token_for_loss': True,\n",
      "                   'input_columns': ['input_ids', 'labels'],\n",
      "                   'max_source_length': 64,\n",
      "                   'max_target_length': 128,\n",
      "                   'num_parallel_workers': 8,\n",
      "                   'numa_enable': False,\n",
      "                   'prefetch_size': 1,\n",
      "                   'profile': False,\n",
      "                   'python_multiprocessing': False,\n",
      "                   'rank_id': 0,\n",
      "                   'repeat': 1,\n",
      "                   'seed': 0,\n",
      "                   'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                 'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}},\n",
      " 'train_dataset_task': {'dataset_config': {'auto_tune': False,\n",
      "                                           'autotune_per_step': 10,\n",
      "                                           'batch_size': 8,\n",
      "                                           'data_loader': {'origin_columns': ['questions',\n",
      "                                                                              'answers'],\n",
      "                                                           'phase': 'train',\n",
      "                                                           'shuffle': True,\n",
      "                                                           'type': 'ADGenDataLoader',\n",
      "                                                           'version': 2},\n",
      "                                           'device_num': 1,\n",
      "                                           'do_eval': False,\n",
      "                                           'drop_remainder': True,\n",
      "                                           'filepath_prefix': './autotune',\n",
      "                                           'ignore_pad_token_for_loss': True,\n",
      "                                           'input_columns': ['input_ids',\n",
      "                                                             'labels'],\n",
      "                                           'max_source_length': 64,\n",
      "                                           'max_target_length': 128,\n",
      "                                           'num_parallel_workers': 8,\n",
      "                                           'numa_enable': False,\n",
      "                                           'prefetch_size': 1,\n",
      "                                           'profile': False,\n",
      "                                           'python_multiprocessing': False,\n",
      "                                           'rank_id': 0,\n",
      "                                           'repeat': 1,\n",
      "                                           'seed': 0,\n",
      "                                           'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                                         'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}},\n",
      "                        'type': 'KeyWordGenDataset'},\n",
      " 'trainer': {'model_name': 'glm2_6b_lora',\n",
      "             'type': 'CausalLanguageModelingTrainer'},\n",
      " 'use_parallel': False}\n",
      "2023-11-17 09:42:38,818 - mindformers[base_trainer.py:661] - INFO - .........Model Compiling, Please Wait a Moment...........\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:38.818.660 [mindspore/train/model.py:1106] For MFLossMonitor callback, {'epoch_begin', 'epoch_end', 'step_begin', 'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:38.818.795 [mindspore/train/model.py:1106] For Local2ObsMonitor callback, {'epoch_end', 'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(17499:281473121235120,MainProcess):2023-11-17-09:42:38.819.124 [mindspore/train/model.py:651] In dataset_sink mode (dataset_size % sink_size) should equal to 0, it is suggested to pad/drop data or adjust sink_size. But got 'dataset_size': 3750, 'sink_size': 4.\n",
      "<string>:1: RuntimeWarning: divide by zero encountered in log\n",
      "<string>:1: RuntimeWarning: divide by zero encountered in log\n",
      "2023-11-17 09:47:01,113 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[    4/ 3750], loss: 3.336, per_step_time: 65571ms, lr: 4.9986927e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:01,123 - mindformers[callback.py:317] - INFO -    0.1% |                                                  | 0.12 samples/s/p  2 days, 20:13:51 }\n",
      "2023-11-17 09:47:04,094 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[    8/ 3750], loss: 5.045, per_step_time: 562ms, lr: 4.9934628e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:04,100 - mindformers[callback.py:317] - INFO -    0.2% |                                                  | 14.22 samples/s/p  0:35:04 }\n",
      "2023-11-17 09:47:06,356 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   12/ 3750], loss: 5.229, per_step_time: 556ms, lr: 4.9882332e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:06,356 - mindformers[callback.py:317] - INFO -    0.3% |                                                  | 14.38 samples/s/p  0:34:40 }\n",
      "2023-11-17 09:47:08,590 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   16/ 3750], loss: 4.221, per_step_time: 556ms, lr: 4.9830043e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:08,590 - mindformers[callback.py:317] - INFO -    0.4% |                                                  | 14.37 samples/s/p  0:34:39 }\n",
      "2023-11-17 09:47:10,823 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   20/ 3750], loss: 3.244, per_step_time: 556ms, lr: 4.9777744e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:10,823 - mindformers[callback.py:317] - INFO -    0.5% |                                                  | 14.37 samples/s/p  0:34:36 }\n",
      "2023-11-17 09:47:13,055 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   24/ 3750], loss: 3.739, per_step_time: 556ms, lr: 4.972545e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:13,056 - mindformers[callback.py:317] - INFO -    0.6% |                                                  | 14.37 samples/s/p  0:34:33 }\n",
      "2023-11-17 09:47:15,288 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   28/ 3750], loss: 4.256, per_step_time: 556ms, lr: 4.9673155e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:15,289 - mindformers[callback.py:317] - INFO -    0.7% |                                                  | 14.37 samples/s/p  0:34:32 }\n",
      "2023-11-17 09:47:17,522 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   32/ 3750], loss: 3.709, per_step_time: 556ms, lr: 4.9620863e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:17,522 - mindformers[callback.py:317] - INFO -    0.9% |                                                  | 14.37 samples/s/p  0:34:30 }\n",
      "2023-11-17 09:47:19,755 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   36/ 3750], loss: 3.538, per_step_time: 556ms, lr: 4.9568567e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:19,755 - mindformers[callback.py:317] - INFO -    1.0% |                                                  | 14.37 samples/s/p  0:34:27 }\n",
      "2023-11-17 09:47:21,989 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   40/ 3750], loss: 4.328, per_step_time: 556ms, lr: 4.951627e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:47:21,989 - mindformers[callback.py:317] - INFO -    1.1% |                                                  | 14.37 samples/s/p  0:34:25 }\n",
      "2023-11-17 09:47:24,220 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   44/ 3750], loss: 4.924, per_step_time: 556ms, lr: 4.946398e-05, overflow cond: True, loss_scale: 16384.0\n",
      "2023-11-17 09:47:24,221 - mindformers[callback.py:317] - INFO -    1.2% |                                                  | 14.38 samples/s/p  0:34:21 }\n",
      "2023-11-17 09:47:26,461 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   48/ 3750], loss: 2.863, per_step_time: 558ms, lr: 4.9424758e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:26,462 - mindformers[callback.py:317] - INFO -    1.3% |                                                  | 14.32 samples/s/p  0:34:28 }\n",
      "2023-11-17 09:47:28,694 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   52/ 3750], loss: 3.651, per_step_time: 556ms, lr: 4.9372462e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:28,695 - mindformers[callback.py:317] - INFO -    1.4% |                                                  | 14.37 samples/s/p  0:34:18 }\n",
      "2023-11-17 09:47:30,930 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   56/ 3750], loss: 2.757, per_step_time: 556ms, lr: 4.932017e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:30,931 - mindformers[callback.py:317] - INFO -    1.5% |                                                  | 14.37 samples/s/p  0:34:16 }\n",
      "2023-11-17 09:47:33,163 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   60/ 3750], loss: 3.789, per_step_time: 556ms, lr: 4.9267874e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:33,164 - mindformers[callback.py:317] - INFO -    1.6% |                                                  | 14.37 samples/s/p  0:34:14 }\n",
      "2023-11-17 09:47:35,396 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   64/ 3750], loss: 3.209, per_step_time: 556ms, lr: 4.9215578e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:35,397 - mindformers[callback.py:317] - INFO -    1.7% |                                                  | 14.37 samples/s/p  0:34:11 }\n",
      "2023-11-17 09:47:37,630 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   68/ 3750], loss: 3.422, per_step_time: 556ms, lr: 4.9163285e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:37,630 - mindformers[callback.py:317] - INFO -    1.8% |                                                  | 14.37 samples/s/p  0:34:10 }\n",
      "2023-11-17 09:47:39,863 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   72/ 3750], loss: 3.465, per_step_time: 556ms, lr: 4.9110993e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:39,863 - mindformers[callback.py:317] - INFO -    1.9% |                                                  | 14.37 samples/s/p  0:34:07 }\n",
      "2023-11-17 09:47:42,096 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   76/ 3750], loss: 3.177, per_step_time: 556ms, lr: 4.9058697e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:42,097 - mindformers[callback.py:317] - INFO -    2.0% |█                                                 | 14.37 samples/s/p  0:34:05 }\n",
      "2023-11-17 09:47:44,339 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   80/ 3750], loss: 3.332, per_step_time: 558ms, lr: 4.90064e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:44,339 - mindformers[callback.py:317] - INFO -    2.1% |█                                                 | 14.31 samples/s/p  0:34:11 }\n",
      "2023-11-17 09:47:46,571 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   84/ 3750], loss: 2.728, per_step_time: 556ms, lr: 4.895411e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:46,572 - mindformers[callback.py:317] - INFO -    2.2% |█                                                 | 14.37 samples/s/p  0:34:00 }\n",
      "2023-11-17 09:47:48,804 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   88/ 3750], loss: 3.238, per_step_time: 556ms, lr: 4.8901813e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:48,805 - mindformers[callback.py:317] - INFO -    2.3% |█                                                 | 14.37 samples/s/p  0:33:58 }\n",
      "2023-11-17 09:47:51,039 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   92/ 3750], loss: 2.943, per_step_time: 557ms, lr: 4.8849517e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:51,039 - mindformers[callback.py:317] - INFO -    2.5% |█                                                 | 14.36 samples/s/p  0:33:57 }\n",
      "2023-11-17 09:47:53,274 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[   96/ 3750], loss: 2.947, per_step_time: 556ms, lr: 4.8797225e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:53,274 - mindformers[callback.py:317] - INFO -    2.6% |█                                                 | 14.37 samples/s/p  0:33:54 }\n",
      "2023-11-17 09:47:55,508 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  100/ 3750], loss: 3.194, per_step_time: 556ms, lr: 4.874493e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:55,509 - mindformers[callback.py:317] - INFO -    2.7% |█                                                 | 14.36 samples/s/p  0:33:52 }\n",
      "2023-11-17 09:47:55,510 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 54828), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.21 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.28 seconds.\n",
      "2023-11-17 09:47:56,361 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.285214\n",
      "INFO:root:List OBS time cost: 0.01 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.02 seconds.\n",
      "2023-11-17 09:47:56,414 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.023069\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:47:56,457 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.017600\n",
      "INFO:root:List OBS time cost: 0.05 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.16 seconds.\n",
      "2023-11-17 09:47:56,645 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.163418\n",
      "2023-11-17 09:47:58,334 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  104/ 3750], loss: 3.384, per_step_time: 575ms, lr: 4.8692636e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:47:58,334 - mindformers[callback.py:317] - INFO -    2.8% |█                                                 | 13.89 samples/s/p  0:34:59 }\n",
      "2023-11-17 09:48:00,570 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  108/ 3750], loss: 3.400, per_step_time: 555ms, lr: 4.864034e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:00,570 - mindformers[callback.py:317] - INFO -    2.9% |█                                                 | 14.39 samples/s/p  0:33:44 }\n",
      "2023-11-17 09:48:02,800 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  112/ 3750], loss: 3.619, per_step_time: 555ms, lr: 4.858804e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:02,800 - mindformers[callback.py:317] - INFO -    3.0% |█                                                 | 14.39 samples/s/p  0:33:42 }\n",
      "2023-11-17 09:48:05,031 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  116/ 3750], loss: 3.018, per_step_time: 556ms, lr: 4.8535752e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:05,031 - mindformers[callback.py:317] - INFO -    3.1% |█                                                 | 14.39 samples/s/p  0:33:40 }\n",
      "2023-11-17 09:48:07,261 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  120/ 3750], loss: 2.368, per_step_time: 555ms, lr: 4.8483453e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:07,261 - mindformers[callback.py:317] - INFO -    3.2% |█                                                 | 14.39 samples/s/p  0:33:38 }\n",
      "2023-11-17 09:48:09,491 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  124/ 3750], loss: 2.941, per_step_time: 555ms, lr: 4.8431164e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:09,491 - mindformers[callback.py:317] - INFO -    3.3% |█                                                 | 14.39 samples/s/p  0:33:35 }\n",
      "2023-11-17 09:48:11,721 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  128/ 3750], loss: 3.142, per_step_time: 555ms, lr: 4.8378864e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:11,722 - mindformers[callback.py:317] - INFO -    3.4% |█                                                 | 14.39 samples/s/p  0:33:33 }\n",
      "2023-11-17 09:48:13,951 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  132/ 3750], loss: 1.918, per_step_time: 555ms, lr: 4.8326572e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:13,951 - mindformers[callback.py:317] - INFO -    3.5% |█                                                 | 14.39 samples/s/p  0:33:30 }\n",
      "2023-11-17 09:48:16,182 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  136/ 3750], loss: 2.724, per_step_time: 556ms, lr: 4.8274276e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:16,183 - mindformers[callback.py:317] - INFO -    3.6% |█                                                 | 14.39 samples/s/p  0:33:29 }\n",
      "2023-11-17 09:48:18,413 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  140/ 3750], loss: 3.283, per_step_time: 555ms, lr: 4.8221984e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:18,413 - mindformers[callback.py:317] - INFO -    3.7% |█                                                 | 14.39 samples/s/p  0:33:26 }\n",
      "2023-11-17 09:48:20,642 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  144/ 3750], loss: 2.655, per_step_time: 555ms, lr: 4.8169688e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:20,643 - mindformers[callback.py:317] - INFO -    3.8% |█                                                 | 14.39 samples/s/p  0:33:24 }\n",
      "2023-11-17 09:48:22,873 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  148/ 3750], loss: 3.574, per_step_time: 556ms, lr: 4.8117396e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:22,873 - mindformers[callback.py:317] - INFO -    3.9% |█                                                 | 14.39 samples/s/p  0:33:22 }\n",
      "2023-11-17 09:48:25,103 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  152/ 3750], loss: 3.245, per_step_time: 555ms, lr: 4.80651e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:25,103 - mindformers[callback.py:317] - INFO -    4.1% |██                                                | 14.39 samples/s/p  0:33:19 }\n",
      "2023-11-17 09:48:27,333 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  156/ 3750], loss: 3.317, per_step_time: 555ms, lr: 4.8012807e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:27,334 - mindformers[callback.py:317] - INFO -    4.2% |██                                                | 14.39 samples/s/p  0:33:17 }\n",
      "2023-11-17 09:48:29,563 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  160/ 3750], loss: 3.320, per_step_time: 555ms, lr: 4.796051e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:29,563 - mindformers[callback.py:317] - INFO -    4.3% |██                                                | 14.39 samples/s/p  0:33:15 }\n",
      "2023-11-17 09:48:31,793 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  164/ 3750], loss: 3.248, per_step_time: 555ms, lr: 4.7908215e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:31,794 - mindformers[callback.py:317] - INFO -    4.4% |██                                                | 14.39 samples/s/p  0:33:13 }\n",
      "2023-11-17 09:48:34,033 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  168/ 3750], loss: 1.915, per_step_time: 558ms, lr: 4.7855923e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:34,033 - mindformers[callback.py:317] - INFO -    4.5% |██                                                | 14.33 samples/s/p  0:33:19 }\n",
      "2023-11-17 09:48:36,263 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  172/ 3750], loss: 2.234, per_step_time: 555ms, lr: 4.7803627e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:36,263 - mindformers[callback.py:317] - INFO -    4.6% |██                                                | 14.39 samples/s/p  0:33:09 }\n",
      "2023-11-17 09:48:38,494 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  176/ 3750], loss: 2.457, per_step_time: 555ms, lr: 4.775133e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:38,494 - mindformers[callback.py:317] - INFO -    4.7% |██                                                | 14.39 samples/s/p  0:33:06 }\n",
      "2023-11-17 09:48:40,724 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  180/ 3750], loss: 2.671, per_step_time: 555ms, lr: 4.769904e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:40,724 - mindformers[callback.py:317] - INFO -    4.8% |██                                                | 14.39 samples/s/p  0:33:04 }\n",
      "2023-11-17 09:48:42,954 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  184/ 3750], loss: 2.699, per_step_time: 555ms, lr: 4.7646743e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:42,955 - mindformers[callback.py:317] - INFO -    4.9% |██                                                | 14.39 samples/s/p  0:33:02 }\n",
      "2023-11-17 09:48:45,185 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  188/ 3750], loss: 2.698, per_step_time: 556ms, lr: 4.759445e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:45,186 - mindformers[callback.py:317] - INFO -    5.0% |██                                                | 14.39 samples/s/p  0:33:00 }\n",
      "2023-11-17 09:48:47,415 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  192/ 3750], loss: 2.573, per_step_time: 555ms, lr: 4.7542155e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:47,416 - mindformers[callback.py:317] - INFO -    5.1% |██                                                | 14.39 samples/s/p  0:32:57 }\n",
      "2023-11-17 09:48:49,646 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  196/ 3750], loss: 2.729, per_step_time: 556ms, lr: 4.7489855e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:49,647 - mindformers[callback.py:317] - INFO -    5.2% |██                                                | 14.39 samples/s/p  0:32:56 }\n",
      "2023-11-17 09:48:51,877 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  200/ 3750], loss: 3.705, per_step_time: 556ms, lr: 4.7437567e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:51,878 - mindformers[callback.py:317] - INFO -    5.3% |██                                                | 14.39 samples/s/p  0:32:54 }\n",
      "2023-11-17 09:48:51,880 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57756), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.19 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.23 seconds.\n",
      "2023-11-17 09:48:52,628 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.227221\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 09:48:52,708 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.044847\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:48:52,804 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.022123\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.14 seconds.\n",
      "2023-11-17 09:48:52,972 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.143635\n",
      "2023-11-17 09:48:54,643 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  204/ 3750], loss: 2.892, per_step_time: 570ms, lr: 4.7385267e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:54,644 - mindformers[callback.py:317] - INFO -    5.4% |██                                                | 14.02 samples/s/p  0:33:42 }\n",
      "2023-11-17 09:48:56,874 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  208/ 3750], loss: 2.501, per_step_time: 555ms, lr: 4.7332975e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:56,875 - mindformers[callback.py:317] - INFO -    5.5% |██                                                | 14.39 samples/s/p  0:32:48 }\n",
      "2023-11-17 09:48:59,106 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  212/ 3750], loss: 3.380, per_step_time: 556ms, lr: 4.728068e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:48:59,106 - mindformers[callback.py:317] - INFO -    5.7% |██                                                | 14.39 samples/s/p  0:32:47 }\n",
      "2023-11-17 09:49:01,336 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  216/ 3750], loss: 2.310, per_step_time: 555ms, lr: 4.7228386e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:01,336 - mindformers[callback.py:317] - INFO -    5.8% |██                                                | 14.40 samples/s/p  0:32:43 }\n",
      "2023-11-17 09:49:03,566 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  220/ 3750], loss: 2.237, per_step_time: 555ms, lr: 4.717609e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:03,566 - mindformers[callback.py:317] - INFO -    5.9% |██                                                | 14.39 samples/s/p  0:32:41 }\n",
      "2023-11-17 09:49:05,803 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  224/ 3750], loss: 3.104, per_step_time: 557ms, lr: 4.7123798e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:05,803 - mindformers[callback.py:317] - INFO -    6.0% |██                                                | 14.35 samples/s/p  0:32:45 }\n",
      "2023-11-17 09:49:08,049 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  228/ 3750], loss: 2.938, per_step_time: 559ms, lr: 4.7071502e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:08,049 - mindformers[callback.py:317] - INFO -    6.1% |███                                               | 14.30 samples/s/p  0:32:50 }\n",
      "2023-11-17 09:49:10,284 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  232/ 3750], loss: 2.447, per_step_time: 557ms, lr: 4.701921e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:10,285 - mindformers[callback.py:317] - INFO -    6.2% |███                                               | 14.36 samples/s/p  0:32:40 }\n",
      "2023-11-17 09:49:12,512 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  236/ 3750], loss: 3.041, per_step_time: 555ms, lr: 4.6966914e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:12,513 - mindformers[callback.py:317] - INFO -    6.3% |███                                               | 14.41 samples/s/p  0:32:30 }\n",
      "2023-11-17 09:49:14,739 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  240/ 3750], loss: 2.409, per_step_time: 554ms, lr: 4.6914618e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:14,739 - mindformers[callback.py:317] - INFO -    6.4% |███                                               | 14.42 samples/s/p  0:32:27 }\n",
      "2023-11-17 09:49:16,966 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  244/ 3750], loss: 2.111, per_step_time: 555ms, lr: 4.6862326e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:16,967 - mindformers[callback.py:317] - INFO -    6.5% |███                                               | 14.41 samples/s/p  0:32:26 }\n",
      "2023-11-17 09:49:19,193 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  248/ 3750], loss: 3.285, per_step_time: 554ms, lr: 4.681003e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:19,193 - mindformers[callback.py:317] - INFO -    6.6% |███                                               | 14.42 samples/s/p  0:32:23 }\n",
      "2023-11-17 09:49:21,422 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  252/ 3750], loss: 2.596, per_step_time: 555ms, lr: 4.6757737e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:21,422 - mindformers[callback.py:317] - INFO -    6.7% |███                                               | 14.41 samples/s/p  0:32:22 }\n",
      "2023-11-17 09:49:23,653 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  256/ 3750], loss: 2.259, per_step_time: 555ms, lr: 4.670544e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:23,654 - mindformers[callback.py:317] - INFO -    6.8% |███                                               | 14.40 samples/s/p  0:32:20 }\n",
      "2023-11-17 09:49:25,881 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  260/ 3750], loss: 2.944, per_step_time: 555ms, lr: 4.6653146e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:25,882 - mindformers[callback.py:317] - INFO -    6.9% |███                                               | 14.41 samples/s/p  0:32:17 }\n",
      "2023-11-17 09:49:28,116 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  264/ 3750], loss: 3.158, per_step_time: 556ms, lr: 4.6600853e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:28,116 - mindformers[callback.py:317] - INFO -    7.0% |███                                               | 14.37 samples/s/p  0:32:20 }\n",
      "2023-11-17 09:49:30,348 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  268/ 3750], loss: 2.204, per_step_time: 556ms, lr: 4.6548557e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:30,348 - mindformers[callback.py:317] - INFO -    7.1% |███                                               | 14.38 samples/s/p  0:32:16 }\n",
      "2023-11-17 09:49:32,582 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  272/ 3750], loss: 2.445, per_step_time: 556ms, lr: 4.6496258e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:32,583 - mindformers[callback.py:317] - INFO -    7.3% |███                                               | 14.37 samples/s/p  0:32:16 }\n",
      "2023-11-17 09:49:34,813 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  276/ 3750], loss: 3.462, per_step_time: 555ms, lr: 4.644397e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:34,814 - mindformers[callback.py:317] - INFO -    7.4% |███                                               | 14.39 samples/s/p  0:32:10 }\n",
      "2023-11-17 09:49:37,054 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  280/ 3750], loss: 2.125, per_step_time: 558ms, lr: 4.639167e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:37,055 - mindformers[callback.py:317] - INFO -    7.5% |███                                               | 14.33 samples/s/p  0:32:17 }\n",
      "2023-11-17 09:49:39,396 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  284/ 3750], loss: 2.479, per_step_time: 583ms, lr: 4.633938e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:39,396 - mindformers[callback.py:317] - INFO -    7.6% |███                                               | 13.71 samples/s/p  0:33:41 }\n",
      "2023-11-17 09:49:41,646 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  288/ 3750], loss: 2.938, per_step_time: 560ms, lr: 4.628709e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:41,647 - mindformers[callback.py:317] - INFO -    7.7% |███                                               | 14.27 samples/s/p  0:32:21 }\n",
      "2023-11-17 09:49:43,874 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  292/ 3750], loss: 2.773, per_step_time: 555ms, lr: 4.623479e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:43,874 - mindformers[callback.py:317] - INFO -    7.8% |███                                               | 14.41 samples/s/p  0:31:59 }\n",
      "2023-11-17 09:49:46,107 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  296/ 3750], loss: 2.718, per_step_time: 556ms, lr: 4.6182493e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:46,108 - mindformers[callback.py:317] - INFO -    7.9% |███                                               | 14.38 samples/s/p  0:32:01 }\n",
      "2023-11-17 09:49:48,333 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  300/ 3750], loss: 2.453, per_step_time: 554ms, lr: 4.61302e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:48,334 - mindformers[callback.py:317] - INFO -    8.0% |████                                              | 14.42 samples/s/p  0:31:53 }\n",
      "2023-11-17 09:49:48,336 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34982), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.17 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.21 seconds.\n",
      "2023-11-17 09:49:49,069 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.209595\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 09:49:49,367 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039504\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:49:49,413 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020852\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.14 seconds.\n",
      "2023-11-17 09:49:49,575 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.135960\n",
      "2023-11-17 09:49:51,113 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  304/ 3750], loss: 2.572, per_step_time: 569ms, lr: 4.6077905e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:51,113 - mindformers[callback.py:317] - INFO -    8.1% |████                                              | 14.04 samples/s/p  0:32:44 }\n",
      "2023-11-17 09:49:53,341 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  308/ 3750], loss: 3.310, per_step_time: 555ms, lr: 4.6025612e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:53,342 - mindformers[callback.py:317] - INFO -    8.2% |████                                              | 14.41 samples/s/p  0:31:50 }\n",
      "2023-11-17 09:49:55,568 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  312/ 3750], loss: 2.417, per_step_time: 554ms, lr: 4.5973316e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:55,568 - mindformers[callback.py:317] - INFO -    8.3% |████                                              | 14.42 samples/s/p  0:31:47 }\n",
      "2023-11-17 09:49:57,794 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  316/ 3750], loss: 2.487, per_step_time: 554ms, lr: 4.5921024e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:49:57,795 - mindformers[callback.py:317] - INFO -    8.4% |████                                              | 14.42 samples/s/p  0:31:44 }\n",
      "2023-11-17 09:50:00,024 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  320/ 3750], loss: 3.106, per_step_time: 555ms, lr: 4.586873e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:00,025 - mindformers[callback.py:317] - INFO -    8.5% |████                                              | 14.40 samples/s/p  0:31:45 }\n",
      "2023-11-17 09:50:02,253 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  324/ 3750], loss: 2.702, per_step_time: 554ms, lr: 4.5816432e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:02,253 - mindformers[callback.py:317] - INFO -    8.6% |████                                              | 14.41 samples/s/p  0:31:41 }\n",
      "2023-11-17 09:50:04,480 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  328/ 3750], loss: 2.239, per_step_time: 554ms, lr: 4.576414e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:04,480 - mindformers[callback.py:317] - INFO -    8.7% |████                                              | 14.42 samples/s/p  0:31:39 }\n",
      "2023-11-17 09:50:06,707 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  332/ 3750], loss: 3.005, per_step_time: 555ms, lr: 4.5711848e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:06,708 - mindformers[callback.py:317] - INFO -    8.9% |████                                              | 14.41 samples/s/p  0:31:37 }\n",
      "2023-11-17 09:50:08,944 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  336/ 3750], loss: 2.743, per_step_time: 557ms, lr: 4.565955e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:08,944 - mindformers[callback.py:317] - INFO -    9.0% |████                                              | 14.36 samples/s/p  0:31:41 }\n",
      "2023-11-17 09:50:11,187 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  340/ 3750], loss: 3.268, per_step_time: 559ms, lr: 4.5607256e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:11,188 - mindformers[callback.py:317] - INFO -    9.1% |████                                              | 14.31 samples/s/p  0:31:46 }\n",
      "2023-11-17 09:50:13,432 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  344/ 3750], loss: 1.994, per_step_time: 559ms, lr: 4.555496e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:13,432 - mindformers[callback.py:317] - INFO -    9.2% |████                                              | 14.31 samples/s/p  0:31:44 }\n",
      "2023-11-17 09:50:15,657 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  348/ 3750], loss: 3.335, per_step_time: 554ms, lr: 4.5502667e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:15,658 - mindformers[callback.py:317] - INFO -    9.3% |████                                              | 14.43 samples/s/p  0:31:26 }\n",
      "2023-11-17 09:50:17,884 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  352/ 3750], loss: 2.478, per_step_time: 554ms, lr: 4.545037e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:17,884 - mindformers[callback.py:317] - INFO -    9.4% |████                                              | 14.42 samples/s/p  0:31:25 }\n",
      "2023-11-17 09:50:20,111 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  356/ 3750], loss: 2.320, per_step_time: 554ms, lr: 4.5398072e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:20,111 - mindformers[callback.py:317] - INFO -    9.5% |████                                              | 14.42 samples/s/p  0:31:23 }\n",
      "2023-11-17 09:50:22,338 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  360/ 3750], loss: 2.116, per_step_time: 554ms, lr: 4.5345783e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:22,338 - mindformers[callback.py:317] - INFO -    9.6% |████                                              | 14.42 samples/s/p  0:31:20 }\n",
      "2023-11-17 09:50:24,564 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  364/ 3750], loss: 3.274, per_step_time: 554ms, lr: 4.529349e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:24,564 - mindformers[callback.py:317] - INFO -    9.7% |████                                              | 14.42 samples/s/p  0:31:18 }\n",
      "2023-11-17 09:50:26,797 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  368/ 3750], loss: 3.439, per_step_time: 556ms, lr: 4.5241195e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:26,798 - mindformers[callback.py:317] - INFO -    9.8% |████                                              | 14.38 samples/s/p  0:31:21 }\n",
      "2023-11-17 09:50:29,034 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  372/ 3750], loss: 2.669, per_step_time: 557ms, lr: 4.5188895e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:29,034 - mindformers[callback.py:317] - INFO -    9.9% |████                                              | 14.36 samples/s/p  0:31:21 }\n",
      "2023-11-17 09:50:31,262 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  376/ 3750], loss: 2.538, per_step_time: 555ms, lr: 4.5136603e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:31,263 - mindformers[callback.py:317] - INFO -   10.0% |█████                                             | 14.41 samples/s/p  0:31:12 }\n",
      "2023-11-17 09:50:33,492 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  380/ 3750], loss: 3.182, per_step_time: 555ms, lr: 4.5084307e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:33,493 - mindformers[callback.py:317] - INFO -   10.1% |█████                                             | 14.40 samples/s/p  0:31:12 }\n",
      "2023-11-17 09:50:35,722 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  384/ 3750], loss: 2.689, per_step_time: 555ms, lr: 4.503202e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:35,723 - mindformers[callback.py:317] - INFO -   10.2% |█████                                             | 14.40 samples/s/p  0:31:09 }\n",
      "2023-11-17 09:50:37,965 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  388/ 3750], loss: 2.259, per_step_time: 558ms, lr: 4.497972e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:37,965 - mindformers[callback.py:317] - INFO -   10.3% |█████                                             | 14.32 samples/s/p  0:31:18 }\n",
      "2023-11-17 09:50:40,201 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  392/ 3750], loss: 3.391, per_step_time: 557ms, lr: 4.4927427e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:40,201 - mindformers[callback.py:317] - INFO -   10.5% |█████                                             | 14.36 samples/s/p  0:31:10 }\n",
      "2023-11-17 09:50:42,434 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  396/ 3750], loss: 3.115, per_step_time: 556ms, lr: 4.487513e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:42,435 - mindformers[callback.py:317] - INFO -   10.6% |█████                                             | 14.38 samples/s/p  0:31:06 }\n",
      "2023-11-17 09:50:44,660 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  400/ 3750], loss: 2.684, per_step_time: 554ms, lr: 4.482284e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:44,661 - mindformers[callback.py:317] - INFO -   10.7% |█████                                             | 14.42 samples/s/p  0:30:57 }\n",
      "2023-11-17 09:50:44,664 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 45328), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.18 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.27 seconds.\n",
      "2023-11-17 09:50:45,457 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.271345\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 09:50:45,519 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.038541\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:50:45,584 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.019556\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.12 seconds.\n",
      "2023-11-17 09:50:45,731 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.119921\n",
      "2023-11-17 09:50:47,417 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  404/ 3750], loss: 2.583, per_step_time: 568ms, lr: 4.4770542e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:47,418 - mindformers[callback.py:317] - INFO -   10.8% |█████                                             | 14.07 samples/s/p  0:31:43 }\n",
      "2023-11-17 09:50:49,644 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  408/ 3750], loss: 2.176, per_step_time: 554ms, lr: 4.471825e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:49,645 - mindformers[callback.py:317] - INFO -   10.9% |█████                                             | 14.42 samples/s/p  0:30:53 }\n",
      "2023-11-17 09:50:51,870 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  412/ 3750], loss: 2.459, per_step_time: 554ms, lr: 4.4665954e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:51,871 - mindformers[callback.py:317] - INFO -   11.0% |█████                                             | 14.43 samples/s/p  0:30:51 }\n",
      "2023-11-17 09:50:54,098 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  416/ 3750], loss: 3.280, per_step_time: 554ms, lr: 4.4613662e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:54,098 - mindformers[callback.py:317] - INFO -   11.1% |█████                                             | 14.42 samples/s/p  0:30:49 }\n",
      "2023-11-17 09:50:56,324 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  420/ 3750], loss: 2.934, per_step_time: 554ms, lr: 4.4561362e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:56,324 - mindformers[callback.py:317] - INFO -   11.2% |█████                                             | 14.42 samples/s/p  0:30:46 }\n",
      "2023-11-17 09:50:58,550 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  424/ 3750], loss: 3.394, per_step_time: 554ms, lr: 4.450907e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:50:58,551 - mindformers[callback.py:317] - INFO -   11.3% |█████                                             | 14.42 samples/s/p  0:30:44 }\n",
      "2023-11-17 09:51:00,780 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  428/ 3750], loss: 3.161, per_step_time: 555ms, lr: 4.4456774e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:00,780 - mindformers[callback.py:317] - INFO -   11.4% |█████                                             | 14.41 samples/s/p  0:30:44 }\n",
      "2023-11-17 09:51:03,008 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  432/ 3750], loss: 3.213, per_step_time: 555ms, lr: 4.440448e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:03,009 - mindformers[callback.py:317] - INFO -   11.5% |█████                                             | 14.41 samples/s/p  0:30:42 }\n",
      "2023-11-17 09:51:05,236 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  436/ 3750], loss: 2.680, per_step_time: 554ms, lr: 4.4352186e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:05,236 - mindformers[callback.py:317] - INFO -   11.6% |█████                                             | 14.42 samples/s/p  0:30:38 }\n",
      "2023-11-17 09:51:07,469 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  440/ 3750], loss: 3.092, per_step_time: 556ms, lr: 4.4299893e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:07,469 - mindformers[callback.py:317] - INFO -   11.7% |█████                                             | 14.38 samples/s/p  0:30:41 }\n",
      "2023-11-17 09:51:09,701 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  444/ 3750], loss: 2.956, per_step_time: 556ms, lr: 4.4247598e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:09,702 - mindformers[callback.py:317] - INFO -   11.8% |█████                                             | 14.39 samples/s/p  0:30:38 }\n",
      "2023-11-17 09:51:11,946 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  448/ 3750], loss: 2.841, per_step_time: 559ms, lr: 4.4195305e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:11,946 - mindformers[callback.py:317] - INFO -   11.9% |█████                                             | 14.31 samples/s/p  0:30:46 }\n",
      "2023-11-17 09:51:14,179 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  452/ 3750], loss: 3.143, per_step_time: 556ms, lr: 4.4143006e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:14,180 - mindformers[callback.py:317] - INFO -   12.1% |██████                                            | 14.38 samples/s/p  0:30:34 }\n",
      "2023-11-17 09:51:16,405 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  456/ 3750], loss: 2.605, per_step_time: 554ms, lr: 4.409071e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:16,406 - mindformers[callback.py:317] - INFO -   12.2% |██████                                            | 14.43 samples/s/p  0:30:26 }\n",
      "2023-11-17 09:51:18,632 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  460/ 3750], loss: 2.976, per_step_time: 554ms, lr: 4.403842e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:18,632 - mindformers[callback.py:317] - INFO -   12.3% |██████                                            | 14.42 samples/s/p  0:30:25 }\n",
      "2023-11-17 09:51:20,859 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  464/ 3750], loss: 2.409, per_step_time: 554ms, lr: 4.398612e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:20,859 - mindformers[callback.py:317] - INFO -   12.4% |██████                                            | 14.42 samples/s/p  0:30:22 }\n",
      "2023-11-17 09:51:23,086 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  468/ 3750], loss: 2.172, per_step_time: 554ms, lr: 4.3933833e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:23,086 - mindformers[callback.py:317] - INFO -   12.5% |██████                                            | 14.42 samples/s/p  0:30:20 }\n",
      "2023-11-17 09:51:25,313 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  472/ 3750], loss: 2.400, per_step_time: 554ms, lr: 4.3881533e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:25,314 - mindformers[callback.py:317] - INFO -   12.6% |██████                                            | 14.42 samples/s/p  0:30:18 }\n",
      "2023-11-17 09:51:27,547 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  476/ 3750], loss: 3.371, per_step_time: 556ms, lr: 4.382924e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:27,548 - mindformers[callback.py:317] - INFO -   12.7% |██████                                            | 14.38 samples/s/p  0:30:21 }\n",
      "2023-11-17 09:51:29,775 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  480/ 3750], loss: 2.473, per_step_time: 554ms, lr: 4.3776945e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:29,776 - mindformers[callback.py:317] - INFO -   12.8% |██████                                            | 14.42 samples/s/p  0:30:14 }\n",
      "2023-11-17 09:51:32,003 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  484/ 3750], loss: 2.355, per_step_time: 554ms, lr: 4.3724653e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:32,003 - mindformers[callback.py:317] - INFO -   12.9% |██████                                            | 14.42 samples/s/p  0:30:11 }\n",
      "2023-11-17 09:51:34,230 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  488/ 3750], loss: 2.734, per_step_time: 554ms, lr: 4.3672357e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:34,231 - mindformers[callback.py:317] - INFO -   13.0% |██████                                            | 14.42 samples/s/p  0:30:09 }\n",
      "2023-11-17 09:51:36,463 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  492/ 3750], loss: 2.398, per_step_time: 556ms, lr: 4.3620064e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:36,464 - mindformers[callback.py:317] - INFO -   13.1% |██████                                            | 14.39 samples/s/p  0:30:11 }\n",
      "2023-11-17 09:51:38,701 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  496/ 3750], loss: 2.709, per_step_time: 557ms, lr: 4.356777e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:38,701 - mindformers[callback.py:317] - INFO -   13.2% |██████                                            | 14.35 samples/s/p  0:30:13 }\n",
      "2023-11-17 09:51:40,940 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  500/ 3750], loss: 2.737, per_step_time: 557ms, lr: 4.3515476e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:40,940 - mindformers[callback.py:317] - INFO -   13.3% |██████                                            | 14.35 samples/s/p  0:30:12 }\n",
      "2023-11-17 09:51:40,944 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 44054), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.19 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.24 seconds.\n",
      "2023-11-17 09:51:41,732 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.240776\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.05 seconds.\n",
      "2023-11-17 09:51:41,846 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.049389\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:51:41,893 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.023450\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.12 seconds.\n",
      "2023-11-17 09:51:42,042 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.121248\n",
      "2023-11-17 09:51:43,735 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  504/ 3750], loss: 2.493, per_step_time: 573ms, lr: 4.3463177e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:43,736 - mindformers[callback.py:317] - INFO -   13.4% |██████                                            | 13.95 samples/s/p  0:31:01 }\n",
      "2023-11-17 09:51:45,963 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  508/ 3750], loss: 2.718, per_step_time: 554ms, lr: 4.3410884e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:45,963 - mindformers[callback.py:317] - INFO -   13.5% |██████                                            | 14.42 samples/s/p  0:29:58 }\n",
      "2023-11-17 09:51:48,190 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  512/ 3750], loss: 2.456, per_step_time: 554ms, lr: 4.335859e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:48,190 - mindformers[callback.py:317] - INFO -   13.7% |██████                                            | 14.42 samples/s/p  0:29:55 }\n",
      "2023-11-17 09:51:50,417 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  516/ 3750], loss: 2.600, per_step_time: 554ms, lr: 4.3306296e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:50,417 - mindformers[callback.py:317] - INFO -   13.8% |██████                                            | 14.42 samples/s/p  0:29:54 }\n",
      "2023-11-17 09:51:52,645 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  520/ 3750], loss: 2.700, per_step_time: 554ms, lr: 4.3254e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:52,645 - mindformers[callback.py:317] - INFO -   13.9% |██████                                            | 14.42 samples/s/p  0:29:52 }\n",
      "2023-11-17 09:51:54,874 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  524/ 3750], loss: 3.034, per_step_time: 554ms, lr: 4.3201708e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:54,875 - mindformers[callback.py:317] - INFO -   14.0% |██████                                            | 14.41 samples/s/p  0:29:50 }\n",
      "2023-11-17 09:51:57,101 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  528/ 3750], loss: 2.918, per_step_time: 554ms, lr: 4.314941e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:57,101 - mindformers[callback.py:317] - INFO -   14.1% |███████                                           | 14.43 samples/s/p  0:29:46 }\n",
      "2023-11-17 09:51:59,335 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  532/ 3750], loss: 3.258, per_step_time: 556ms, lr: 4.309712e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:51:59,336 - mindformers[callback.py:317] - INFO -   14.2% |███████                                           | 14.37 samples/s/p  0:29:51 }\n",
      "2023-11-17 09:52:01,582 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  536/ 3750], loss: 2.696, per_step_time: 559ms, lr: 4.3044824e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:01,583 - mindformers[callback.py:317] - INFO -   14.3% |███████                                           | 14.30 samples/s/p  0:29:58 }\n",
      "2023-11-17 09:52:03,854 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  540/ 3750], loss: 3.523, per_step_time: 565ms, lr: 4.2992524e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:03,855 - mindformers[callback.py:317] - INFO -   14.4% |███████                                           | 14.14 samples/s/p  0:30:16 }\n",
      "2023-11-17 09:52:06,096 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  544/ 3750], loss: 2.533, per_step_time: 558ms, lr: 4.2940235e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:06,097 - mindformers[callback.py:317] - INFO -   14.5% |███████                                           | 14.33 samples/s/p  0:29:50 }\n",
      "2023-11-17 09:52:08,352 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  548/ 3750], loss: 2.612, per_step_time: 561ms, lr: 4.2887936e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:08,353 - mindformers[callback.py:317] - INFO -   14.6% |███████                                           | 14.24 samples/s/p  0:29:59 }\n",
      "2023-11-17 09:52:10,598 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  552/ 3750], loss: 2.190, per_step_time: 559ms, lr: 4.2835643e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:10,599 - mindformers[callback.py:317] - INFO -   14.7% |███████                                           | 14.30 samples/s/p  0:29:48 }\n",
      "2023-11-17 09:52:12,825 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  556/ 3750], loss: 3.286, per_step_time: 554ms, lr: 4.2783347e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:12,825 - mindformers[callback.py:317] - INFO -   14.8% |███████                                           | 14.42 samples/s/p  0:29:31 }\n",
      "2023-11-17 09:52:15,052 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  560/ 3750], loss: 2.734, per_step_time: 554ms, lr: 4.2731055e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:15,052 - mindformers[callback.py:317] - INFO -   14.9% |███████                                           | 14.42 samples/s/p  0:29:29 }\n",
      "2023-11-17 09:52:17,278 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  564/ 3750], loss: 2.857, per_step_time: 554ms, lr: 4.267876e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:17,279 - mindformers[callback.py:317] - INFO -   15.0% |███████                                           | 14.43 samples/s/p  0:29:26 }\n",
      "2023-11-17 09:52:19,506 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  568/ 3750], loss: 2.889, per_step_time: 554ms, lr: 4.2626467e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:19,506 - mindformers[callback.py:317] - INFO -   15.1% |███████                                           | 14.42 samples/s/p  0:29:25 }\n",
      "2023-11-17 09:52:21,735 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  572/ 3750], loss: 3.450, per_step_time: 554ms, lr: 4.257417e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:21,735 - mindformers[callback.py:317] - INFO -   15.3% |███████                                           | 14.41 samples/s/p  0:29:23 }\n",
      "2023-11-17 09:52:23,963 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  576/ 3750], loss: 2.766, per_step_time: 554ms, lr: 4.252188e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:23,964 - mindformers[callback.py:317] - INFO -   15.4% |███████                                           | 14.42 samples/s/p  0:29:21 }\n",
      "2023-11-17 09:52:26,193 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  580/ 3750], loss: 2.344, per_step_time: 555ms, lr: 4.2469583e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:26,193 - mindformers[callback.py:317] - INFO -   15.5% |███████                                           | 14.41 samples/s/p  0:29:20 }\n",
      "2023-11-17 09:52:28,437 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  584/ 3750], loss: 3.265, per_step_time: 558ms, lr: 4.2417287e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:28,437 - mindformers[callback.py:317] - INFO -   15.6% |███████                                           | 14.32 samples/s/p  0:29:28 }\n",
      "2023-11-17 09:52:30,736 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  588/ 3750], loss: 3.508, per_step_time: 572ms, lr: 4.236499e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:30,737 - mindformers[callback.py:317] - INFO -   15.7% |███████                                           | 13.97 samples/s/p  0:30:10 }\n",
      "2023-11-17 09:52:33,018 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  592/ 3750], loss: 3.345, per_step_time: 568ms, lr: 4.23127e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:33,018 - mindformers[callback.py:317] - INFO -   15.8% |███████                                           | 14.08 samples/s/p  0:29:53 }\n",
      "2023-11-17 09:52:35,271 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  596/ 3750], loss: 3.336, per_step_time: 561ms, lr: 4.2260403e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:35,272 - mindformers[callback.py:317] - INFO -   15.9% |███████                                           | 14.25 samples/s/p  0:29:30 }\n",
      "2023-11-17 09:52:37,504 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  600/ 3750], loss: 2.591, per_step_time: 555ms, lr: 4.220811e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:37,504 - mindformers[callback.py:317] - INFO -   16.0% |████████                                          | 14.39 samples/s/p  0:29:11 }\n",
      "2023-11-17 09:52:37,508 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 55708), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.14 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.21 seconds.\n",
      "2023-11-17 09:52:38,244 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.208664\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 09:52:38,308 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039176\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:52:38,357 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.021139\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.16 seconds.\n",
      "2023-11-17 09:52:38,542 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.158171\n",
      "2023-11-17 09:52:40,285 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  604/ 3750], loss: 2.904, per_step_time: 574ms, lr: 4.2155814e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:40,286 - mindformers[callback.py:317] - INFO -   16.1% |████████                                          | 13.92 samples/s/p  0:30:08 }\n",
      "2023-11-17 09:52:42,516 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  608/ 3750], loss: 2.762, per_step_time: 555ms, lr: 4.2103522e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:42,517 - mindformers[callback.py:317] - INFO -   16.2% |████████                                          | 14.41 samples/s/p  0:29:04 }\n",
      "2023-11-17 09:52:44,748 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  612/ 3750], loss: 2.686, per_step_time: 555ms, lr: 4.2051226e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:44,749 - mindformers[callback.py:317] - INFO -   16.3% |████████                                          | 14.39 samples/s/p  0:29:04 }\n",
      "2023-11-17 09:52:46,980 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  616/ 3750], loss: 2.644, per_step_time: 555ms, lr: 4.1998926e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:46,981 - mindformers[callback.py:317] - INFO -   16.4% |████████                                          | 14.40 samples/s/p  0:29:00 }\n",
      "2023-11-17 09:52:49,212 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  620/ 3750], loss: 2.947, per_step_time: 555ms, lr: 4.1946638e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:49,212 - mindformers[callback.py:317] - INFO -   16.5% |████████                                          | 14.40 samples/s/p  0:28:59 }\n",
      "2023-11-17 09:52:51,441 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  624/ 3750], loss: 3.044, per_step_time: 555ms, lr: 4.189434e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:51,442 - mindformers[callback.py:317] - INFO -   16.6% |████████                                          | 14.41 samples/s/p  0:28:55 }\n",
      "2023-11-17 09:52:53,671 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  628/ 3750], loss: 3.038, per_step_time: 555ms, lr: 4.184205e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:53,672 - mindformers[callback.py:317] - INFO -   16.7% |████████                                          | 14.41 samples/s/p  0:28:53 }\n",
      "2023-11-17 09:52:55,918 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  632/ 3750], loss: 2.626, per_step_time: 559ms, lr: 4.178975e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:55,918 - mindformers[callback.py:317] - INFO -   16.9% |████████                                          | 14.30 samples/s/p  0:29:03 }\n",
      "2023-11-17 09:52:58,187 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  636/ 3750], loss: 2.142, per_step_time: 564ms, lr: 4.1737458e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:52:58,187 - mindformers[callback.py:317] - INFO -   17.0% |████████                                          | 14.16 samples/s/p  0:29:19 }\n",
      "2023-11-17 09:53:00,460 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  640/ 3750], loss: 2.974, per_step_time: 565ms, lr: 4.168516e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:00,460 - mindformers[callback.py:317] - INFO -   17.1% |████████                                          | 14.14 samples/s/p  0:29:20 }\n",
      "2023-11-17 09:53:02,686 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  644/ 3750], loss: 2.626, per_step_time: 554ms, lr: 4.163287e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:02,687 - mindformers[callback.py:317] - INFO -   17.2% |████████                                          | 14.43 samples/s/p  0:28:42 }\n",
      "2023-11-17 09:53:04,913 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  648/ 3750], loss: 3.324, per_step_time: 554ms, lr: 4.1580573e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:04,913 - mindformers[callback.py:317] - INFO -   17.3% |████████                                          | 14.43 samples/s/p  0:28:39 }\n",
      "2023-11-17 09:53:07,139 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  652/ 3750], loss: 2.441, per_step_time: 554ms, lr: 4.152828e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:07,140 - mindformers[callback.py:317] - INFO -   17.4% |████████                                          | 14.43 samples/s/p  0:28:37 }\n",
      "2023-11-17 09:53:09,365 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  656/ 3750], loss: 1.735, per_step_time: 554ms, lr: 4.1475985e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:09,366 - mindformers[callback.py:317] - INFO -   17.5% |████████                                          | 14.43 samples/s/p  0:28:35 }\n",
      "2023-11-17 09:53:11,592 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  660/ 3750], loss: 2.765, per_step_time: 554ms, lr: 4.1423693e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:11,592 - mindformers[callback.py:317] - INFO -   17.6% |████████                                          | 14.43 samples/s/p  0:28:33 }\n",
      "2023-11-17 09:53:13,819 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  664/ 3750], loss: 2.652, per_step_time: 554ms, lr: 4.1371397e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:13,819 - mindformers[callback.py:317] - INFO -   17.7% |████████                                          | 14.43 samples/s/p  0:28:31 }\n",
      "2023-11-17 09:53:16,046 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  668/ 3750], loss: 3.188, per_step_time: 554ms, lr: 4.13191e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:16,046 - mindformers[callback.py:317] - INFO -   17.8% |████████                                          | 14.43 samples/s/p  0:28:29 }\n",
      "2023-11-17 09:53:18,272 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  672/ 3750], loss: 3.670, per_step_time: 554ms, lr: 4.1266805e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:18,273 - mindformers[callback.py:317] - INFO -   17.9% |████████                                          | 14.43 samples/s/p  0:28:26 }\n",
      "2023-11-17 09:53:20,501 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  676/ 3750], loss: 2.856, per_step_time: 554ms, lr: 4.1214516e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:20,501 - mindformers[callback.py:317] - INFO -   18.0% |█████████                                         | 14.42 samples/s/p  0:28:25 }\n",
      "2023-11-17 09:53:22,729 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  680/ 3750], loss: 2.963, per_step_time: 554ms, lr: 4.1162217e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:22,730 - mindformers[callback.py:317] - INFO -   18.1% |█████████                                         | 14.42 samples/s/p  0:28:22 }\n",
      "2023-11-17 09:53:24,956 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  684/ 3750], loss: 2.869, per_step_time: 554ms, lr: 4.1109924e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:24,956 - mindformers[callback.py:317] - INFO -   18.2% |█████████                                         | 14.43 samples/s/p  0:28:19 }\n",
      "2023-11-17 09:53:27,186 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  688/ 3750], loss: 3.059, per_step_time: 555ms, lr: 4.105763e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:27,186 - mindformers[callback.py:317] - INFO -   18.3% |█████████                                         | 14.41 samples/s/p  0:28:20 }\n",
      "2023-11-17 09:53:29,416 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  692/ 3750], loss: 3.380, per_step_time: 555ms, lr: 4.1005336e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:29,417 - mindformers[callback.py:317] - INFO -   18.5% |█████████                                         | 14.40 samples/s/p  0:28:18 }\n",
      "2023-11-17 09:53:31,652 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  696/ 3750], loss: 2.721, per_step_time: 556ms, lr: 4.095304e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:31,652 - mindformers[callback.py:317] - INFO -   18.6% |█████████                                         | 14.38 samples/s/p  0:28:19 }\n",
      "2023-11-17 09:53:33,890 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  700/ 3750], loss: 2.851, per_step_time: 557ms, lr: 4.090074e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:33,891 - mindformers[callback.py:317] - INFO -   18.7% |█████████                                         | 14.35 samples/s/p  0:28:19 }\n",
      "2023-11-17 09:53:33,895 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 49188), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.16 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.20 seconds.\n",
      "2023-11-17 09:53:34,619 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.195504\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 09:53:34,685 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.040362\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:53:34,747 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.034627\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.12 seconds.\n",
      "2023-11-17 09:53:34,892 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.117758\n",
      "2023-11-17 09:53:36,656 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  704/ 3750], loss: 3.215, per_step_time: 570ms, lr: 4.0848452e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:36,657 - mindformers[callback.py:317] - INFO -   18.8% |█████████                                         | 14.02 samples/s/p  0:28:57 }\n",
      "2023-11-17 09:53:38,883 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  708/ 3750], loss: 2.287, per_step_time: 554ms, lr: 4.079616e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:38,884 - mindformers[callback.py:317] - INFO -   18.9% |█████████                                         | 14.43 samples/s/p  0:28:06 }\n",
      "2023-11-17 09:53:41,110 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  712/ 3750], loss: 2.477, per_step_time: 554ms, lr: 4.0743864e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:41,110 - mindformers[callback.py:317] - INFO -   19.0% |█████████                                         | 14.43 samples/s/p  0:28:04 }\n",
      "2023-11-17 09:53:43,337 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  716/ 3750], loss: 2.680, per_step_time: 554ms, lr: 4.0691564e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:43,337 - mindformers[callback.py:317] - INFO -   19.1% |█████████                                         | 14.43 samples/s/p  0:28:02 }\n",
      "2023-11-17 09:53:45,564 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  720/ 3750], loss: 2.318, per_step_time: 554ms, lr: 4.0639272e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:45,564 - mindformers[callback.py:317] - INFO -   19.2% |█████████                                         | 14.43 samples/s/p  0:28:00 }\n",
      "2023-11-17 09:53:47,791 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  724/ 3750], loss: 3.181, per_step_time: 554ms, lr: 4.0586976e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:47,791 - mindformers[callback.py:317] - INFO -   19.3% |█████████                                         | 14.42 samples/s/p  0:27:58 }\n",
      "2023-11-17 09:53:50,018 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  728/ 3750], loss: 2.676, per_step_time: 554ms, lr: 4.0534684e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:50,019 - mindformers[callback.py:317] - INFO -   19.4% |█████████                                         | 14.43 samples/s/p  0:27:55 }\n",
      "2023-11-17 09:53:52,245 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  732/ 3750], loss: 2.670, per_step_time: 554ms, lr: 4.0482388e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:52,246 - mindformers[callback.py:317] - INFO -   19.5% |█████████                                         | 14.43 samples/s/p  0:27:53 }\n",
      "2023-11-17 09:53:54,481 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  736/ 3750], loss: 2.603, per_step_time: 556ms, lr: 4.0430095e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:54,482 - mindformers[callback.py:317] - INFO -   19.6% |█████████                                         | 14.37 samples/s/p  0:27:57 }\n",
      "2023-11-17 09:53:56,708 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  740/ 3750], loss: 2.201, per_step_time: 554ms, lr: 4.03778e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:56,708 - mindformers[callback.py:317] - INFO -   19.7% |█████████                                         | 14.43 samples/s/p  0:27:48 }\n",
      "2023-11-17 09:53:58,934 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  744/ 3750], loss: 2.561, per_step_time: 554ms, lr: 4.0325507e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:53:58,935 - mindformers[callback.py:317] - INFO -   19.8% |█████████                                         | 14.43 samples/s/p  0:27:46 }\n",
      "2023-11-17 09:54:01,161 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  748/ 3750], loss: 3.187, per_step_time: 554ms, lr: 4.027321e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:01,162 - mindformers[callback.py:317] - INFO -   19.9% |█████████                                         | 14.43 samples/s/p  0:27:44 }\n",
      "2023-11-17 09:54:03,398 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  752/ 3750], loss: 2.502, per_step_time: 556ms, lr: 4.022092e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:03,402 - mindformers[callback.py:317] - INFO -   20.1% |██████████                                        | 14.37 samples/s/p  0:27:48 }\n",
      "2023-11-17 09:54:05,640 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  756/ 3750], loss: 2.724, per_step_time: 557ms, lr: 4.016862e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:05,641 - mindformers[callback.py:317] - INFO -   20.2% |██████████                                        | 14.35 samples/s/p  0:27:48 }\n",
      "2023-11-17 09:54:07,878 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  760/ 3750], loss: 3.077, per_step_time: 557ms, lr: 4.0116327e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:07,878 - mindformers[callback.py:317] - INFO -   20.3% |██████████                                        | 14.36 samples/s/p  0:27:45 }\n",
      "2023-11-17 09:54:10,122 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  764/ 3750], loss: 2.331, per_step_time: 558ms, lr: 4.006403e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:10,123 - mindformers[callback.py:317] - INFO -   20.4% |██████████                                        | 14.32 samples/s/p  0:27:48 }\n",
      "2023-11-17 09:54:12,350 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  768/ 3750], loss: 3.094, per_step_time: 554ms, lr: 4.001174e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:12,351 - mindformers[callback.py:317] - INFO -   20.5% |██████████                                        | 14.42 samples/s/p  0:27:33 }\n",
      "2023-11-17 09:54:14,577 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  772/ 3750], loss: 3.230, per_step_time: 554ms, lr: 3.9959443e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:14,578 - mindformers[callback.py:317] - INFO -   20.6% |██████████                                        | 14.43 samples/s/p  0:27:31 }\n",
      "2023-11-17 09:54:16,804 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  776/ 3750], loss: 3.421, per_step_time: 554ms, lr: 3.990715e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:16,805 - mindformers[callback.py:317] - INFO -   20.7% |██████████                                        | 14.43 samples/s/p  0:27:28 }\n",
      "2023-11-17 09:54:19,031 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  780/ 3750], loss: 2.614, per_step_time: 554ms, lr: 3.9854855e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:19,031 - mindformers[callback.py:317] - INFO -   20.8% |██████████                                        | 14.43 samples/s/p  0:27:26 }\n",
      "2023-11-17 09:54:21,258 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  784/ 3750], loss: 2.726, per_step_time: 554ms, lr: 3.9802562e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:21,258 - mindformers[callback.py:317] - INFO -   20.9% |██████████                                        | 14.43 samples/s/p  0:27:24 }\n",
      "2023-11-17 09:54:23,486 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  788/ 3750], loss: 2.619, per_step_time: 554ms, lr: 3.9750266e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:23,487 - mindformers[callback.py:317] - INFO -   21.0% |██████████                                        | 14.42 samples/s/p  0:27:23 }\n",
      "2023-11-17 09:54:25,720 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  792/ 3750], loss: 2.581, per_step_time: 554ms, lr: 3.9697967e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:25,721 - mindformers[callback.py:317] - INFO -   21.1% |██████████                                        | 14.42 samples/s/p  0:27:21 }\n",
      "2023-11-17 09:54:27,948 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  796/ 3750], loss: 3.152, per_step_time: 554ms, lr: 3.9645678e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:27,948 - mindformers[callback.py:317] - INFO -   21.2% |██████████                                        | 14.43 samples/s/p  0:27:17 }\n",
      "2023-11-17 09:54:30,175 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  800/ 3750], loss: 2.908, per_step_time: 554ms, lr: 3.959338e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:30,175 - mindformers[callback.py:317] - INFO -   21.3% |██████████                                        | 14.43 samples/s/p  0:27:15 }\n",
      "2023-11-17 09:54:30,180 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43786), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.15 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.20 seconds.\n",
      "2023-11-17 09:54:30,905 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.199209\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 09:54:30,970 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.040112\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:54:31,018 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.022952\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.13 seconds.\n",
      "2023-11-17 09:54:31,170 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.127505\n",
      "2023-11-17 09:54:32,934 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  804/ 3750], loss: 2.670, per_step_time: 568ms, lr: 3.954109e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:32,935 - mindformers[callback.py:317] - INFO -   21.4% |██████████                                        | 14.06 samples/s/p  0:27:56 }\n",
      "2023-11-17 09:54:35,167 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  808/ 3750], loss: 3.024, per_step_time: 555ms, lr: 3.948879e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:35,168 - mindformers[callback.py:317] - INFO -   21.5% |██████████                                        | 14.39 samples/s/p  0:27:15 }\n",
      "2023-11-17 09:54:37,405 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  812/ 3750], loss: 3.045, per_step_time: 557ms, lr: 3.9436498e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:37,406 - mindformers[callback.py:317] - INFO -   21.7% |██████████                                        | 14.36 samples/s/p  0:27:16 }\n",
      "2023-11-17 09:54:39,641 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  816/ 3750], loss: 2.468, per_step_time: 556ms, lr: 3.9384202e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:39,641 - mindformers[callback.py:317] - INFO -   21.8% |██████████                                        | 14.38 samples/s/p  0:27:12 }\n",
      "2023-11-17 09:54:41,884 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  820/ 3750], loss: 3.403, per_step_time: 558ms, lr: 3.933191e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:41,885 - mindformers[callback.py:317] - INFO -   21.9% |██████████                                        | 14.33 samples/s/p  0:27:16 }\n",
      "2023-11-17 09:54:44,111 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  824/ 3750], loss: 1.862, per_step_time: 554ms, lr: 3.9279614e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:44,111 - mindformers[callback.py:317] - INFO -   22.0% |██████████                                        | 14.44 samples/s/p  0:27:01 }\n",
      "2023-11-17 09:54:46,337 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  828/ 3750], loss: 1.924, per_step_time: 554ms, lr: 3.922732e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:46,337 - mindformers[callback.py:317] - INFO -   22.1% |███████████                                       | 14.44 samples/s/p  0:26:59 }\n",
      "2023-11-17 09:54:48,563 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  832/ 3750], loss: 2.620, per_step_time: 554ms, lr: 3.9175025e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:48,564 - mindformers[callback.py:317] - INFO -   22.2% |███████████                                       | 14.43 samples/s/p  0:26:57 }\n",
      "2023-11-17 09:54:50,790 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  836/ 3750], loss: 3.169, per_step_time: 554ms, lr: 3.9122733e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:50,790 - mindformers[callback.py:317] - INFO -   22.3% |███████████                                       | 14.43 samples/s/p  0:26:55 }\n",
      "2023-11-17 09:54:53,016 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  840/ 3750], loss: 2.412, per_step_time: 554ms, lr: 3.9070434e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:53,017 - mindformers[callback.py:317] - INFO -   22.4% |███████████                                       | 14.44 samples/s/p  0:26:52 }\n",
      "2023-11-17 09:54:55,244 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  844/ 3750], loss: 2.593, per_step_time: 554ms, lr: 3.901814e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:55,244 - mindformers[callback.py:317] - INFO -   22.5% |███████████                                       | 14.43 samples/s/p  0:26:51 }\n",
      "2023-11-17 09:54:57,479 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  848/ 3750], loss: 3.390, per_step_time: 555ms, lr: 3.8965845e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:57,479 - mindformers[callback.py:317] - INFO -   22.6% |███████████                                       | 14.39 samples/s/p  0:26:53 }\n",
      "2023-11-17 09:54:59,705 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  852/ 3750], loss: 3.319, per_step_time: 554ms, lr: 3.8913553e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:54:59,705 - mindformers[callback.py:317] - INFO -   22.7% |███████████                                       | 14.44 samples/s/p  0:26:45 }\n",
      "2023-11-17 09:55:01,932 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  856/ 3750], loss: 2.169, per_step_time: 554ms, lr: 3.8861257e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:01,932 - mindformers[callback.py:317] - INFO -   22.8% |███████████                                       | 14.43 samples/s/p  0:26:44 }\n",
      "2023-11-17 09:55:04,172 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  860/ 3750], loss: 2.525, per_step_time: 557ms, lr: 3.8808965e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:04,173 - mindformers[callback.py:317] - INFO -   22.9% |███████████                                       | 14.35 samples/s/p  0:26:51 }\n",
      "2023-11-17 09:55:06,404 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  864/ 3750], loss: 2.795, per_step_time: 555ms, lr: 3.875667e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:06,404 - mindformers[callback.py:317] - INFO -   23.0% |███████████                                       | 14.40 samples/s/p  0:26:43 }\n",
      "2023-11-17 09:55:08,638 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  868/ 3750], loss: 3.039, per_step_time: 555ms, lr: 3.8704376e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:08,638 - mindformers[callback.py:317] - INFO -   23.1% |███████████                                       | 14.39 samples/s/p  0:26:42 }\n",
      "2023-11-17 09:55:10,873 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  872/ 3750], loss: 2.976, per_step_time: 556ms, lr: 3.865208e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:10,873 - mindformers[callback.py:317] - INFO -   23.3% |███████████                                       | 14.38 samples/s/p  0:26:40 }\n",
      "2023-11-17 09:55:13,104 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  876/ 3750], loss: 2.507, per_step_time: 555ms, lr: 3.859978e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:13,105 - mindformers[callback.py:317] - INFO -   23.4% |███████████                                       | 14.40 samples/s/p  0:26:36 }\n",
      "2023-11-17 09:55:15,340 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  880/ 3750], loss: 2.304, per_step_time: 556ms, lr: 3.8547492e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:15,340 - mindformers[callback.py:317] - INFO -   23.5% |███████████                                       | 14.38 samples/s/p  0:26:36 }\n",
      "2023-11-17 09:55:17,567 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  884/ 3750], loss: 2.662, per_step_time: 554ms, lr: 3.8495193e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:17,568 - mindformers[callback.py:317] - INFO -   23.6% |███████████                                       | 14.43 samples/s/p  0:26:28 }\n",
      "2023-11-17 09:55:19,794 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  888/ 3750], loss: 3.164, per_step_time: 554ms, lr: 3.84429e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:19,795 - mindformers[callback.py:317] - INFO -   23.7% |███████████                                       | 14.43 samples/s/p  0:26:26 }\n",
      "2023-11-17 09:55:22,021 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  892/ 3750], loss: 3.486, per_step_time: 554ms, lr: 3.8390604e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:22,022 - mindformers[callback.py:317] - INFO -   23.8% |███████████                                       | 14.43 samples/s/p  0:26:24 }\n",
      "2023-11-17 09:55:24,248 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  896/ 3750], loss: 3.059, per_step_time: 554ms, lr: 3.8338312e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:24,249 - mindformers[callback.py:317] - INFO -   23.9% |███████████                                       | 14.43 samples/s/p  0:26:21 }\n",
      "2023-11-17 09:55:26,476 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  900/ 3750], loss: 3.330, per_step_time: 554ms, lr: 3.8286016e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:26,476 - mindformers[callback.py:317] - INFO -   24.0% |████████████                                      | 14.43 samples/s/p  0:26:19 }\n",
      "2023-11-17 09:55:26,481 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 54726), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.15 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.19 seconds.\n",
      "2023-11-17 09:55:27,196 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.194481\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 09:55:27,263 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.037093\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:55:27,309 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.022420\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.15 seconds.\n",
      "2023-11-17 09:55:27,490 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.154482\n",
      "2023-11-17 09:55:29,233 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  904/ 3750], loss: 2.640, per_step_time: 568ms, lr: 3.8233724e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:29,233 - mindformers[callback.py:317] - INFO -   24.1% |████████████                                      | 14.07 samples/s/p  0:26:58 }\n",
      "2023-11-17 09:55:31,461 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  908/ 3750], loss: 3.028, per_step_time: 554ms, lr: 3.8181428e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:31,462 - mindformers[callback.py:317] - INFO -   24.2% |████████████                                      | 14.43 samples/s/p  0:26:15 }\n",
      "2023-11-17 09:55:33,689 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  912/ 3750], loss: 3.505, per_step_time: 554ms, lr: 3.8129136e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:33,690 - mindformers[callback.py:317] - INFO -   24.3% |████████████                                      | 14.43 samples/s/p  0:26:13 }\n",
      "2023-11-17 09:55:35,917 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  916/ 3750], loss: 2.876, per_step_time: 554ms, lr: 3.807684e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:35,917 - mindformers[callback.py:317] - INFO -   24.4% |████████████                                      | 14.44 samples/s/p  0:26:10 }\n",
      "2023-11-17 09:55:38,144 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  920/ 3750], loss: 2.573, per_step_time: 554ms, lr: 3.8024547e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:38,145 - mindformers[callback.py:317] - INFO -   24.5% |████████████                                      | 14.43 samples/s/p  0:26:08 }\n",
      "2023-11-17 09:55:40,371 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  924/ 3750], loss: 3.010, per_step_time: 554ms, lr: 3.7972248e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:40,372 - mindformers[callback.py:317] - INFO -   24.6% |████████████                                      | 14.43 samples/s/p  0:26:06 }\n",
      "2023-11-17 09:55:42,599 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  928/ 3750], loss: 3.336, per_step_time: 554ms, lr: 3.7919956e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:42,599 - mindformers[callback.py:317] - INFO -   24.7% |████████████                                      | 14.43 samples/s/p  0:26:04 }\n",
      "2023-11-17 09:55:44,834 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  932/ 3750], loss: 3.890, per_step_time: 556ms, lr: 3.786766e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:44,834 - mindformers[callback.py:317] - INFO -   24.9% |████████████                                      | 14.39 samples/s/p  0:26:07 }\n",
      "2023-11-17 09:55:47,068 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  936/ 3750], loss: 2.457, per_step_time: 555ms, lr: 3.7815367e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:47,068 - mindformers[callback.py:317] - INFO -   25.0% |████████████                                      | 14.39 samples/s/p  0:26:04 }\n",
      "2023-11-17 09:55:49,303 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  940/ 3750], loss: 2.810, per_step_time: 556ms, lr: 3.776307e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:49,303 - mindformers[callback.py:317] - INFO -   25.1% |████████████                                      | 14.39 samples/s/p  0:26:02 }\n",
      "2023-11-17 09:55:51,536 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  944/ 3750], loss: 2.547, per_step_time: 555ms, lr: 3.771078e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:51,536 - mindformers[callback.py:317] - INFO -   25.2% |████████████                                      | 14.40 samples/s/p  0:25:59 }\n",
      "2023-11-17 09:55:53,763 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  948/ 3750], loss: 3.403, per_step_time: 554ms, lr: 3.7658483e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:53,763 - mindformers[callback.py:317] - INFO -   25.3% |████████████                                      | 14.43 samples/s/p  0:25:53 }\n",
      "2023-11-17 09:55:55,990 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  952/ 3750], loss: 2.268, per_step_time: 554ms, lr: 3.7606183e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:55,990 - mindformers[callback.py:317] - INFO -   25.4% |████████████                                      | 14.43 samples/s/p  0:25:50 }\n",
      "2023-11-17 09:55:58,217 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  956/ 3750], loss: 3.243, per_step_time: 554ms, lr: 3.7553895e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:55:58,218 - mindformers[callback.py:317] - INFO -   25.5% |████████████                                      | 14.43 samples/s/p  0:25:48 }\n",
      "2023-11-17 09:56:00,443 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  960/ 3750], loss: 2.368, per_step_time: 554ms, lr: 3.7501595e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:00,444 - mindformers[callback.py:317] - INFO -   25.6% |████████████                                      | 14.44 samples/s/p  0:25:45 }\n",
      "2023-11-17 09:56:02,670 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  964/ 3750], loss: 2.819, per_step_time: 554ms, lr: 3.7449307e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:02,671 - mindformers[callback.py:317] - INFO -   25.7% |████████████                                      | 14.43 samples/s/p  0:25:44 }\n",
      "2023-11-17 09:56:04,897 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  968/ 3750], loss: 2.891, per_step_time: 554ms, lr: 3.7397007e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:04,898 - mindformers[callback.py:317] - INFO -   25.8% |████████████                                      | 14.44 samples/s/p  0:25:41 }\n",
      "2023-11-17 09:56:07,124 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  972/ 3750], loss: 2.606, per_step_time: 554ms, lr: 3.7344715e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:07,124 - mindformers[callback.py:317] - INFO -   25.9% |████████████                                      | 14.44 samples/s/p  0:25:39 }\n",
      "2023-11-17 09:56:09,351 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  976/ 3750], loss: 3.009, per_step_time: 554ms, lr: 3.729242e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:09,351 - mindformers[callback.py:317] - INFO -   26.0% |█████████████                                     | 14.44 samples/s/p  0:25:37 }\n",
      "2023-11-17 09:56:11,579 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  980/ 3750], loss: 3.006, per_step_time: 554ms, lr: 3.7240126e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:11,579 - mindformers[callback.py:317] - INFO -   26.1% |█████████████                                     | 14.43 samples/s/p  0:25:35 }\n",
      "2023-11-17 09:56:13,808 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  984/ 3750], loss: 3.114, per_step_time: 554ms, lr: 3.718783e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:13,808 - mindformers[callback.py:317] - INFO -   26.2% |█████████████                                     | 14.43 samples/s/p  0:25:33 }\n",
      "2023-11-17 09:56:16,035 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  988/ 3750], loss: 2.265, per_step_time: 554ms, lr: 3.713554e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:16,035 - mindformers[callback.py:317] - INFO -   26.3% |█████████████                                     | 14.43 samples/s/p  0:25:30 }\n",
      "2023-11-17 09:56:18,265 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  992/ 3750], loss: 3.036, per_step_time: 554ms, lr: 3.7083242e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:18,265 - mindformers[callback.py:317] - INFO -   26.5% |█████████████                                     | 14.42 samples/s/p  0:25:30 }\n",
      "2023-11-17 09:56:20,495 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[  996/ 3750], loss: 3.323, per_step_time: 555ms, lr: 3.703095e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:20,495 - mindformers[callback.py:317] - INFO -   26.6% |█████████████                                     | 14.41 samples/s/p  0:25:28 }\n",
      "2023-11-17 09:56:22,729 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1000/ 3750], loss: 2.984, per_step_time: 555ms, lr: 3.6978654e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:56:22,729 - mindformers[callback.py:317] - INFO -   26.7% |█████████████                                     | 14.40 samples/s/p  0:25:28 }\n",
      "2023-11-17 09:56:22,735 - mindformers[callback.py:553] - INFO - ......Saving ckpt......\n",
      "[WARNING] GE_ADPT(17499,ffff9167b0b0,python):2023-11-17-09:56:22.736.131 [mindspore/ccsrc/transform/graph_ir/graph_runner.cc:128] RunGraph] Get graph form DfGraphManager failed!\n",
      "[WARNING] DEVICE(17499,ffff9167b0b0,python):2023-11-17-09:56:22.736.179 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ascend_deprecated_interface.cc:172] DoExecNonInputGraph] Exec graph:save.25767_25758_12920_1_mindspore_train_dataset_helper__DataWrapper_construct_5465 failed\n",
      "2023-11-17 09:58:12,872 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 41220), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.20 seconds.\n",
      "2023-11-17 09:58:15,629 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1004/ 3750], loss: 2.539, per_step_time: 575ms, lr: 3.6926358e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:15,630 - mindformers[callback.py:317] - INFO -   26.8% |█████████████                                     | 13.90 samples/s/p  0:26:20 }\n",
      "2023-11-17 09:58:17,859 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1008/ 3750], loss: 2.843, per_step_time: 554ms, lr: 3.6874062e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:17,859 - mindformers[callback.py:317] - INFO -   26.9% |█████████████                                     | 14.43 samples/s/p  0:25:20 }\n",
      "2023-11-17 09:58:20,087 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1012/ 3750], loss: 2.590, per_step_time: 554ms, lr: 3.682177e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:20,088 - mindformers[callback.py:317] - INFO -   27.0% |█████████████                                     | 14.43 samples/s/p  0:25:18 }\n",
      "2023-11-17 09:58:22,316 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1016/ 3750], loss: 2.891, per_step_time: 554ms, lr: 3.6769474e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:22,316 - mindformers[callback.py:317] - INFO -   27.1% |█████████████                                     | 14.43 samples/s/p  0:25:16 }\n",
      "2023-11-17 09:58:24,544 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1020/ 3750], loss: 3.293, per_step_time: 554ms, lr: 3.6717185e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:24,544 - mindformers[callback.py:317] - INFO -   27.2% |█████████████                                     | 14.43 samples/s/p  0:25:13 }\n",
      "2023-11-17 09:58:26,772 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1024/ 3750], loss: 2.291, per_step_time: 554ms, lr: 3.6664886e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:26,772 - mindformers[callback.py:317] - INFO -   27.3% |█████████████                                     | 14.43 samples/s/p  0:25:11 }\n",
      "2023-11-17 09:58:29,001 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1028/ 3750], loss: 2.718, per_step_time: 554ms, lr: 3.6612593e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:29,001 - mindformers[callback.py:317] - INFO -   27.4% |█████████████                                     | 14.43 samples/s/p  0:25:09 }\n",
      "2023-11-17 09:58:31,229 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1032/ 3750], loss: 2.835, per_step_time: 554ms, lr: 3.6560297e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:31,229 - mindformers[callback.py:317] - INFO -   27.5% |█████████████                                     | 14.43 samples/s/p  0:25:06 }\n",
      "2023-11-17 09:58:33,458 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1036/ 3750], loss: 2.588, per_step_time: 554ms, lr: 3.6507998e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:33,459 - mindformers[callback.py:317] - INFO -   27.6% |█████████████                                     | 14.42 samples/s/p  0:25:05 }\n",
      "2023-11-17 09:58:35,687 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1040/ 3750], loss: 3.303, per_step_time: 554ms, lr: 3.645571e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:35,688 - mindformers[callback.py:317] - INFO -   27.7% |█████████████                                     | 14.43 samples/s/p  0:25:02 }\n",
      "2023-11-17 09:58:37,915 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1044/ 3750], loss: 2.945, per_step_time: 554ms, lr: 3.640341e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:58:37,916 - mindformers[callback.py:317] - INFO -   27.8% |█████████████                                     | 14.43 samples/s/p  0:25:00 }\n",
      "2023-11-17 09:58:40,143 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1048/ 3750], loss: 2.527, per_step_time: 554ms, lr: 3.635112e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 09:58:40,143 - mindformers[callback.py:317] - INFO -   27.9% |█████████████                                     | 14.43 samples/s/p  0:24:57 }\n",
      "2023-11-17 09:58:42,369 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1052/ 3750], loss: 2.614, per_step_time: 553ms, lr: 3.63119e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:42,370 - mindformers[callback.py:317] - INFO -   28.1% |██████████████                                    | 14.44 samples/s/p  0:24:54 }\n",
      "2023-11-17 09:58:44,598 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1056/ 3750], loss: 2.355, per_step_time: 554ms, lr: 3.6259604e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:44,598 - mindformers[callback.py:317] - INFO -   28.2% |██████████████                                    | 14.43 samples/s/p  0:24:53 }\n",
      "2023-11-17 09:58:46,828 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1060/ 3750], loss: 1.846, per_step_time: 554ms, lr: 3.620731e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:46,829 - mindformers[callback.py:317] - INFO -   28.3% |██████████████                                    | 14.42 samples/s/p  0:24:52 }\n",
      "2023-11-17 09:58:49,066 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1064/ 3750], loss: 2.718, per_step_time: 555ms, lr: 3.6155016e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:49,066 - mindformers[callback.py:317] - INFO -   28.4% |██████████████                                    | 14.39 samples/s/p  0:24:53 }\n",
      "2023-11-17 09:58:51,297 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1068/ 3750], loss: 2.396, per_step_time: 555ms, lr: 3.6102716e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:51,298 - mindformers[callback.py:317] - INFO -   28.5% |██████████████                                    | 14.41 samples/s/p  0:24:48 }\n",
      "2023-11-17 09:58:53,536 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1072/ 3750], loss: 2.054, per_step_time: 557ms, lr: 3.6050427e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:53,537 - mindformers[callback.py:317] - INFO -   28.6% |██████████████                                    | 14.36 samples/s/p  0:24:52 }\n",
      "2023-11-17 09:58:55,765 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1076/ 3750], loss: 2.892, per_step_time: 554ms, lr: 3.599813e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:55,765 - mindformers[callback.py:317] - INFO -   28.7% |██████████████                                    | 14.43 samples/s/p  0:24:42 }\n",
      "INFO:root:Copy parallel total time cost: 44.11 seconds.\n",
      "2023-11-17 09:58:57,483 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 44.108332\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 09:58:57,607 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.098506\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 09:58:57,668 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039336\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 09:58:57,711 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020210\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 09:58:57,993 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1080/ 3750], loss: 2.502, per_step_time: 554ms, lr: 3.5945835e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:58:57,994 - mindformers[callback.py:317] - INFO -   28.8% |██████████████                                    | 14.43 samples/s/p  0:24:40 }\n",
      "2023-11-17 09:59:00,222 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1084/ 3750], loss: 2.511, per_step_time: 554ms, lr: 3.5893543e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:00,222 - mindformers[callback.py:317] - INFO -   28.9% |██████████████                                    | 14.43 samples/s/p  0:24:38 }\n",
      "2023-11-17 09:59:02,455 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1088/ 3750], loss: 3.394, per_step_time: 554ms, lr: 3.5841247e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:02,455 - mindformers[callback.py:317] - INFO -   29.0% |██████████████                                    | 14.43 samples/s/p  0:24:35 }\n",
      "2023-11-17 09:59:04,683 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1092/ 3750], loss: 2.292, per_step_time: 554ms, lr: 3.5788955e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:04,683 - mindformers[callback.py:317] - INFO -   29.1% |██████████████                                    | 14.43 samples/s/p  0:24:33 }\n",
      "2023-11-17 09:59:06,912 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1096/ 3750], loss: 2.808, per_step_time: 554ms, lr: 3.573666e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:06,913 - mindformers[callback.py:317] - INFO -   29.2% |██████████████                                    | 14.43 samples/s/p  0:24:31 }\n",
      "2023-11-17 09:59:09,143 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1100/ 3750], loss: 2.806, per_step_time: 554ms, lr: 3.5684367e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:09,144 - mindformers[callback.py:317] - INFO -   29.3% |██████████████                                    | 14.43 samples/s/p  0:24:29 }\n",
      "2023-11-17 09:59:09,150 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 44.19 seconds.\n",
      "2023-11-17 09:59:41,927 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 44.188373\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43610), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 09:59:45,245 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1104/ 3750], loss: 2.421, per_step_time: 569ms, lr: 3.563207e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:45,246 - mindformers[callback.py:317] - INFO -   29.4% |██████████████                                    | 14.06 samples/s/p  0:25:05 }\n",
      "2023-11-17 09:59:47,475 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1108/ 3750], loss: 3.372, per_step_time: 554ms, lr: 3.5579775e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:47,476 - mindformers[callback.py:317] - INFO -   29.5% |██████████████                                    | 14.43 samples/s/p  0:24:24 }\n",
      "2023-11-17 09:59:49,704 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1112/ 3750], loss: 2.652, per_step_time: 554ms, lr: 3.552748e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:49,705 - mindformers[callback.py:317] - INFO -   29.7% |██████████████                                    | 14.43 samples/s/p  0:24:22 }\n",
      "2023-11-17 09:59:51,935 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1116/ 3750], loss: 2.996, per_step_time: 554ms, lr: 3.5475183e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:51,936 - mindformers[callback.py:317] - INFO -   29.8% |██████████████                                    | 14.42 samples/s/p  0:24:21 }\n",
      "2023-11-17 09:59:54,166 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1120/ 3750], loss: 3.210, per_step_time: 555ms, lr: 3.542289e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:54,167 - mindformers[callback.py:317] - INFO -   29.9% |██████████████                                    | 14.41 samples/s/p  0:24:19 }\n",
      "2023-11-17 09:59:56,399 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1124/ 3750], loss: 2.492, per_step_time: 555ms, lr: 3.5370595e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:56,399 - mindformers[callback.py:317] - INFO -   30.0% |██████████████                                    | 14.41 samples/s/p  0:24:18 }\n",
      "2023-11-17 09:59:58,633 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1128/ 3750], loss: 2.608, per_step_time: 555ms, lr: 3.5318302e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 09:59:58,633 - mindformers[callback.py:317] - INFO -   30.1% |███████████████                                   | 14.40 samples/s/p  0:24:17 }\n",
      "2023-11-17 10:00:00,869 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1132/ 3750], loss: 3.125, per_step_time: 556ms, lr: 3.5266006e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:00,869 - mindformers[callback.py:317] - INFO -   30.2% |███████████████                                   | 14.38 samples/s/p  0:24:16 }\n",
      "2023-11-17 10:00:03,098 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1136/ 3750], loss: 2.885, per_step_time: 554ms, lr: 3.5213714e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:03,099 - mindformers[callback.py:317] - INFO -   30.3% |███████████████                                   | 14.42 samples/s/p  0:24:09 }\n",
      "2023-11-17 10:00:05,327 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1140/ 3750], loss: 3.380, per_step_time: 554ms, lr: 3.5161418e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:05,328 - mindformers[callback.py:317] - INFO -   30.4% |███████████████                                   | 14.43 samples/s/p  0:24:07 }\n",
      "2023-11-17 10:00:07,578 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1144/ 3750], loss: 2.883, per_step_time: 559ms, lr: 3.5109126e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:07,579 - mindformers[callback.py:317] - INFO -   30.5% |███████████████                                   | 14.29 samples/s/p  0:24:19 }\n",
      "2023-11-17 10:00:09,807 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1148/ 3750], loss: 2.869, per_step_time: 554ms, lr: 3.505683e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:09,807 - mindformers[callback.py:317] - INFO -   30.6% |███████████████                                   | 14.43 samples/s/p  0:24:02 }\n",
      "2023-11-17 10:00:12,036 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1152/ 3750], loss: 2.264, per_step_time: 554ms, lr: 3.5004534e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:12,036 - mindformers[callback.py:317] - INFO -   30.7% |███████████████                                   | 14.43 samples/s/p  0:24:00 }\n",
      "2023-11-17 10:00:14,264 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1156/ 3750], loss: 2.648, per_step_time: 554ms, lr: 3.495224e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:14,264 - mindformers[callback.py:317] - INFO -   30.8% |███████████████                                   | 14.43 samples/s/p  0:23:57 }\n",
      "2023-11-17 10:00:16,494 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1160/ 3750], loss: 2.680, per_step_time: 554ms, lr: 3.4899946e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:16,494 - mindformers[callback.py:317] - INFO -   30.9% |███████████████                                   | 14.43 samples/s/p  0:23:56 }\n",
      "2023-11-17 10:00:18,727 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1164/ 3750], loss: 2.527, per_step_time: 554ms, lr: 3.4847653e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:18,727 - mindformers[callback.py:317] - INFO -   31.0% |███████████████                                   | 14.43 samples/s/p  0:23:53 }\n",
      "2023-11-17 10:00:20,956 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1168/ 3750], loss: 2.376, per_step_time: 554ms, lr: 3.4795357e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:20,956 - mindformers[callback.py:317] - INFO -   31.1% |███████████████                                   | 14.43 samples/s/p  0:23:51 }\n",
      "2023-11-17 10:00:23,186 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1172/ 3750], loss: 2.502, per_step_time: 554ms, lr: 3.474306e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:23,186 - mindformers[callback.py:317] - INFO -   31.3% |███████████████                                   | 14.42 samples/s/p  0:23:49 }\n",
      "2023-11-17 10:00:25,414 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1176/ 3750], loss: 2.885, per_step_time: 554ms, lr: 3.469077e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:25,414 - mindformers[callback.py:317] - INFO -   31.4% |███████████████                                   | 14.43 samples/s/p  0:23:46 }\n",
      "INFO:root:Copy parallel total time cost: 43.38 seconds.\n",
      "2023-11-17 10:00:26,407 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.380586\n",
      "INFO:root:List OBS time cost: 0.08 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.16 seconds.\n",
      "2023-11-17 10:00:26,595 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.159945\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:00:26,657 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039373\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:00:26,700 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020691\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:00:27,643 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1180/ 3750], loss: 3.314, per_step_time: 554ms, lr: 3.4638473e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:27,643 - mindformers[callback.py:317] - INFO -   31.5% |███████████████                                   | 14.43 samples/s/p  0:23:45 }\n",
      "2023-11-17 10:00:29,876 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1184/ 3750], loss: 2.427, per_step_time: 555ms, lr: 3.4586177e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:29,876 - mindformers[callback.py:317] - INFO -   31.6% |███████████████                                   | 14.40 samples/s/p  0:23:45 }\n",
      "2023-11-17 10:00:32,108 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1188/ 3750], loss: 2.567, per_step_time: 555ms, lr: 3.4533885e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:32,108 - mindformers[callback.py:317] - INFO -   31.7% |███████████████                                   | 14.41 samples/s/p  0:23:42 }\n",
      "2023-11-17 10:00:34,341 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1192/ 3750], loss: 2.962, per_step_time: 555ms, lr: 3.4481585e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:34,341 - mindformers[callback.py:317] - INFO -   31.8% |███████████████                                   | 14.40 samples/s/p  0:23:40 }\n",
      "2023-11-17 10:00:36,572 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1196/ 3750], loss: 2.930, per_step_time: 555ms, lr: 3.4429297e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:36,572 - mindformers[callback.py:317] - INFO -   31.9% |███████████████                                   | 14.41 samples/s/p  0:23:37 }\n",
      "2023-11-17 10:00:38,800 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1200/ 3750], loss: 2.484, per_step_time: 554ms, lr: 3.4376997e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:00:38,801 - mindformers[callback.py:317] - INFO -   32.0% |████████████████                                  | 14.43 samples/s/p  0:23:33 }\n",
      "2023-11-17 10:00:38,807 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 43.49 seconds.\n",
      "2023-11-17 10:01:10,235 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 43.492772\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34870), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:01:13,820 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1204/ 3750], loss: 2.068, per_step_time: 569ms, lr: 3.4324705e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:13,821 - mindformers[callback.py:317] - INFO -   32.1% |████████████████                                  | 14.04 samples/s/p  0:24:10 }\n",
      "2023-11-17 10:01:16,051 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1208/ 3750], loss: 3.548, per_step_time: 554ms, lr: 3.427241e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:16,051 - mindformers[callback.py:317] - INFO -   32.2% |████████████████                                  | 14.42 samples/s/p  0:23:29 }\n",
      "2023-11-17 10:01:18,280 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1212/ 3750], loss: 3.175, per_step_time: 554ms, lr: 3.4220117e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:18,280 - mindformers[callback.py:317] - INFO -   32.3% |████████████████                                  | 14.43 samples/s/p  0:23:27 }\n",
      "2023-11-17 10:01:20,510 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1216/ 3750], loss: 3.009, per_step_time: 554ms, lr: 3.416782e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:20,511 - mindformers[callback.py:317] - INFO -   32.4% |████████████████                                  | 14.43 samples/s/p  0:23:25 }\n",
      "2023-11-17 10:01:22,739 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1220/ 3750], loss: 3.183, per_step_time: 554ms, lr: 3.411553e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:22,740 - mindformers[callback.py:317] - INFO -   32.5% |████████████████                                  | 14.43 samples/s/p  0:23:23 }\n",
      "2023-11-17 10:01:24,968 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1224/ 3750], loss: 2.749, per_step_time: 554ms, lr: 3.4063232e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:24,968 - mindformers[callback.py:317] - INFO -   32.6% |████████████████                                  | 14.43 samples/s/p  0:23:20 }\n",
      "2023-11-17 10:01:27,197 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1228/ 3750], loss: 3.078, per_step_time: 554ms, lr: 3.401094e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:27,197 - mindformers[callback.py:317] - INFO -   32.7% |████████████████                                  | 14.43 samples/s/p  0:23:18 }\n",
      "2023-11-17 10:01:29,429 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1232/ 3750], loss: 1.991, per_step_time: 555ms, lr: 3.3958644e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:29,430 - mindformers[callback.py:317] - INFO -   32.9% |████████████████                                  | 14.41 samples/s/p  0:23:17 }\n",
      "2023-11-17 10:01:31,659 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1236/ 3750], loss: 2.873, per_step_time: 554ms, lr: 3.3906348e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:31,660 - mindformers[callback.py:317] - INFO -   33.0% |████████████████                                  | 14.43 samples/s/p  0:23:13 }\n",
      "2023-11-17 10:01:33,889 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1240/ 3750], loss: 2.928, per_step_time: 554ms, lr: 3.3854056e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:33,889 - mindformers[callback.py:317] - INFO -   33.1% |████████████████                                  | 14.43 samples/s/p  0:23:11 }\n",
      "2023-11-17 10:01:36,119 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1244/ 3750], loss: 2.790, per_step_time: 554ms, lr: 3.380176e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:36,119 - mindformers[callback.py:317] - INFO -   33.2% |████████████████                                  | 14.42 samples/s/p  0:23:09 }\n",
      "2023-11-17 10:01:38,348 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1248/ 3750], loss: 1.649, per_step_time: 554ms, lr: 3.3749468e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:38,348 - mindformers[callback.py:317] - INFO -   33.3% |████████████████                                  | 14.43 samples/s/p  0:23:06 }\n",
      "2023-11-17 10:01:40,577 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1252/ 3750], loss: 3.138, per_step_time: 554ms, lr: 3.369717e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:40,577 - mindformers[callback.py:317] - INFO -   33.4% |████████████████                                  | 14.43 samples/s/p  0:23:04 }\n",
      "2023-11-17 10:01:42,809 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1256/ 3750], loss: 3.002, per_step_time: 555ms, lr: 3.3644876e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:42,810 - mindformers[callback.py:317] - INFO -   33.5% |████████████████                                  | 14.41 samples/s/p  0:23:04 }\n",
      "2023-11-17 10:01:45,041 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1260/ 3750], loss: 2.445, per_step_time: 555ms, lr: 3.3592583e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:45,045 - mindformers[callback.py:317] - INFO -   33.6% |████████████████                                  | 14.41 samples/s/p  0:23:02 }\n",
      "2023-11-17 10:01:47,274 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1264/ 3750], loss: 2.976, per_step_time: 554ms, lr: 3.3540287e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:47,274 - mindformers[callback.py:317] - INFO -   33.7% |████████████████                                  | 14.43 samples/s/p  0:22:58 }\n",
      "2023-11-17 10:01:49,504 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1268/ 3750], loss: 2.985, per_step_time: 554ms, lr: 3.348799e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:49,504 - mindformers[callback.py:317] - INFO -   33.8% |████████████████                                  | 14.43 samples/s/p  0:22:56 }\n",
      "2023-11-17 10:01:51,735 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1272/ 3750], loss: 2.910, per_step_time: 554ms, lr: 3.34357e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:51,735 - mindformers[callback.py:317] - INFO -   33.9% |████████████████                                  | 14.42 samples/s/p  0:22:54 }\n",
      "2023-11-17 10:01:53,967 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1276/ 3750], loss: 2.713, per_step_time: 555ms, lr: 3.33834e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:53,967 - mindformers[callback.py:317] - INFO -   34.0% |█████████████████                                 | 14.41 samples/s/p  0:22:53 }\n",
      "INFO:root:Copy parallel total time cost: 42.87 seconds.\n",
      "2023-11-17 10:01:54,492 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.872364\n",
      "INFO:root:List OBS time cost: 0.10 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.15 seconds.\n",
      "2023-11-17 10:01:54,669 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.149462\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:01:54,733 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039401\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:01:54,778 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020696\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:01:56,202 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1280/ 3750], loss: 2.557, per_step_time: 555ms, lr: 3.333111e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:56,202 - mindformers[callback.py:317] - INFO -   34.1% |█████████████████                                 | 14.40 samples/s/p  0:22:52 }\n",
      "2023-11-17 10:01:58,435 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1284/ 3750], loss: 3.468, per_step_time: 555ms, lr: 3.327881e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:01:58,435 - mindformers[callback.py:317] - INFO -   34.2% |█████████████████                                 | 14.40 samples/s/p  0:22:49 }\n",
      "2023-11-17 10:02:00,664 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1288/ 3750], loss: 3.276, per_step_time: 554ms, lr: 3.322652e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:00,665 - mindformers[callback.py:317] - INFO -   34.3% |█████████████████                                 | 14.43 samples/s/p  0:22:45 }\n",
      "2023-11-17 10:02:02,895 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1292/ 3750], loss: 2.012, per_step_time: 554ms, lr: 3.3174223e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:02,895 - mindformers[callback.py:317] - INFO -   34.5% |█████████████████                                 | 14.42 samples/s/p  0:22:43 }\n",
      "2023-11-17 10:02:05,124 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1296/ 3750], loss: 2.843, per_step_time: 554ms, lr: 3.312193e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:05,125 - mindformers[callback.py:317] - INFO -   34.6% |█████████████████                                 | 14.43 samples/s/p  0:22:40 }\n",
      "2023-11-17 10:02:07,353 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1300/ 3750], loss: 2.881, per_step_time: 554ms, lr: 3.3069635e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:07,354 - mindformers[callback.py:317] - INFO -   34.7% |█████████████████                                 | 14.43 samples/s/p  0:22:38 }\n",
      "2023-11-17 10:02:07,361 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 43.55 seconds.\n",
      "2023-11-17 10:02:38,366 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 43.550670\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 58902), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.10 seconds.\n",
      "2023-11-17 10:02:41,979 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1304/ 3750], loss: 2.750, per_step_time: 569ms, lr: 3.3017343e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:41,980 - mindformers[callback.py:317] - INFO -   34.8% |█████████████████                                 | 14.05 samples/s/p  0:23:12 }\n",
      "2023-11-17 10:02:44,211 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1308/ 3750], loss: 2.046, per_step_time: 554ms, lr: 3.2965047e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:44,211 - mindformers[callback.py:317] - INFO -   34.9% |█████████████████                                 | 14.42 samples/s/p  0:22:34 }\n",
      "2023-11-17 10:02:46,440 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1312/ 3750], loss: 2.923, per_step_time: 554ms, lr: 3.2912754e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:46,441 - mindformers[callback.py:317] - INFO -   35.0% |█████████████████                                 | 14.43 samples/s/p  0:22:31 }\n",
      "2023-11-17 10:02:48,671 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1316/ 3750], loss: 2.336, per_step_time: 554ms, lr: 3.286046e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:48,671 - mindformers[callback.py:317] - INFO -   35.1% |█████████████████                                 | 14.43 samples/s/p  0:22:29 }\n",
      "2023-11-17 10:02:50,900 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1320/ 3750], loss: 2.303, per_step_time: 554ms, lr: 3.2808162e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:50,901 - mindformers[callback.py:317] - INFO -   35.2% |█████████████████                                 | 14.43 samples/s/p  0:22:27 }\n",
      "2023-11-17 10:02:53,131 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1324/ 3750], loss: 2.259, per_step_time: 554ms, lr: 3.275587e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:53,131 - mindformers[callback.py:317] - INFO -   35.3% |█████████████████                                 | 14.42 samples/s/p  0:22:25 }\n",
      "2023-11-17 10:02:55,360 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1328/ 3750], loss: 2.199, per_step_time: 554ms, lr: 3.2703574e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:55,361 - mindformers[callback.py:317] - INFO -   35.4% |█████████████████                                 | 14.43 samples/s/p  0:22:23 }\n",
      "2023-11-17 10:02:57,591 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1332/ 3750], loss: 2.934, per_step_time: 554ms, lr: 3.2651278e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:57,591 - mindformers[callback.py:317] - INFO -   35.5% |█████████████████                                 | 14.43 samples/s/p  0:22:20 }\n",
      "2023-11-17 10:02:59,825 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1336/ 3750], loss: 2.634, per_step_time: 555ms, lr: 3.259899e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:02:59,825 - mindformers[callback.py:317] - INFO -   35.6% |█████████████████                                 | 14.40 samples/s/p  0:22:21 }\n",
      "2023-11-17 10:03:02,060 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1340/ 3750], loss: 3.149, per_step_time: 555ms, lr: 3.254669e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:02,060 - mindformers[callback.py:317] - INFO -   35.7% |█████████████████                                 | 14.40 samples/s/p  0:22:19 }\n",
      "2023-11-17 10:03:04,296 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1344/ 3750], loss: 2.345, per_step_time: 556ms, lr: 3.2494398e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:04,296 - mindformers[callback.py:317] - INFO -   35.8% |█████████████████                                 | 14.38 samples/s/p  0:22:18 }\n",
      "2023-11-17 10:03:06,529 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1348/ 3750], loss: 2.444, per_step_time: 555ms, lr: 3.24421e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:06,529 - mindformers[callback.py:317] - INFO -   35.9% |█████████████████                                 | 14.41 samples/s/p  0:22:13 }\n",
      "2023-11-17 10:03:08,763 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1352/ 3750], loss: 3.037, per_step_time: 555ms, lr: 3.2389806e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:08,763 - mindformers[callback.py:317] - INFO -   36.1% |██████████████████                                | 14.40 samples/s/p  0:22:12 }\n",
      "2023-11-17 10:03:10,992 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1356/ 3750], loss: 2.812, per_step_time: 554ms, lr: 3.2337513e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:10,992 - mindformers[callback.py:317] - INFO -   36.2% |██████████████████                                | 14.43 samples/s/p  0:22:07 }\n",
      "2023-11-17 10:03:13,221 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1360/ 3750], loss: 2.867, per_step_time: 554ms, lr: 3.2285214e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:13,222 - mindformers[callback.py:317] - INFO -   36.3% |██████████████████                                | 14.43 samples/s/p  0:22:04 }\n",
      "2023-11-17 10:03:15,475 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1364/ 3750], loss: 2.330, per_step_time: 560ms, lr: 3.223292e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:15,475 - mindformers[callback.py:317] - INFO -   36.4% |██████████████████                                | 14.27 samples/s/p  0:22:17 }\n",
      "2023-11-17 10:03:17,707 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1368/ 3750], loss: 2.967, per_step_time: 554ms, lr: 3.2180633e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:17,707 - mindformers[callback.py:317] - INFO -   36.5% |██████████████████                                | 14.43 samples/s/p  0:22:00 }\n",
      "2023-11-17 10:03:19,937 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1372/ 3750], loss: 2.395, per_step_time: 554ms, lr: 3.2128333e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:19,937 - mindformers[callback.py:317] - INFO -   36.6% |██████████████████                                | 14.43 samples/s/p  0:21:58 }\n",
      "2023-11-17 10:03:22,167 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1376/ 3750], loss: 2.735, per_step_time: 554ms, lr: 3.2076037e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:22,167 - mindformers[callback.py:317] - INFO -   36.7% |██████████████████                                | 14.43 samples/s/p  0:21:56 }\n",
      "INFO:root:Copy parallel total time cost: 44.19 seconds.\n",
      "2023-11-17 10:03:23,951 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 44.187254\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.13 seconds.\n",
      "2023-11-17 10:03:24,108 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.129882\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:03:24,173 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039796\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:03:24,229 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.022563\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:03:24,412 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1380/ 3750], loss: 1.830, per_step_time: 558ms, lr: 3.2023745e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:24,413 - mindformers[callback.py:317] - INFO -   36.8% |██████████████████                                | 14.32 samples/s/p  0:22:03 }\n",
      "2023-11-17 10:03:26,642 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1384/ 3750], loss: 2.753, per_step_time: 554ms, lr: 3.197145e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:26,643 - mindformers[callback.py:317] - INFO -   36.9% |██████████████████                                | 14.43 samples/s/p  0:21:52 }\n",
      "2023-11-17 10:03:28,873 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1388/ 3750], loss: 2.327, per_step_time: 554ms, lr: 3.1919157e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:28,873 - mindformers[callback.py:317] - INFO -   37.0% |██████████████████                                | 14.43 samples/s/p  0:21:49 }\n",
      "2023-11-17 10:03:31,104 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1392/ 3750], loss: 2.967, per_step_time: 554ms, lr: 3.186686e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:31,104 - mindformers[callback.py:317] - INFO -   37.1% |██████████████████                                | 14.42 samples/s/p  0:21:47 }\n",
      "2023-11-17 10:03:33,335 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1396/ 3750], loss: 2.731, per_step_time: 554ms, lr: 3.1814565e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:33,336 - mindformers[callback.py:317] - INFO -   37.2% |██████████████████                                | 14.42 samples/s/p  0:21:45 }\n",
      "2023-11-17 10:03:35,565 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1400/ 3750], loss: 3.037, per_step_time: 554ms, lr: 3.1762273e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:03:35,566 - mindformers[callback.py:317] - INFO -   37.3% |██████████████████                                | 14.43 samples/s/p  0:21:43 }\n",
      "2023-11-17 10:03:35,573 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 44.34 seconds.\n",
      "2023-11-17 10:04:08,596 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 44.342058\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 44150), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:04:12,217 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1404/ 3750], loss: 2.600, per_step_time: 569ms, lr: 3.1709977e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:12,218 - mindformers[callback.py:317] - INFO -   37.4% |██████████████████                                | 14.04 samples/s/p  0:22:17 }\n",
      "2023-11-17 10:04:14,448 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1408/ 3750], loss: 2.683, per_step_time: 554ms, lr: 3.1657684e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:14,448 - mindformers[callback.py:317] - INFO -   37.5% |██████████████████                                | 14.43 samples/s/p  0:21:38 }\n",
      "2023-11-17 10:04:16,680 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1412/ 3750], loss: 2.497, per_step_time: 554ms, lr: 3.1605392e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:16,680 - mindformers[callback.py:317] - INFO -   37.7% |██████████████████                                | 14.41 samples/s/p  0:21:37 }\n",
      "2023-11-17 10:04:18,915 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1416/ 3750], loss: 2.599, per_step_time: 555ms, lr: 3.1553092e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:18,916 - mindformers[callback.py:317] - INFO -   37.8% |██████████████████                                | 14.39 samples/s/p  0:21:37 }\n",
      "2023-11-17 10:04:21,156 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1420/ 3750], loss: 2.343, per_step_time: 557ms, lr: 3.15008e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:21,157 - mindformers[callback.py:317] - INFO -   37.9% |██████████████████                                | 14.36 samples/s/p  0:21:38 }\n",
      "2023-11-17 10:04:23,388 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1424/ 3750], loss: 3.833, per_step_time: 554ms, lr: 3.1448504e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:23,388 - mindformers[callback.py:317] - INFO -   38.0% |██████████████████                                | 14.41 samples/s/p  0:21:30 }\n",
      "2023-11-17 10:04:25,617 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1428/ 3750], loss: 2.488, per_step_time: 554ms, lr: 3.139621e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:25,618 - mindformers[callback.py:317] - INFO -   38.1% |███████████████████                               | 14.43 samples/s/p  0:21:27 }\n",
      "2023-11-17 10:04:27,847 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1432/ 3750], loss: 1.850, per_step_time: 554ms, lr: 3.1343916e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:27,847 - mindformers[callback.py:317] - INFO -   38.2% |███████████████████                               | 14.43 samples/s/p  0:21:25 }\n",
      "2023-11-17 10:04:30,079 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1436/ 3750], loss: 2.825, per_step_time: 555ms, lr: 3.129162e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:30,080 - mindformers[callback.py:317] - INFO -   38.3% |███████████████████                               | 14.41 samples/s/p  0:21:24 }\n",
      "2023-11-17 10:04:32,309 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1440/ 3750], loss: 3.500, per_step_time: 554ms, lr: 3.1239328e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:32,310 - mindformers[callback.py:317] - INFO -   38.4% |███████████████████                               | 14.43 samples/s/p  0:21:20 }\n",
      "2023-11-17 10:04:34,540 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1444/ 3750], loss: 2.886, per_step_time: 554ms, lr: 3.1187028e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:34,540 - mindformers[callback.py:317] - INFO -   38.5% |███████████████████                               | 14.43 samples/s/p  0:21:18 }\n",
      "2023-11-17 10:04:36,770 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1448/ 3750], loss: 2.397, per_step_time: 554ms, lr: 3.1134736e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:36,771 - mindformers[callback.py:317] - INFO -   38.6% |███████████████████                               | 14.43 samples/s/p  0:21:16 }\n",
      "2023-11-17 10:04:39,000 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1452/ 3750], loss: 2.188, per_step_time: 554ms, lr: 3.108244e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:39,000 - mindformers[callback.py:317] - INFO -   38.7% |███████████████████                               | 14.44 samples/s/p  0:21:13 }\n",
      "2023-11-17 10:04:41,229 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1456/ 3750], loss: 2.597, per_step_time: 554ms, lr: 3.1030148e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:41,230 - mindformers[callback.py:317] - INFO -   38.8% |███████████████████                               | 14.43 samples/s/p  0:21:11 }\n",
      "2023-11-17 10:04:43,459 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1460/ 3750], loss: 3.150, per_step_time: 554ms, lr: 3.097785e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:43,460 - mindformers[callback.py:317] - INFO -   38.9% |███████████████████                               | 14.43 samples/s/p  0:21:09 }\n",
      "2023-11-17 10:04:45,688 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1464/ 3750], loss: 2.654, per_step_time: 554ms, lr: 3.092556e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:45,688 - mindformers[callback.py:317] - INFO -   39.0% |███████████████████                               | 14.44 samples/s/p  0:21:06 }\n",
      "2023-11-17 10:04:47,916 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1468/ 3750], loss: 3.364, per_step_time: 554ms, lr: 3.0873263e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:47,917 - mindformers[callback.py:317] - INFO -   39.1% |███████████████████                               | 14.44 samples/s/p  0:21:04 }\n",
      "2023-11-17 10:04:50,147 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1472/ 3750], loss: 2.624, per_step_time: 554ms, lr: 3.082097e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:50,147 - mindformers[callback.py:317] - INFO -   39.3% |███████████████████                               | 14.43 samples/s/p  0:21:03 }\n",
      "2023-11-17 10:04:52,377 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1476/ 3750], loss: 2.243, per_step_time: 554ms, lr: 3.0768675e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:52,377 - mindformers[callback.py:317] - INFO -   39.4% |███████████████████                               | 14.43 samples/s/p  0:21:00 }\n",
      "INFO:root:Copy parallel total time cost: 43.57 seconds.\n",
      "2023-11-17 10:04:53,559 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.571390\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.11 seconds.\n",
      "2023-11-17 10:04:53,693 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.109680\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:04:53,762 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.043361\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:04:53,808 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.021298\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:04:54,610 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1480/ 3750], loss: 2.529, per_step_time: 555ms, lr: 3.071638e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:54,611 - mindformers[callback.py:317] - INFO -   39.5% |███████████████████                               | 14.41 samples/s/p  0:21:00 }\n",
      "2023-11-17 10:04:56,845 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1484/ 3750], loss: 3.038, per_step_time: 555ms, lr: 3.0664087e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:56,846 - mindformers[callback.py:317] - INFO -   39.6% |███████████████████                               | 14.40 samples/s/p  0:20:58 }\n",
      "2023-11-17 10:04:59,079 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1488/ 3750], loss: 2.338, per_step_time: 555ms, lr: 3.061179e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:04:59,080 - mindformers[callback.py:317] - INFO -   39.7% |███████████████████                               | 14.41 samples/s/p  0:20:55 }\n",
      "2023-11-17 10:05:01,310 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1492/ 3750], loss: 2.402, per_step_time: 554ms, lr: 3.05595e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:01,310 - mindformers[callback.py:317] - INFO -   39.8% |███████████████████                               | 14.43 samples/s/p  0:20:51 }\n",
      "2023-11-17 10:05:03,540 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1496/ 3750], loss: 2.884, per_step_time: 554ms, lr: 3.0507203e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:03,541 - mindformers[callback.py:317] - INFO -   39.9% |███████████████████                               | 14.43 samples/s/p  0:20:49 }\n",
      "2023-11-17 10:05:05,769 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1500/ 3750], loss: 2.435, per_step_time: 554ms, lr: 3.0454908e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:05,770 - mindformers[callback.py:317] - INFO -   40.0% |████████████████████                              | 14.44 samples/s/p  0:20:46 }\n",
      "2023-11-17 10:05:05,778 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 43.44 seconds.\n",
      "2023-11-17 10:05:37,288 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 43.441946\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 35220), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:05:40,872 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1504/ 3750], loss: 3.026, per_step_time: 568ms, lr: 3.0402614e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:40,873 - mindformers[callback.py:317] - INFO -   40.1% |████████████████████                              | 14.06 samples/s/p  0:21:17 }\n",
      "2023-11-17 10:05:43,104 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1508/ 3750], loss: 2.623, per_step_time: 554ms, lr: 3.035032e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:43,105 - mindformers[callback.py:317] - INFO -   40.2% |████████████████████                              | 14.43 samples/s/p  0:20:42 }\n",
      "2023-11-17 10:05:45,334 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1512/ 3750], loss: 2.859, per_step_time: 554ms, lr: 3.0298024e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:45,335 - mindformers[callback.py:317] - INFO -   40.3% |████████████████████                              | 14.43 samples/s/p  0:20:40 }\n",
      "2023-11-17 10:05:47,564 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1516/ 3750], loss: 2.609, per_step_time: 554ms, lr: 3.0245732e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:47,564 - mindformers[callback.py:317] - INFO -   40.4% |████████████████████                              | 14.44 samples/s/p  0:20:38 }\n",
      "2023-11-17 10:05:49,794 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1520/ 3750], loss: 3.139, per_step_time: 554ms, lr: 3.0193436e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:49,795 - mindformers[callback.py:317] - INFO -   40.5% |████████████████████                              | 14.43 samples/s/p  0:20:36 }\n",
      "2023-11-17 10:05:52,025 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1524/ 3750], loss: 1.865, per_step_time: 554ms, lr: 3.0141142e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:52,026 - mindformers[callback.py:317] - INFO -   40.6% |████████████████████                              | 14.43 samples/s/p  0:20:34 }\n",
      "2023-11-17 10:05:54,257 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1528/ 3750], loss: 2.639, per_step_time: 554ms, lr: 3.0088848e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:54,257 - mindformers[callback.py:317] - INFO -   40.7% |████████████████████                              | 14.43 samples/s/p  0:20:32 }\n",
      "2023-11-17 10:05:56,487 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1532/ 3750], loss: 2.841, per_step_time: 554ms, lr: 3.003655e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:56,488 - mindformers[callback.py:317] - INFO -   40.9% |████████████████████                              | 14.43 samples/s/p  0:20:29 }\n",
      "2023-11-17 10:05:58,717 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1536/ 3750], loss: 2.934, per_step_time: 554ms, lr: 2.9984256e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:05:58,717 - mindformers[callback.py:317] - INFO -   41.0% |████████████████████                              | 14.44 samples/s/p  0:20:26 }\n",
      "2023-11-17 10:06:00,947 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1540/ 3750], loss: 2.478, per_step_time: 554ms, lr: 2.9931962e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:00,947 - mindformers[callback.py:317] - INFO -   41.1% |████████████████████                              | 14.44 samples/s/p  0:20:24 }\n",
      "2023-11-17 10:06:03,180 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1544/ 3750], loss: 2.870, per_step_time: 555ms, lr: 2.9879668e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:03,181 - mindformers[callback.py:317] - INFO -   41.2% |████████████████████                              | 14.41 samples/s/p  0:20:24 }\n",
      "2023-11-17 10:06:05,412 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1548/ 3750], loss: 2.505, per_step_time: 554ms, lr: 2.9827377e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:05,413 - mindformers[callback.py:317] - INFO -   41.3% |████████████████████                              | 14.42 samples/s/p  0:20:21 }\n",
      "2023-11-17 10:06:07,649 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1552/ 3750], loss: 2.924, per_step_time: 555ms, lr: 2.977508e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:07,649 - mindformers[callback.py:317] - INFO -   41.4% |████████████████████                              | 14.39 samples/s/p  0:20:21 }\n",
      "2023-11-17 10:06:09,883 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1556/ 3750], loss: 2.871, per_step_time: 555ms, lr: 2.9722785e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:09,884 - mindformers[callback.py:317] - INFO -   41.5% |████████████████████                              | 14.41 samples/s/p  0:20:18 }\n",
      "2023-11-17 10:06:12,113 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1560/ 3750], loss: 2.338, per_step_time: 554ms, lr: 2.9670491e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:12,113 - mindformers[callback.py:317] - INFO -   41.6% |████████████████████                              | 14.44 samples/s/p  0:20:13 }\n",
      "2023-11-17 10:06:14,368 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1564/ 3750], loss: 2.647, per_step_time: 560ms, lr: 2.9618193e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:14,369 - mindformers[callback.py:317] - INFO -   41.7% |████████████████████                              | 14.27 samples/s/p  0:20:25 }\n",
      "2023-11-17 10:06:16,602 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1568/ 3750], loss: 2.741, per_step_time: 555ms, lr: 2.95659e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:16,603 - mindformers[callback.py:317] - INFO -   41.8% |████████████████████                              | 14.41 samples/s/p  0:20:11 }\n",
      "2023-11-17 10:06:18,836 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1572/ 3750], loss: 2.534, per_step_time: 555ms, lr: 2.9513605e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:18,836 - mindformers[callback.py:317] - INFO -   41.9% |████████████████████                              | 14.41 samples/s/p  0:20:09 }\n",
      "2023-11-17 10:06:21,069 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1576/ 3750], loss: 3.426, per_step_time: 555ms, lr: 2.9461311e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:21,070 - mindformers[callback.py:317] - INFO -   42.0% |█████████████████████                             | 14.41 samples/s/p  0:20:07 }\n",
      "INFO:root:Copy parallel total time cost: 43.60 seconds.\n",
      "2023-11-17 10:06:22,260 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.603648\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.09 seconds.\n",
      "2023-11-17 10:06:22,378 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.094425\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:06:22,445 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.042363\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:06:22,497 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.023173\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:06:23,300 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1580/ 3750], loss: 2.279, per_step_time: 554ms, lr: 2.940902e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:23,301 - mindformers[callback.py:317] - INFO -   42.1% |█████████████████████                             | 14.43 samples/s/p  0:20:03 }\n",
      "2023-11-17 10:06:25,532 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1584/ 3750], loss: 2.121, per_step_time: 554ms, lr: 2.9356723e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:25,532 - mindformers[callback.py:317] - INFO -   42.2% |█████████████████████                             | 14.42 samples/s/p  0:20:01 }\n",
      "2023-11-17 10:06:27,765 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1588/ 3750], loss: 3.172, per_step_time: 554ms, lr: 2.9304429e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:27,765 - mindformers[callback.py:317] - INFO -   42.3% |█████████████████████                             | 14.43 samples/s/p  0:19:58 }\n",
      "2023-11-17 10:06:29,995 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1592/ 3750], loss: 2.698, per_step_time: 554ms, lr: 2.9252134e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:29,996 - mindformers[callback.py:317] - INFO -   42.5% |█████████████████████                             | 14.43 samples/s/p  0:19:56 }\n",
      "2023-11-17 10:06:32,226 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1596/ 3750], loss: 3.342, per_step_time: 554ms, lr: 2.9199839e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:32,226 - mindformers[callback.py:317] - INFO -   42.6% |█████████████████████                             | 14.43 samples/s/p  0:19:54 }\n",
      "2023-11-17 10:06:34,456 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1600/ 3750], loss: 2.904, per_step_time: 554ms, lr: 2.9147544e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:06:34,457 - mindformers[callback.py:317] - INFO -   42.7% |█████████████████████                             | 14.43 samples/s/p  0:19:52 }\n",
      "2023-11-17 10:06:34,465 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 43.23 seconds.\n",
      "2023-11-17 10:07:05,758 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 43.231230\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43770), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.13 seconds.\n",
      "2023-11-17 10:07:09,341 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1604/ 3750], loss: 2.680, per_step_time: 570ms, lr: 2.909525e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:09,342 - mindformers[callback.py:317] - INFO -   42.8% |█████████████████████                             | 14.02 samples/s/p  0:20:24 }\n",
      "2023-11-17 10:07:11,576 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1608/ 3750], loss: 2.410, per_step_time: 554ms, lr: 2.9042953e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:11,576 - mindformers[callback.py:317] - INFO -   42.9% |█████████████████████                             | 14.42 samples/s/p  0:19:48 }\n",
      "2023-11-17 10:07:13,810 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1612/ 3750], loss: 2.508, per_step_time: 555ms, lr: 2.8990664e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:13,811 - mindformers[callback.py:317] - INFO -   43.0% |█████████████████████                             | 14.40 samples/s/p  0:19:47 }\n",
      "2023-11-17 10:07:16,040 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1616/ 3750], loss: 2.567, per_step_time: 554ms, lr: 2.8938364e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:16,041 - mindformers[callback.py:317] - INFO -   43.1% |█████████████████████                             | 14.43 samples/s/p  0:19:43 }\n",
      "2023-11-17 10:07:18,274 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1620/ 3750], loss: 3.054, per_step_time: 555ms, lr: 2.888607e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:18,274 - mindformers[callback.py:317] - INFO -   43.2% |█████████████████████                             | 14.41 samples/s/p  0:19:42 }\n",
      "2023-11-17 10:07:20,513 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1624/ 3750], loss: 3.560, per_step_time: 556ms, lr: 2.883378e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:20,513 - mindformers[callback.py:317] - INFO -   43.3% |█████████████████████                             | 14.38 samples/s/p  0:19:43 }\n",
      "2023-11-17 10:07:22,743 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1628/ 3750], loss: 2.765, per_step_time: 554ms, lr: 2.8781482e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:22,744 - mindformers[callback.py:317] - INFO -   43.4% |█████████████████████                             | 14.43 samples/s/p  0:19:36 }\n",
      "2023-11-17 10:07:24,974 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1632/ 3750], loss: 2.380, per_step_time: 554ms, lr: 2.8729188e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:24,975 - mindformers[callback.py:317] - INFO -   43.5% |█████████████████████                             | 14.43 samples/s/p  0:19:34 }\n",
      "2023-11-17 10:07:27,205 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1636/ 3750], loss: 2.681, per_step_time: 554ms, lr: 2.8676894e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:27,205 - mindformers[callback.py:317] - INFO -   43.6% |█████████████████████                             | 14.43 samples/s/p  0:19:32 }\n",
      "2023-11-17 10:07:29,436 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1640/ 3750], loss: 2.138, per_step_time: 554ms, lr: 2.8624596e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:29,436 - mindformers[callback.py:317] - INFO -   43.7% |█████████████████████                             | 14.43 samples/s/p  0:19:29 }\n",
      "2023-11-17 10:07:31,666 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1644/ 3750], loss: 3.353, per_step_time: 554ms, lr: 2.8572305e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:31,667 - mindformers[callback.py:317] - INFO -   43.8% |█████████████████████                             | 14.43 samples/s/p  0:19:27 }\n",
      "2023-11-17 10:07:33,897 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1648/ 3750], loss: 3.017, per_step_time: 554ms, lr: 2.8520008e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:33,897 - mindformers[callback.py:317] - INFO -   43.9% |█████████████████████                             | 14.43 samples/s/p  0:19:25 }\n",
      "2023-11-17 10:07:36,131 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1652/ 3750], loss: 2.757, per_step_time: 555ms, lr: 2.8467717e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:36,132 - mindformers[callback.py:317] - INFO -   44.1% |██████████████████████                            | 14.40 samples/s/p  0:19:25 }\n",
      "2023-11-17 10:07:38,362 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1656/ 3750], loss: 2.552, per_step_time: 554ms, lr: 2.8415423e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:38,363 - mindformers[callback.py:317] - INFO -   44.2% |██████████████████████                            | 14.43 samples/s/p  0:19:20 }\n",
      "2023-11-17 10:07:40,594 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1660/ 3750], loss: 3.014, per_step_time: 554ms, lr: 2.8363125e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:40,594 - mindformers[callback.py:317] - INFO -   44.3% |██████████████████████                            | 14.43 samples/s/p  0:19:18 }\n",
      "2023-11-17 10:07:42,825 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1664/ 3750], loss: 3.039, per_step_time: 554ms, lr: 2.8310831e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:42,826 - mindformers[callback.py:317] - INFO -   44.4% |██████████████████████                            | 14.43 samples/s/p  0:19:16 }\n",
      "2023-11-17 10:07:45,057 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1668/ 3750], loss: 2.781, per_step_time: 554ms, lr: 2.8258537e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:45,058 - mindformers[callback.py:317] - INFO -   44.5% |██████████████████████                            | 14.42 samples/s/p  0:19:14 }\n",
      "2023-11-17 10:07:47,288 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1672/ 3750], loss: 2.366, per_step_time: 554ms, lr: 2.8206241e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:47,288 - mindformers[callback.py:317] - INFO -   44.6% |██████████████████████                            | 14.43 samples/s/p  0:19:11 }\n",
      "INFO:root:Copy parallel total time cost: 41.43 seconds.\n",
      "2023-11-17 10:07:48,543 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 41.434774\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:07:48,673 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.101265\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:07:48,739 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.040980\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:07:48,783 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020644\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:07:49,519 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1676/ 3750], loss: 2.799, per_step_time: 554ms, lr: 2.8153949e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:49,520 - mindformers[callback.py:317] - INFO -   44.7% |██████████████████████                            | 14.43 samples/s/p  0:19:10 }\n",
      "2023-11-17 10:07:51,750 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1680/ 3750], loss: 2.958, per_step_time: 554ms, lr: 2.8101653e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:51,751 - mindformers[callback.py:317] - INFO -   44.8% |██████████████████████                            | 14.43 samples/s/p  0:19:07 }\n",
      "2023-11-17 10:07:53,986 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1684/ 3750], loss: 2.630, per_step_time: 555ms, lr: 2.804936e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:53,986 - mindformers[callback.py:317] - INFO -   44.9% |██████████████████████                            | 14.40 samples/s/p  0:19:07 }\n",
      "2023-11-17 10:07:56,220 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1688/ 3750], loss: 2.700, per_step_time: 555ms, lr: 2.7997066e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:56,220 - mindformers[callback.py:317] - INFO -   45.0% |██████████████████████                            | 14.41 samples/s/p  0:19:04 }\n",
      "2023-11-17 10:07:58,455 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1692/ 3750], loss: 3.444, per_step_time: 555ms, lr: 2.7944767e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:07:58,455 - mindformers[callback.py:317] - INFO -   45.1% |██████████████████████                            | 14.40 samples/s/p  0:19:03 }\n",
      "2023-11-17 10:08:00,689 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1696/ 3750], loss: 2.192, per_step_time: 555ms, lr: 2.7892473e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:00,690 - mindformers[callback.py:317] - INFO -   45.2% |██████████████████████                            | 14.40 samples/s/p  0:19:00 }\n",
      "2023-11-17 10:08:02,927 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1700/ 3750], loss: 3.135, per_step_time: 556ms, lr: 2.7840182e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:02,928 - mindformers[callback.py:317] - INFO -   45.3% |██████████████████████                            | 14.39 samples/s/p  0:18:59 }\n",
      "2023-11-17 10:08:02,936 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 41.36 seconds.\n",
      "2023-11-17 10:08:30,163 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 41.355935\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 44074), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:08:33,824 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1704/ 3750], loss: 2.186, per_step_time: 571ms, lr: 2.7787884e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:33,825 - mindformers[callback.py:317] - INFO -   45.4% |██████████████████████                            | 13.99 samples/s/p  0:19:30 }\n",
      "2023-11-17 10:08:36,057 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1708/ 3750], loss: 2.501, per_step_time: 554ms, lr: 2.7735594e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:36,057 - mindformers[callback.py:317] - INFO -   45.5% |██████████████████████                            | 14.43 samples/s/p  0:18:52 }\n",
      "2023-11-17 10:08:38,306 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1712/ 3750], loss: 2.595, per_step_time: 554ms, lr: 2.7683296e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:38,306 - mindformers[callback.py:317] - INFO -   45.7% |██████████████████████                            | 14.43 samples/s/p  0:18:49 }\n",
      "2023-11-17 10:08:40,537 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1716/ 3750], loss: 2.134, per_step_time: 554ms, lr: 2.7631002e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:40,537 - mindformers[callback.py:317] - INFO -   45.8% |██████████████████████                            | 14.43 samples/s/p  0:18:47 }\n",
      "2023-11-17 10:08:42,788 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1720/ 3750], loss: 2.669, per_step_time: 559ms, lr: 2.7578708e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:42,789 - mindformers[callback.py:317] - INFO -   45.9% |██████████████████████                            | 14.30 samples/s/p  0:18:56 }\n",
      "2023-11-17 10:08:45,020 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1724/ 3750], loss: 2.130, per_step_time: 554ms, lr: 2.752641e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:45,021 - mindformers[callback.py:317] - INFO -   46.0% |██████████████████████                            | 14.42 samples/s/p  0:18:43 }\n",
      "2023-11-17 10:08:47,251 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1728/ 3750], loss: 2.101, per_step_time: 554ms, lr: 2.747412e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:47,252 - mindformers[callback.py:317] - INFO -   46.1% |███████████████████████                           | 14.43 samples/s/p  0:18:41 }\n",
      "2023-11-17 10:08:49,483 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1732/ 3750], loss: 2.557, per_step_time: 554ms, lr: 2.7421826e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:49,484 - mindformers[callback.py:317] - INFO -   46.2% |███████████████████████                           | 14.43 samples/s/p  0:18:39 }\n",
      "2023-11-17 10:08:51,716 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1736/ 3750], loss: 3.638, per_step_time: 554ms, lr: 2.7369531e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:51,717 - mindformers[callback.py:317] - INFO -   46.3% |███████████████████████                           | 14.42 samples/s/p  0:18:37 }\n",
      "2023-11-17 10:08:53,949 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1740/ 3750], loss: 3.211, per_step_time: 554ms, lr: 2.7317237e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:53,949 - mindformers[callback.py:317] - INFO -   46.4% |███████████████████████                           | 14.42 samples/s/p  0:18:34 }\n",
      "2023-11-17 10:08:56,182 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1744/ 3750], loss: 2.370, per_step_time: 555ms, lr: 2.726494e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:56,183 - mindformers[callback.py:317] - INFO -   46.5% |███████████████████████                           | 14.41 samples/s/p  0:18:33 }\n",
      "2023-11-17 10:08:58,414 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1748/ 3750], loss: 2.396, per_step_time: 554ms, lr: 2.7212645e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:08:58,415 - mindformers[callback.py:317] - INFO -   46.6% |███████████████████████                           | 14.42 samples/s/p  0:18:30 }\n",
      "2023-11-17 10:09:00,646 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1752/ 3750], loss: 2.947, per_step_time: 554ms, lr: 2.7160351e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:00,646 - mindformers[callback.py:317] - INFO -   46.7% |███████████████████████                           | 14.43 samples/s/p  0:18:27 }\n",
      "2023-11-17 10:09:02,882 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1756/ 3750], loss: 2.868, per_step_time: 555ms, lr: 2.7108055e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:02,882 - mindformers[callback.py:317] - INFO -   46.8% |███████████████████████                           | 14.40 samples/s/p  0:18:28 }\n",
      "2023-11-17 10:09:05,122 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1760/ 3750], loss: 2.645, per_step_time: 556ms, lr: 2.7055763e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:05,122 - mindformers[callback.py:317] - INFO -   46.9% |███████████████████████                           | 14.37 samples/s/p  0:18:27 }\n",
      "2023-11-17 10:09:07,360 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1764/ 3750], loss: 2.855, per_step_time: 556ms, lr: 2.7003469e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:07,360 - mindformers[callback.py:317] - INFO -   47.0% |███████████████████████                           | 14.38 samples/s/p  0:18:24 }\n",
      "2023-11-17 10:09:09,591 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1768/ 3750], loss: 4.162, per_step_time: 554ms, lr: 2.6951175e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:09,591 - mindformers[callback.py:317] - INFO -   47.1% |███████████████████████                           | 14.43 samples/s/p  0:18:18 }\n",
      "2023-11-17 10:09:11,822 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1772/ 3750], loss: 3.008, per_step_time: 554ms, lr: 2.689888e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:11,822 - mindformers[callback.py:317] - INFO -   47.3% |███████████████████████                           | 14.43 samples/s/p  0:18:16 }\n",
      "INFO:root:Copy parallel total time cost: 42.29 seconds.\n",
      "2023-11-17 10:09:13,888 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.289949\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.11 seconds.\n",
      "2023-11-17 10:09:14,023 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.108879\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:09:14,052 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1776/ 3750], loss: 2.848, per_step_time: 554ms, lr: 2.6846585e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:14,053 - mindformers[callback.py:317] - INFO -   47.4% |███████████████████████                           | 14.43 samples/s/p  0:18:14 }\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:09:14,102 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.041909\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:09:14,158 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.026232\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:09:16,283 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1780/ 3750], loss: 2.759, per_step_time: 554ms, lr: 2.679429e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:16,283 - mindformers[callback.py:317] - INFO -   47.5% |███████████████████████                           | 14.43 samples/s/p  0:18:12 }\n",
      "2023-11-17 10:09:18,515 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1784/ 3750], loss: 3.321, per_step_time: 554ms, lr: 2.6741996e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:18,515 - mindformers[callback.py:317] - INFO -   47.6% |███████████████████████                           | 14.42 samples/s/p  0:18:10 }\n",
      "2023-11-17 10:09:20,746 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1788/ 3750], loss: 2.609, per_step_time: 554ms, lr: 2.6689699e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:20,747 - mindformers[callback.py:317] - INFO -   47.7% |███████████████████████                           | 14.43 samples/s/p  0:18:07 }\n",
      "2023-11-17 10:09:22,979 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1792/ 3750], loss: 2.501, per_step_time: 554ms, lr: 2.6637408e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:22,980 - mindformers[callback.py:317] - INFO -   47.8% |███████████████████████                           | 14.42 samples/s/p  0:18:06 }\n",
      "2023-11-17 10:09:25,212 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1796/ 3750], loss: 3.194, per_step_time: 554ms, lr: 2.658511e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:25,213 - mindformers[callback.py:317] - INFO -   47.9% |███████████████████████                           | 14.42 samples/s/p  0:18:04 }\n",
      "2023-11-17 10:09:27,447 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1800/ 3750], loss: 2.383, per_step_time: 555ms, lr: 2.6532816e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:09:27,447 - mindformers[callback.py:317] - INFO -   48.0% |████████████████████████                          | 14.41 samples/s/p  0:18:02 }\n",
      "2023-11-17 10:09:27,456 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.54 seconds.\n",
      "2023-11-17 10:09:56,726 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.541883\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 53116), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.12 seconds.\n",
      "2023-11-17 10:10:00,337 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1804/ 3750], loss: 2.640, per_step_time: 571ms, lr: 2.6480522e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:00,338 - mindformers[callback.py:317] - INFO -   48.1% |████████████████████████                          | 13.99 samples/s/p  0:18:32 }\n",
      "2023-11-17 10:10:02,580 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1808/ 3750], loss: 2.984, per_step_time: 556ms, lr: 2.6428228e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:02,581 - mindformers[callback.py:317] - INFO -   48.2% |████████████████████████                          | 14.36 samples/s/p  0:18:01 }\n",
      "2023-11-17 10:10:04,812 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1812/ 3750], loss: 2.422, per_step_time: 554ms, lr: 2.6375934e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:04,813 - mindformers[callback.py:317] - INFO -   48.3% |████████████████████████                          | 14.43 samples/s/p  0:17:54 }\n",
      "2023-11-17 10:10:07,044 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1816/ 3750], loss: 2.616, per_step_time: 554ms, lr: 2.632364e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:07,045 - mindformers[callback.py:317] - INFO -   48.4% |████████████████████████                          | 14.43 samples/s/p  0:17:52 }\n",
      "2023-11-17 10:10:09,300 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1820/ 3750], loss: 2.452, per_step_time: 560ms, lr: 2.6271342e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:09,300 - mindformers[callback.py:317] - INFO -   48.5% |████████████████████████                          | 14.28 samples/s/p  0:18:01 }\n",
      "2023-11-17 10:10:11,531 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1824/ 3750], loss: 3.265, per_step_time: 554ms, lr: 2.6219052e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:11,532 - mindformers[callback.py:317] - INFO -   48.6% |████████████████████████                          | 14.43 samples/s/p  0:17:47 }\n",
      "2023-11-17 10:10:13,763 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1828/ 3750], loss: 2.979, per_step_time: 554ms, lr: 2.6166754e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:13,763 - mindformers[callback.py:317] - INFO -   48.7% |████████████████████████                          | 14.43 samples/s/p  0:17:45 }\n",
      "2023-11-17 10:10:15,995 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1832/ 3750], loss: 3.062, per_step_time: 554ms, lr: 2.611446e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:15,996 - mindformers[callback.py:317] - INFO -   48.9% |████████████████████████                          | 14.42 samples/s/p  0:17:43 }\n",
      "2023-11-17 10:10:18,228 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1836/ 3750], loss: 2.903, per_step_time: 554ms, lr: 2.6062165e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:18,228 - mindformers[callback.py:317] - INFO -   49.0% |████████████████████████                          | 14.42 samples/s/p  0:17:41 }\n",
      "2023-11-17 10:10:20,462 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1840/ 3750], loss: 4.082, per_step_time: 554ms, lr: 2.6009871e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:20,462 - mindformers[callback.py:317] - INFO -   49.1% |████████████████████████                          | 14.42 samples/s/p  0:17:39 }\n",
      "2023-11-17 10:10:22,694 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1844/ 3750], loss: 2.996, per_step_time: 554ms, lr: 2.5957577e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:22,695 - mindformers[callback.py:317] - INFO -   49.2% |████████████████████████                          | 14.43 samples/s/p  0:17:36 }\n",
      "2023-11-17 10:10:24,926 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1848/ 3750], loss: 2.557, per_step_time: 554ms, lr: 2.5905283e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:24,927 - mindformers[callback.py:317] - INFO -   49.3% |████████████████████████                          | 14.43 samples/s/p  0:17:34 }\n",
      "2023-11-17 10:10:27,158 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1852/ 3750], loss: 2.866, per_step_time: 554ms, lr: 2.5852987e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:27,159 - mindformers[callback.py:317] - INFO -   49.4% |████████████████████████                          | 14.43 samples/s/p  0:17:32 }\n",
      "2023-11-17 10:10:29,393 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1856/ 3750], loss: 2.293, per_step_time: 555ms, lr: 2.5800695e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:29,393 - mindformers[callback.py:317] - INFO -   49.5% |████████████████████████                          | 14.41 samples/s/p  0:17:31 }\n",
      "2023-11-17 10:10:31,629 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1860/ 3750], loss: 2.183, per_step_time: 555ms, lr: 2.5748399e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:31,629 - mindformers[callback.py:317] - INFO -   49.6% |████████████████████████                          | 14.40 samples/s/p  0:17:29 }\n",
      "2023-11-17 10:10:33,867 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1864/ 3750], loss: 3.270, per_step_time: 555ms, lr: 2.5696105e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:33,868 - mindformers[callback.py:317] - INFO -   49.7% |████████████████████████                          | 14.39 samples/s/p  0:17:28 }\n",
      "2023-11-17 10:10:36,106 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1868/ 3750], loss: 2.645, per_step_time: 556ms, lr: 2.564381e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:36,106 - mindformers[callback.py:317] - INFO -   49.8% |████████████████████████                          | 14.38 samples/s/p  0:17:26 }\n",
      "2023-11-17 10:10:38,337 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1872/ 3750], loss: 2.879, per_step_time: 554ms, lr: 2.5591513e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:38,338 - mindformers[callback.py:317] - INFO -   49.9% |████████████████████████                          | 14.43 samples/s/p  0:17:21 }\n",
      "2023-11-17 10:10:40,570 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1876/ 3750], loss: 2.784, per_step_time: 554ms, lr: 2.5539219e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:40,570 - mindformers[callback.py:317] - INFO -   50.0% |█████████████████████████                         | 14.42 samples/s/p  0:17:19 }\n",
      "INFO:root:Copy parallel total time cost: 43.95 seconds.\n",
      "2023-11-17 10:10:42,049 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.946226\n",
      "INFO:root:List OBS time cost: 0.08 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.13 seconds.\n",
      "2023-11-17 10:10:42,205 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.129993\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:10:42,279 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.043315\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:10:42,330 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.026711\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:10:42,802 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1880/ 3750], loss: 3.400, per_step_time: 554ms, lr: 2.5486925e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:42,802 - mindformers[callback.py:317] - INFO -   50.1% |█████████████████████████                         | 14.43 samples/s/p  0:17:17 }\n",
      "2023-11-17 10:10:45,033 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1884/ 3750], loss: 2.669, per_step_time: 554ms, lr: 2.543463e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:45,034 - mindformers[callback.py:317] - INFO -   50.2% |█████████████████████████                         | 14.43 samples/s/p  0:17:14 }\n",
      "2023-11-17 10:10:47,307 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1888/ 3750], loss: 1.836, per_step_time: 564ms, lr: 2.5382336e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:47,308 - mindformers[callback.py:317] - INFO -   50.3% |█████████████████████████                         | 14.16 samples/s/p  0:17:31 }\n",
      "2023-11-17 10:10:49,539 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1892/ 3750], loss: 2.832, per_step_time: 554ms, lr: 2.5330039e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:49,540 - mindformers[callback.py:317] - INFO -   50.5% |█████████████████████████                         | 14.42 samples/s/p  0:17:10 }\n",
      "2023-11-17 10:10:51,772 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1896/ 3750], loss: 2.482, per_step_time: 554ms, lr: 2.5277744e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:51,772 - mindformers[callback.py:317] - INFO -   50.6% |█████████████████████████                         | 14.43 samples/s/p  0:17:07 }\n",
      "2023-11-17 10:10:54,007 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1900/ 3750], loss: 2.642, per_step_time: 554ms, lr: 2.5225454e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:10:54,007 - mindformers[callback.py:317] - INFO -   50.7% |█████████████████████████                         | 14.43 samples/s/p  0:17:05 }\n",
      "2023-11-17 10:10:54,016 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 43.32 seconds.\n",
      "2023-11-17 10:11:25,679 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 43.318138\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34318), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:11:29,296 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1904/ 3750], loss: 3.070, per_step_time: 569ms, lr: 2.5173156e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:29,297 - mindformers[callback.py:317] - INFO -   50.8% |█████████████████████████                         | 14.06 samples/s/p  0:17:30 }\n",
      "2023-11-17 10:11:31,530 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1908/ 3750], loss: 3.072, per_step_time: 554ms, lr: 2.5120862e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:31,531 - mindformers[callback.py:317] - INFO -   50.9% |█████████████████████████                         | 14.43 samples/s/p  0:17:01 }\n",
      "2023-11-17 10:11:33,766 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1912/ 3750], loss: 3.554, per_step_time: 555ms, lr: 2.5068568e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:33,767 - mindformers[callback.py:317] - INFO -   51.0% |█████████████████████████                         | 14.40 samples/s/p  0:17:01 }\n",
      "2023-11-17 10:11:36,005 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1916/ 3750], loss: 2.261, per_step_time: 556ms, lr: 2.5016274e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:36,006 - mindformers[callback.py:317] - INFO -   51.1% |█████████████████████████                         | 14.38 samples/s/p  0:17:00 }\n",
      "2023-11-17 10:11:38,239 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1920/ 3750], loss: 2.578, per_step_time: 554ms, lr: 2.4963978e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:38,239 - mindformers[callback.py:317] - INFO -   51.2% |█████████████████████████                         | 14.42 samples/s/p  0:16:55 }\n",
      "2023-11-17 10:11:40,469 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1924/ 3750], loss: 3.358, per_step_time: 554ms, lr: 2.4911684e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:40,470 - mindformers[callback.py:317] - INFO -   51.3% |█████████████████████████                         | 14.43 samples/s/p  0:16:52 }\n",
      "2023-11-17 10:11:42,701 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1928/ 3750], loss: 2.425, per_step_time: 554ms, lr: 2.485939e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:42,702 - mindformers[callback.py:317] - INFO -   51.4% |█████████████████████████                         | 14.43 samples/s/p  0:16:50 }\n",
      "2023-11-17 10:11:44,933 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1932/ 3750], loss: 3.029, per_step_time: 554ms, lr: 2.4807096e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:44,933 - mindformers[callback.py:317] - INFO -   51.5% |█████████████████████████                         | 14.43 samples/s/p  0:16:47 }\n",
      "2023-11-17 10:11:47,165 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1936/ 3750], loss: 3.159, per_step_time: 554ms, lr: 2.4754801e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:47,166 - mindformers[callback.py:317] - INFO -   51.6% |█████████████████████████                         | 14.43 samples/s/p  0:16:45 }\n",
      "2023-11-17 10:11:49,397 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1940/ 3750], loss: 3.333, per_step_time: 554ms, lr: 2.4702507e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:49,398 - mindformers[callback.py:317] - INFO -   51.7% |█████████████████████████                         | 14.43 samples/s/p  0:16:43 }\n",
      "2023-11-17 10:11:51,629 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1944/ 3750], loss: 3.168, per_step_time: 554ms, lr: 2.4650213e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:51,630 - mindformers[callback.py:317] - INFO -   51.8% |█████████████████████████                         | 14.43 samples/s/p  0:16:41 }\n",
      "2023-11-17 10:11:53,862 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1948/ 3750], loss: 2.820, per_step_time: 554ms, lr: 2.4597919e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:53,863 - mindformers[callback.py:317] - INFO -   51.9% |█████████████████████████                         | 14.43 samples/s/p  0:16:39 }\n",
      "2023-11-17 10:11:56,095 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1952/ 3750], loss: 2.716, per_step_time: 554ms, lr: 2.4545623e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:56,095 - mindformers[callback.py:317] - INFO -   52.1% |██████████████████████████                        | 14.43 samples/s/p  0:16:36 }\n",
      "2023-11-17 10:11:58,327 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1956/ 3750], loss: 2.603, per_step_time: 554ms, lr: 2.4493329e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:11:58,328 - mindformers[callback.py:317] - INFO -   52.2% |██████████████████████████                        | 14.42 samples/s/p  0:16:34 }\n",
      "2023-11-17 10:12:00,565 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1960/ 3750], loss: 2.569, per_step_time: 555ms, lr: 2.4441035e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:00,565 - mindformers[callback.py:317] - INFO -   52.3% |██████████████████████████                        | 14.39 samples/s/p  0:16:34 }\n",
      "2023-11-17 10:12:02,802 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1964/ 3750], loss: 2.320, per_step_time: 555ms, lr: 2.4388743e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:02,802 - mindformers[callback.py:317] - INFO -   52.4% |██████████████████████████                        | 14.40 samples/s/p  0:16:32 }\n",
      "2023-11-17 10:12:05,039 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1968/ 3750], loss: 2.751, per_step_time: 555ms, lr: 2.4336445e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:05,039 - mindformers[callback.py:317] - INFO -   52.5% |██████████████████████████                        | 14.40 samples/s/p  0:16:30 }\n",
      "2023-11-17 10:12:07,273 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1972/ 3750], loss: 2.509, per_step_time: 555ms, lr: 2.428415e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:07,273 - mindformers[callback.py:317] - INFO -   52.6% |██████████████████████████                        | 14.41 samples/s/p  0:16:26 }\n",
      "2023-11-17 10:12:09,505 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1976/ 3750], loss: 2.729, per_step_time: 554ms, lr: 2.4231857e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:09,506 - mindformers[callback.py:317] - INFO -   52.7% |██████████████████████████                        | 14.43 samples/s/p  0:16:23 }\n",
      "INFO:root:Copy parallel total time cost: 43.64 seconds.\n",
      "2023-11-17 10:12:10,721 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.643874\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.11 seconds.\n",
      "2023-11-17 10:12:10,856 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.108717\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:12:10,929 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.043105\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:12:11,002 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.049801\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:12:11,736 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1980/ 3750], loss: 2.742, per_step_time: 554ms, lr: 2.4179564e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:11,737 - mindformers[callback.py:317] - INFO -   52.8% |██████████████████████████                        | 14.43 samples/s/p  0:16:20 }\n",
      "2023-11-17 10:12:13,969 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1984/ 3750], loss: 2.990, per_step_time: 554ms, lr: 2.4127265e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:13,969 - mindformers[callback.py:317] - INFO -   52.9% |██████████████████████████                        | 14.43 samples/s/p  0:16:19 }\n",
      "2023-11-17 10:12:16,201 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1988/ 3750], loss: 3.018, per_step_time: 554ms, lr: 2.407497e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:16,201 - mindformers[callback.py:317] - INFO -   53.0% |██████████████████████████                        | 14.43 samples/s/p  0:16:16 }\n",
      "2023-11-17 10:12:18,432 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1992/ 3750], loss: 2.470, per_step_time: 554ms, lr: 2.4022676e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:18,432 - mindformers[callback.py:317] - INFO -   53.1% |██████████████████████████                        | 14.44 samples/s/p  0:16:14 }\n",
      "2023-11-17 10:12:20,664 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 1996/ 3750], loss: 2.479, per_step_time: 554ms, lr: 2.3970386e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:20,664 - mindformers[callback.py:317] - INFO -   53.2% |██████████████████████████                        | 14.43 samples/s/p  0:16:12 }\n",
      "2023-11-17 10:12:22,897 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2000/ 3750], loss: 2.835, per_step_time: 554ms, lr: 2.3918092e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:12:22,897 - mindformers[callback.py:317] - INFO -   53.3% |██████████████████████████                        | 14.43 samples/s/p  0:16:10 }\n",
      "2023-11-17 10:12:22,907 - mindformers[callback.py:553] - INFO - ......Saving ckpt......\n",
      "[WARNING] GE_ADPT(17499,ffff9167b0b0,python):2023-11-17-10:12:22.908.139 [mindspore/ccsrc/transform/graph_ir/graph_runner.cc:128] RunGraph] Get graph form DfGraphManager failed!\n",
      "[WARNING] DEVICE(17499,ffff9167b0b0,python):2023-11-17-10:12:22.908.190 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ascend_deprecated_interface.cc:172] DoExecNonInputGraph] Exec graph:save.25767_25758_12920_1_mindspore_train_dataset_helper__DataWrapper_construct_5465 failed\n",
      "WARNING:root:Retry=9, Wait=0.1, Timestamp=1700187145.354562, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43092), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=8, Wait=0.2, Timestamp=1700187145.4725225, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43094), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=7, Wait=0.4, Timestamp=1700187145.682085, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43096), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=6, Wait=0.8, Timestamp=1700187146.0904372, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43098), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=5, Wait=1.6, Timestamp=1700187146.8991857, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43100), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=4, Wait=3.2, Timestamp=1700187148.5097032, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43102), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=3, Wait=6.4, Timestamp=1700187151.7194383, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51020), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=2, Wait=12.8, Timestamp=1700187158.1315358, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51022), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=1, Wait=25.6, Timestamp=1700187170.9515243, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 42484), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "ERROR:root:Failed to call:\n",
      "\tfunc=<bound method ObsClient.uploadFile of <moxing.framework.file.file_io_obs_wrapper.ObsClientDecorated object at 0xfffe1c7f1400>>\n",
      "\targs=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt', 10485760, 32, True)\n",
      "\tkwargs={}\n",
      "WARNING:root:skip file not found: /cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-250_4.ckpt\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=127, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35782), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=169, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34832), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=168, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55826), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=166, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55822), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=187, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55832), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=163, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34824), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=171, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34828), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=183, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35684), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=175, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 36404), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=161, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 36406), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=159, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35678), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=157, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 37024), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=180, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34842), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=186, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35686), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=185, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34848), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=167, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35786), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=165, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 37026), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=177, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34846), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=125, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34818), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=173, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55830), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=170, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34852), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=189, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55834), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=188, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55836), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=184, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 37028), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=174, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34844), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=172, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34834), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=160, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34820), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=158, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35784), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=178, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55828), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=164, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 36402), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=162, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34822), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=181, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34850), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "INFO:root:Copy parallel total time cost: 65.79 seconds.\n",
      "2023-11-17 10:13:16,819 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 65.791673\n",
      "2023-11-17 10:14:29,920 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 42890), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.10 seconds.\n",
      "2023-11-17 10:14:32,694 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2004/ 3750], loss: 2.546, per_step_time: 579ms, lr: 2.3865792e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:32,694 - mindformers[callback.py:317] - INFO -   53.4% |██████████████████████████                        | 13.82 samples/s/p  0:16:51 }\n",
      "2023-11-17 10:14:34,928 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2008/ 3750], loss: 2.854, per_step_time: 554ms, lr: 2.3813498e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:34,928 - mindformers[callback.py:317] - INFO -   53.5% |██████████████████████████                        | 14.43 samples/s/p  0:16:05 }\n",
      "2023-11-17 10:14:37,161 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2012/ 3750], loss: 2.948, per_step_time: 554ms, lr: 2.3761204e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:37,161 - mindformers[callback.py:317] - INFO -   53.7% |██████████████████████████                        | 14.42 samples/s/p  0:16:03 }\n",
      "2023-11-17 10:14:39,392 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2016/ 3750], loss: 2.956, per_step_time: 554ms, lr: 2.370891e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:39,393 - mindformers[callback.py:317] - INFO -   53.8% |██████████████████████████                        | 14.43 samples/s/p  0:16:01 }\n",
      "2023-11-17 10:14:41,626 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2020/ 3750], loss: 2.239, per_step_time: 554ms, lr: 2.3656616e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:41,627 - mindformers[callback.py:317] - INFO -   53.9% |██████████████████████████                        | 14.42 samples/s/p  0:15:59 }\n",
      "2023-11-17 10:14:43,861 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2024/ 3750], loss: 2.423, per_step_time: 554ms, lr: 2.3604322e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:43,861 - mindformers[callback.py:317] - INFO -   54.0% |██████████████████████████                        | 14.42 samples/s/p  0:15:57 }\n",
      "2023-11-17 10:14:46,093 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2028/ 3750], loss: 2.927, per_step_time: 554ms, lr: 2.3552027e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:46,094 - mindformers[callback.py:317] - INFO -   54.1% |███████████████████████████                       | 14.43 samples/s/p  0:15:54 }\n",
      "2023-11-17 10:14:48,325 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2032/ 3750], loss: 2.770, per_step_time: 554ms, lr: 2.3499733e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:48,326 - mindformers[callback.py:317] - INFO -   54.2% |███████████████████████████                       | 14.43 samples/s/p  0:15:52 }\n",
      "2023-11-17 10:14:50,559 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2036/ 3750], loss: 2.609, per_step_time: 554ms, lr: 2.3447437e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:50,560 - mindformers[callback.py:317] - INFO -   54.3% |███████████████████████████                       | 14.42 samples/s/p  0:15:50 }\n",
      "2023-11-17 10:14:52,792 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2040/ 3750], loss: 3.031, per_step_time: 554ms, lr: 2.3395145e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:52,792 - mindformers[callback.py:317] - INFO -   54.4% |███████████████████████████                       | 14.43 samples/s/p  0:15:48 }\n",
      "2023-11-17 10:14:55,029 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2044/ 3750], loss: 3.149, per_step_time: 555ms, lr: 2.334285e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:55,029 - mindformers[callback.py:317] - INFO -   54.5% |███████████████████████████                       | 14.40 samples/s/p  0:15:47 }\n",
      "2023-11-17 10:14:57,266 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2048/ 3750], loss: 3.234, per_step_time: 555ms, lr: 2.3290557e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:14:57,266 - mindformers[callback.py:317] - INFO -   54.6% |███████████████████████████                       | 14.40 samples/s/p  0:15:45 }\n",
      "2023-11-17 10:14:59,506 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2052/ 3750], loss: 1.297, per_step_time: 556ms, lr: 2.3238259e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:14:59,506 - mindformers[callback.py:317] - INFO -   54.7% |███████████████████████████                       | 14.38 samples/s/p  0:15:44 }\n",
      "2023-11-17 10:15:01,739 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2056/ 3750], loss: 2.962, per_step_time: 554ms, lr: 2.3185967e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:15:01,740 - mindformers[callback.py:317] - INFO -   54.8% |███████████████████████████                       | 14.42 samples/s/p  0:15:39 }\n",
      "2023-11-17 10:15:03,973 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2060/ 3750], loss: 3.074, per_step_time: 554ms, lr: 2.3133673e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:15:03,973 - mindformers[callback.py:317] - INFO -   54.9% |███████████████████████████                       | 14.42 samples/s/p  0:15:37 }\n",
      "2023-11-17 10:15:06,206 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2064/ 3750], loss: 2.771, per_step_time: 554ms, lr: 2.3081378e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:15:06,206 - mindformers[callback.py:317] - INFO -   55.0% |███████████████████████████                       | 14.43 samples/s/p  0:15:34 }\n",
      "2023-11-17 10:15:08,439 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2068/ 3750], loss: 3.135, per_step_time: 554ms, lr: 2.3029079e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:15:08,439 - mindformers[callback.py:317] - INFO -   55.1% |███████████████████████████                       | 14.42 samples/s/p  0:15:32 }\n",
      "2023-11-17 10:15:10,674 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2072/ 3750], loss: 2.454, per_step_time: 555ms, lr: 2.2976788e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:15:10,675 - mindformers[callback.py:317] - INFO -   55.3% |███████████████████████████                       | 14.41 samples/s/p  0:15:31 }\n",
      "2023-11-17 10:15:12,909 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2076/ 3750], loss: 2.479, per_step_time: 555ms, lr: 2.2924494e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:15:12,910 - mindformers[callback.py:317] - INFO -   55.4% |███████████████████████████                       | 14.41 samples/s/p  0:15:29 }\n",
      "INFO:root:Copy parallel total time cost: 43.71 seconds.\n",
      "2023-11-17 10:15:14,145 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.705927\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.12 seconds.\n",
      "2023-11-17 10:15:14,296 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.124586\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:15:14,360 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.037683\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:15:14,402 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020995\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:15:15,145 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2080/ 3750], loss: 2.593, per_step_time: 555ms, lr: 2.28722e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:15:15,146 - mindformers[callback.py:317] - INFO -   55.5% |███████████████████████████                       | 14.41 samples/s/p  0:15:27 }\n",
      "2023-11-17 10:15:17,381 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2084/ 3750], loss: 3.265, per_step_time: 555ms, lr: 2.28199e-05, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:15:17,381 - mindformers[callback.py:317] - INFO -   55.6% |███████████████████████████                       | 14.41 samples/s/p  0:15:25 }\n",
      "2023-11-17 10:15:19,611 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2088/ 3750], loss: 2.230, per_step_time: 553ms, lr: 2.278068e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:15:19,612 - mindformers[callback.py:317] - INFO -   55.7% |███████████████████████████                       | 14.45 samples/s/p  0:15:20 }\n",
      "2023-11-17 10:15:21,845 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2092/ 3750], loss: 2.861, per_step_time: 554ms, lr: 2.2728389e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:15:21,846 - mindformers[callback.py:317] - INFO -   55.8% |███████████████████████████                       | 14.42 samples/s/p  0:15:19 }\n",
      "2023-11-17 10:15:24,087 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2096/ 3750], loss: 2.478, per_step_time: 556ms, lr: 2.2676095e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:15:24,087 - mindformers[callback.py:317] - INFO -   55.9% |███████████████████████████                       | 14.37 samples/s/p  0:15:20 }\n",
      "2023-11-17 10:15:26,319 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2100/ 3750], loss: 2.683, per_step_time: 554ms, lr: 2.26238e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:15:26,320 - mindformers[callback.py:317] - INFO -   56.0% |████████████████████████████                      | 14.43 samples/s/p  0:15:14 }\n",
      "2023-11-17 10:15:26,330 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 43.33 seconds.\n",
      "2023-11-17 10:15:57,758 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 43.326593\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 49794), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.14 seconds.\n",
      "2023-11-17 10:16:01,061 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2104/ 3750], loss: 2.548, per_step_time: 571ms, lr: 2.2571501e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:01,062 - mindformers[callback.py:317] - INFO -   56.1% |████████████████████████████                      | 13.99 samples/s/p  0:15:41 }\n",
      "2023-11-17 10:16:03,298 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2108/ 3750], loss: 2.826, per_step_time: 555ms, lr: 2.2519209e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:03,299 - mindformers[callback.py:317] - INFO -   56.2% |████████████████████████████                      | 14.41 samples/s/p  0:15:11 }\n",
      "2023-11-17 10:16:05,536 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2112/ 3750], loss: 2.852, per_step_time: 555ms, lr: 2.2466915e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:05,537 - mindformers[callback.py:317] - INFO -   56.3% |████████████████████████████                      | 14.40 samples/s/p  0:15:10 }\n",
      "2023-11-17 10:16:07,784 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2116/ 3750], loss: 2.131, per_step_time: 558ms, lr: 2.241462e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:07,784 - mindformers[callback.py:317] - INFO -   56.4% |████████████████████████████                      | 14.33 samples/s/p  0:15:12 }\n",
      "2023-11-17 10:16:10,017 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2120/ 3750], loss: 2.353, per_step_time: 554ms, lr: 2.2362325e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:10,017 - mindformers[callback.py:317] - INFO -   56.5% |████████████████████████████                      | 14.43 samples/s/p  0:15:03 }\n",
      "2023-11-17 10:16:12,249 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2124/ 3750], loss: 1.970, per_step_time: 554ms, lr: 2.231003e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:12,250 - mindformers[callback.py:317] - INFO -   56.6% |████████████████████████████                      | 14.43 samples/s/p  0:15:01 }\n",
      "2023-11-17 10:16:14,485 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2128/ 3750], loss: 3.565, per_step_time: 555ms, lr: 2.2257736e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:14,485 - mindformers[callback.py:317] - INFO -   56.7% |████████████████████████████                      | 14.41 samples/s/p  0:15:00 }\n",
      "2023-11-17 10:16:16,721 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2132/ 3750], loss: 3.797, per_step_time: 555ms, lr: 2.2205442e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:16,721 - mindformers[callback.py:317] - INFO -   56.9% |████████████████████████████                      | 14.41 samples/s/p  0:14:58 }\n",
      "2023-11-17 10:16:18,956 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2136/ 3750], loss: 3.006, per_step_time: 555ms, lr: 2.2153148e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:18,957 - mindformers[callback.py:317] - INFO -   57.0% |████████████████████████████                      | 14.41 samples/s/p  0:14:55 }\n",
      "2023-11-17 10:16:21,190 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2140/ 3750], loss: 2.969, per_step_time: 554ms, lr: 2.2100854e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:21,190 - mindformers[callback.py:317] - INFO -   57.1% |████████████████████████████                      | 14.43 samples/s/p  0:14:52 }\n",
      "2023-11-17 10:16:23,424 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2144/ 3750], loss: 2.809, per_step_time: 554ms, lr: 2.204856e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:23,424 - mindformers[callback.py:317] - INFO -   57.2% |████████████████████████████                      | 14.42 samples/s/p  0:14:50 }\n",
      "2023-11-17 10:16:25,657 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2148/ 3750], loss: 2.310, per_step_time: 554ms, lr: 2.1996266e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:25,657 - mindformers[callback.py:317] - INFO -   57.3% |████████████████████████████                      | 14.43 samples/s/p  0:14:48 }\n",
      "2023-11-17 10:16:27,890 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2152/ 3750], loss: 2.606, per_step_time: 554ms, lr: 2.194397e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:27,890 - mindformers[callback.py:317] - INFO -   57.4% |████████████████████████████                      | 14.43 samples/s/p  0:14:46 }\n",
      "2023-11-17 10:16:30,123 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2156/ 3750], loss: 3.025, per_step_time: 554ms, lr: 2.1891676e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:30,124 - mindformers[callback.py:317] - INFO -   57.5% |████████████████████████████                      | 14.43 samples/s/p  0:14:44 }\n",
      "2023-11-17 10:16:32,356 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2160/ 3750], loss: 2.600, per_step_time: 554ms, lr: 2.1839382e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:32,356 - mindformers[callback.py:317] - INFO -   57.6% |████████████████████████████                      | 14.43 samples/s/p  0:14:41 }\n",
      "2023-11-17 10:16:34,591 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2164/ 3750], loss: 2.877, per_step_time: 554ms, lr: 2.1787087e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:34,591 - mindformers[callback.py:317] - INFO -   57.7% |████████████████████████████                      | 14.42 samples/s/p  0:14:40 }\n",
      "2023-11-17 10:16:36,830 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2168/ 3750], loss: 2.784, per_step_time: 555ms, lr: 2.1734792e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:36,830 - mindformers[callback.py:317] - INFO -   57.8% |████████████████████████████                      | 14.39 samples/s/p  0:14:39 }\n",
      "2023-11-17 10:16:39,066 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2172/ 3750], loss: 2.096, per_step_time: 555ms, lr: 2.1682497e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:39,066 - mindformers[callback.py:317] - INFO -   57.9% |████████████████████████████                      | 14.41 samples/s/p  0:14:36 }\n",
      "2023-11-17 10:16:41,316 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2176/ 3750], loss: 2.401, per_step_time: 558ms, lr: 2.16302e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:41,317 - mindformers[callback.py:317] - INFO -   58.0% |█████████████████████████████                     | 14.31 samples/s/p  0:14:39 }\n",
      "INFO:root:Copy parallel total time cost: 43.29 seconds.\n",
      "2023-11-17 10:16:42,112 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.292614\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.13 seconds.\n",
      "2023-11-17 10:16:42,275 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.128837\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:16:42,335 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.038252\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:16:42,424 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.022015\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:16:43,549 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2180/ 3750], loss: 3.133, per_step_time: 554ms, lr: 2.157791e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:43,550 - mindformers[callback.py:317] - INFO -   58.1% |█████████████████████████████                     | 14.43 samples/s/p  0:14:30 }\n",
      "2023-11-17 10:16:45,782 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2184/ 3750], loss: 2.736, per_step_time: 554ms, lr: 2.1525615e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:45,782 - mindformers[callback.py:317] - INFO -   58.2% |█████████████████████████████                     | 14.43 samples/s/p  0:14:28 }\n",
      "2023-11-17 10:16:48,016 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2188/ 3750], loss: 2.426, per_step_time: 554ms, lr: 2.1473317e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:48,016 - mindformers[callback.py:317] - INFO -   58.3% |█████████████████████████████                     | 14.43 samples/s/p  0:14:26 }\n",
      "2023-11-17 10:16:50,249 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2192/ 3750], loss: 2.850, per_step_time: 554ms, lr: 2.1421023e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:50,249 - mindformers[callback.py:317] - INFO -   58.5% |█████████████████████████████                     | 14.42 samples/s/p  0:14:24 }\n",
      "2023-11-17 10:16:52,485 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2196/ 3750], loss: 2.997, per_step_time: 555ms, lr: 2.1368729e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:52,485 - mindformers[callback.py:317] - INFO -   58.6% |█████████████████████████████                     | 14.41 samples/s/p  0:14:22 }\n",
      "2023-11-17 10:16:54,721 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2200/ 3750], loss: 2.479, per_step_time: 555ms, lr: 2.1316437e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:16:54,721 - mindformers[callback.py:317] - INFO -   58.7% |█████████████████████████████                     | 14.41 samples/s/p  0:14:20 }\n",
      "2023-11-17 10:16:54,732 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.32 seconds.\n",
      "2023-11-17 10:17:24,775 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.324078\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 44824), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:17:28,373 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2204/ 3750], loss: 2.511, per_step_time: 569ms, lr: 2.1264139e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:28,374 - mindformers[callback.py:317] - INFO -   58.8% |█████████████████████████████                     | 14.04 samples/s/p  0:14:41 }\n",
      "2023-11-17 10:17:30,608 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2208/ 3750], loss: 1.956, per_step_time: 554ms, lr: 2.1211845e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:30,608 - mindformers[callback.py:317] - INFO -   58.9% |█████████████████████████████                     | 14.43 samples/s/p  0:14:14 }\n",
      "2023-11-17 10:17:32,840 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2212/ 3750], loss: 2.186, per_step_time: 554ms, lr: 2.115955e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:32,841 - mindformers[callback.py:317] - INFO -   59.0% |█████████████████████████████                     | 14.43 samples/s/p  0:14:12 }\n",
      "2023-11-17 10:17:35,073 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2216/ 3750], loss: 2.413, per_step_time: 554ms, lr: 2.1107257e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:35,074 - mindformers[callback.py:317] - INFO -   59.1% |█████████████████████████████                     | 14.43 samples/s/p  0:14:10 }\n",
      "2023-11-17 10:17:37,306 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2220/ 3750], loss: 2.399, per_step_time: 554ms, lr: 2.1054962e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:37,306 - mindformers[callback.py:317] - INFO -   59.2% |█████████████████████████████                     | 14.43 samples/s/p  0:14:07 }\n",
      "2023-11-17 10:17:39,541 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2224/ 3750], loss: 3.171, per_step_time: 554ms, lr: 2.1002668e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:39,541 - mindformers[callback.py:317] - INFO -   59.3% |█████████████████████████████                     | 14.41 samples/s/p  0:14:06 }\n",
      "2023-11-17 10:17:41,785 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2228/ 3750], loss: 3.208, per_step_time: 557ms, lr: 2.0950374e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:41,785 - mindformers[callback.py:317] - INFO -   59.4% |█████████████████████████████                     | 14.36 samples/s/p  0:14:07 }\n",
      "2023-11-17 10:17:44,022 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2232/ 3750], loss: 2.650, per_step_time: 555ms, lr: 2.089808e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:44,022 - mindformers[callback.py:317] - INFO -   59.5% |█████████████████████████████                     | 14.41 samples/s/p  0:14:02 }\n",
      "2023-11-17 10:17:46,269 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2236/ 3750], loss: 2.915, per_step_time: 557ms, lr: 2.0845784e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:46,269 - mindformers[callback.py:317] - INFO -   59.6% |█████████████████████████████                     | 14.34 samples/s/p  0:14:04 }\n",
      "2023-11-17 10:17:48,502 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2240/ 3750], loss: 2.724, per_step_time: 554ms, lr: 2.079349e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:48,502 - mindformers[callback.py:317] - INFO -   59.7% |█████████████████████████████                     | 14.43 samples/s/p  0:13:57 }\n",
      "2023-11-17 10:17:50,734 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2244/ 3750], loss: 2.965, per_step_time: 554ms, lr: 2.0741196e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:50,734 - mindformers[callback.py:317] - INFO -   59.8% |█████████████████████████████                     | 14.44 samples/s/p  0:13:54 }\n",
      "2023-11-17 10:17:52,966 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2248/ 3750], loss: 3.408, per_step_time: 554ms, lr: 2.0688902e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:52,966 - mindformers[callback.py:317] - INFO -   59.9% |█████████████████████████████                     | 14.43 samples/s/p  0:13:52 }\n",
      "2023-11-17 10:17:55,199 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2252/ 3750], loss: 3.414, per_step_time: 554ms, lr: 2.0636606e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:55,199 - mindformers[callback.py:317] - INFO -   60.1% |██████████████████████████████                    | 14.43 samples/s/p  0:13:50 }\n",
      "2023-11-17 10:17:57,433 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2256/ 3750], loss: 2.821, per_step_time: 554ms, lr: 2.0584312e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:57,434 - mindformers[callback.py:317] - INFO -   60.2% |██████████████████████████████                    | 14.42 samples/s/p  0:13:48 }\n",
      "2023-11-17 10:17:59,667 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2260/ 3750], loss: 2.185, per_step_time: 554ms, lr: 2.0532018e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:17:59,667 - mindformers[callback.py:317] - INFO -   60.3% |██████████████████████████████                    | 14.43 samples/s/p  0:13:45 }\n",
      "2023-11-17 10:18:01,901 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2264/ 3750], loss: 2.871, per_step_time: 554ms, lr: 2.0479723e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:01,901 - mindformers[callback.py:317] - INFO -   60.4% |██████████████████████████████                    | 14.43 samples/s/p  0:13:43 }\n",
      "2023-11-17 10:18:04,133 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2268/ 3750], loss: 2.829, per_step_time: 554ms, lr: 2.0427427e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:04,133 - mindformers[callback.py:317] - INFO -   60.5% |██████████████████████████████                    | 14.44 samples/s/p  0:13:41 }\n",
      "2023-11-17 10:18:06,366 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2272/ 3750], loss: 2.989, per_step_time: 554ms, lr: 2.0375133e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:06,366 - mindformers[callback.py:317] - INFO -   60.6% |██████████████████████████████                    | 14.43 samples/s/p  0:13:39 }\n",
      "2023-11-17 10:18:08,599 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2276/ 3750], loss: 2.965, per_step_time: 554ms, lr: 2.032284e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:08,599 - mindformers[callback.py:317] - INFO -   60.7% |██████████████████████████████                    | 14.43 samples/s/p  0:13:37 }\n",
      "INFO:root:Copy parallel total time cost: 43.81 seconds.\n",
      "2023-11-17 10:18:10,014 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.814525\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.11 seconds.\n",
      "2023-11-17 10:18:10,151 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.111940\n",
      "INFO:root:List OBS time cost: 0.03 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.05 seconds.\n",
      "2023-11-17 10:18:10,229 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.050155\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:18:10,275 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.021538\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:18:10,831 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2280/ 3750], loss: 3.010, per_step_time: 554ms, lr: 2.0270545e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:10,832 - mindformers[callback.py:317] - INFO -   60.8% |██████████████████████████████                    | 14.44 samples/s/p  0:13:34 }\n",
      "2023-11-17 10:18:13,066 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2284/ 3750], loss: 3.173, per_step_time: 554ms, lr: 2.0218247e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:13,067 - mindformers[callback.py:317] - INFO -   60.9% |██████████████████████████████                    | 14.42 samples/s/p  0:13:33 }\n",
      "2023-11-17 10:18:15,303 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2288/ 3750], loss: 3.099, per_step_time: 554ms, lr: 2.0165953e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:15,303 - mindformers[callback.py:317] - INFO -   61.0% |██████████████████████████████                    | 14.42 samples/s/p  0:13:31 }\n",
      "2023-11-17 10:18:17,540 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2292/ 3750], loss: 2.629, per_step_time: 555ms, lr: 2.0113659e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:17,540 - mindformers[callback.py:317] - INFO -   61.1% |██████████████████████████████                    | 14.41 samples/s/p  0:13:29 }\n",
      "2023-11-17 10:18:19,773 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2296/ 3750], loss: 2.648, per_step_time: 554ms, lr: 2.0061365e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:19,773 - mindformers[callback.py:317] - INFO -   61.2% |██████████████████████████████                    | 14.43 samples/s/p  0:13:26 }\n",
      "2023-11-17 10:18:22,013 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2300/ 3750], loss: 2.907, per_step_time: 555ms, lr: 2.000907e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:22,013 - mindformers[callback.py:317] - INFO -   61.3% |██████████████████████████████                    | 14.39 samples/s/p  0:13:26 }\n",
      "2023-11-17 10:18:22,024 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 41.97 seconds.\n",
      "2023-11-17 10:18:52,333 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 41.972968\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 35192), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:18:55,988 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2304/ 3750], loss: 2.606, per_step_time: 569ms, lr: 1.9956777e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:55,988 - mindformers[callback.py:317] - INFO -   61.4% |██████████████████████████████                    | 14.05 samples/s/p  0:13:43 }\n",
      "2023-11-17 10:18:58,222 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2308/ 3750], loss: 2.609, per_step_time: 554ms, lr: 1.9904483e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:18:58,223 - mindformers[callback.py:317] - INFO -   61.5% |██████████████████████████████                    | 14.43 samples/s/p  0:13:19 }\n",
      "2023-11-17 10:19:00,456 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2312/ 3750], loss: 2.585, per_step_time: 554ms, lr: 1.9852188e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:00,456 - mindformers[callback.py:317] - INFO -   61.7% |██████████████████████████████                    | 14.43 samples/s/p  0:13:17 }\n",
      "2023-11-17 10:19:02,691 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2316/ 3750], loss: 2.686, per_step_time: 554ms, lr: 1.9799892e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:02,691 - mindformers[callback.py:317] - INFO -   61.8% |██████████████████████████████                    | 14.43 samples/s/p  0:13:15 }\n",
      "2023-11-17 10:19:04,925 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2320/ 3750], loss: 2.788, per_step_time: 554ms, lr: 1.9747598e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:04,925 - mindformers[callback.py:317] - INFO -   61.9% |██████████████████████████████                    | 14.43 samples/s/p  0:13:12 }\n",
      "2023-11-17 10:19:07,159 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2324/ 3750], loss: 2.180, per_step_time: 554ms, lr: 1.9695304e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:07,160 - mindformers[callback.py:317] - INFO -   62.0% |██████████████████████████████                    | 14.43 samples/s/p  0:13:10 }\n",
      "2023-11-17 10:19:09,393 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2328/ 3750], loss: 2.556, per_step_time: 554ms, lr: 1.9643012e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:09,393 - mindformers[callback.py:317] - INFO -   62.1% |███████████████████████████████                   | 14.43 samples/s/p  0:13:08 }\n",
      "2023-11-17 10:19:11,629 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2332/ 3750], loss: 2.661, per_step_time: 555ms, lr: 1.9590714e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:11,630 - mindformers[callback.py:317] - INFO -   62.2% |███████████████████████████████                   | 14.41 samples/s/p  0:13:07 }\n",
      "2023-11-17 10:19:13,867 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2336/ 3750], loss: 2.274, per_step_time: 555ms, lr: 1.953842e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:13,868 - mindformers[callback.py:317] - INFO -   62.3% |███████████████████████████████                   | 14.40 samples/s/p  0:13:05 }\n",
      "2023-11-17 10:19:16,109 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2340/ 3750], loss: 2.303, per_step_time: 556ms, lr: 1.9486126e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:16,109 - mindformers[callback.py:317] - INFO -   62.4% |███████████████████████████████                   | 14.38 samples/s/p  0:13:04 }\n",
      "2023-11-17 10:19:18,343 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2344/ 3750], loss: 2.511, per_step_time: 554ms, lr: 1.9433834e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:18,343 - mindformers[callback.py:317] - INFO -   62.5% |███████████████████████████████                   | 14.43 samples/s/p  0:12:59 }\n",
      "2023-11-17 10:19:20,576 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2348/ 3750], loss: 2.238, per_step_time: 554ms, lr: 1.9381536e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:20,577 - mindformers[callback.py:317] - INFO -   62.6% |███████████████████████████████                   | 14.43 samples/s/p  0:12:57 }\n",
      "2023-11-17 10:19:22,810 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2352/ 3750], loss: 3.073, per_step_time: 554ms, lr: 1.9329242e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:22,811 - mindformers[callback.py:317] - INFO -   62.7% |███████████████████████████████                   | 14.43 samples/s/p  0:12:55 }\n",
      "2023-11-17 10:19:25,043 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2356/ 3750], loss: 2.567, per_step_time: 554ms, lr: 1.9276948e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:25,044 - mindformers[callback.py:317] - INFO -   62.8% |███████████████████████████████                   | 14.43 samples/s/p  0:12:52 }\n",
      "2023-11-17 10:19:27,297 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2360/ 3750], loss: 3.101, per_step_time: 554ms, lr: 1.9224655e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:27,297 - mindformers[callback.py:317] - INFO -   62.9% |███████████████████████████████                   | 14.43 samples/s/p  0:12:50 }\n",
      "2023-11-17 10:19:29,531 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2364/ 3750], loss: 2.756, per_step_time: 554ms, lr: 1.9172356e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:29,531 - mindformers[callback.py:317] - INFO -   63.0% |███████████████████████████████                   | 14.43 samples/s/p  0:12:48 }\n",
      "2023-11-17 10:19:31,766 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2368/ 3750], loss: 3.285, per_step_time: 554ms, lr: 1.9120062e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:31,766 - mindformers[callback.py:317] - INFO -   63.1% |███████████████████████████████                   | 14.42 samples/s/p  0:12:46 }\n",
      "2023-11-17 10:19:34,001 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2372/ 3750], loss: 2.309, per_step_time: 554ms, lr: 1.9067771e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:34,002 - mindformers[callback.py:317] - INFO -   63.3% |███████████████████████████████                   | 14.43 samples/s/p  0:12:44 }\n",
      "INFO:root:Copy parallel total time cost: 42.34 seconds.\n",
      "2023-11-17 10:19:36,097 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.336226\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.11 seconds.\n",
      "2023-11-17 10:19:36,228 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.106801\n",
      "2023-11-17 10:19:36,237 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2376/ 3750], loss: 3.739, per_step_time: 554ms, lr: 1.9015477e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:36,238 - mindformers[callback.py:317] - INFO -   63.4% |███████████████████████████████                   | 14.42 samples/s/p  0:12:42 }\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:19:36,293 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.038814\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:19:36,343 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.024693\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:19:38,473 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2380/ 3750], loss: 2.163, per_step_time: 554ms, lr: 1.896318e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:38,473 - mindformers[callback.py:317] - INFO -   63.5% |███████████████████████████████                   | 14.43 samples/s/p  0:12:39 }\n",
      "2023-11-17 10:19:40,708 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2384/ 3750], loss: 2.841, per_step_time: 554ms, lr: 1.8910885e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:40,708 - mindformers[callback.py:317] - INFO -   63.6% |███████████████████████████████                   | 14.42 samples/s/p  0:12:37 }\n",
      "2023-11-17 10:19:42,942 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2388/ 3750], loss: 2.913, per_step_time: 554ms, lr: 1.8858591e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:42,942 - mindformers[callback.py:317] - INFO -   63.7% |███████████████████████████████                   | 14.43 samples/s/p  0:12:35 }\n",
      "2023-11-17 10:19:45,180 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2392/ 3750], loss: 2.421, per_step_time: 555ms, lr: 1.8806297e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:45,180 - mindformers[callback.py:317] - INFO -   63.8% |███████████████████████████████                   | 14.40 samples/s/p  0:12:34 }\n",
      "2023-11-17 10:19:47,420 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2396/ 3750], loss: 1.551, per_step_time: 556ms, lr: 1.8754003e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:47,421 - mindformers[callback.py:317] - INFO -   63.9% |███████████████████████████████                   | 14.39 samples/s/p  0:12:32 }\n",
      "2023-11-17 10:19:49,653 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2400/ 3750], loss: 2.924, per_step_time: 554ms, lr: 1.8701707e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:19:49,654 - mindformers[callback.py:317] - INFO -   64.0% |████████████████████████████████                  | 14.43 samples/s/p  0:12:28 }\n",
      "2023-11-17 10:19:49,665 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.23 seconds.\n",
      "2023-11-17 10:20:18,607 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.231932\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 58256), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:20:22,271 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2404/ 3750], loss: 2.240, per_step_time: 568ms, lr: 1.8649414e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:22,272 - mindformers[callback.py:317] - INFO -   64.1% |████████████████████████████████                  | 14.06 samples/s/p  0:12:45 }\n",
      "2023-11-17 10:20:24,506 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2408/ 3750], loss: 2.594, per_step_time: 554ms, lr: 1.859712e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:24,507 - mindformers[callback.py:317] - INFO -   64.2% |████████████████████████████████                  | 14.43 samples/s/p  0:12:24 }\n",
      "2023-11-17 10:20:26,740 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2412/ 3750], loss: 2.297, per_step_time: 554ms, lr: 1.8544823e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:26,740 - mindformers[callback.py:317] - INFO -   64.3% |████████████████████████████████                  | 14.44 samples/s/p  0:12:21 }\n",
      "2023-11-17 10:20:28,973 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2416/ 3750], loss: 2.239, per_step_time: 554ms, lr: 1.849253e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:28,974 - mindformers[callback.py:317] - INFO -   64.4% |████████████████████████████████                  | 14.43 samples/s/p  0:12:19 }\n",
      "2023-11-17 10:20:31,207 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2420/ 3750], loss: 2.859, per_step_time: 554ms, lr: 1.8440236e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:31,208 - mindformers[callback.py:317] - INFO -   64.5% |████████████████████████████████                  | 14.43 samples/s/p  0:12:17 }\n",
      "2023-11-17 10:20:33,443 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2424/ 3750], loss: 2.739, per_step_time: 554ms, lr: 1.8387944e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:33,443 - mindformers[callback.py:317] - INFO -   64.6% |████████████████████████████████                  | 14.43 samples/s/p  0:12:15 }\n",
      "2023-11-17 10:20:35,677 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2428/ 3750], loss: 2.740, per_step_time: 554ms, lr: 1.8335646e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:35,677 - mindformers[callback.py:317] - INFO -   64.7% |████████████████████████████████                  | 14.43 samples/s/p  0:12:13 }\n",
      "2023-11-17 10:20:37,910 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2432/ 3750], loss: 3.163, per_step_time: 554ms, lr: 1.828335e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:37,911 - mindformers[callback.py:317] - INFO -   64.9% |████████████████████████████████                  | 14.43 samples/s/p  0:12:10 }\n",
      "2023-11-17 10:20:40,144 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2436/ 3750], loss: 2.229, per_step_time: 554ms, lr: 1.8231058e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:40,144 - mindformers[callback.py:317] - INFO -   65.0% |████████████████████████████████                  | 14.43 samples/s/p  0:12:08 }\n",
      "2023-11-17 10:20:42,380 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2440/ 3750], loss: 2.595, per_step_time: 554ms, lr: 1.8178764e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:42,380 - mindformers[callback.py:317] - INFO -   65.1% |████████████████████████████████                  | 14.42 samples/s/p  0:12:06 }\n",
      "2023-11-17 10:20:44,618 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2444/ 3750], loss: 2.505, per_step_time: 555ms, lr: 1.8126468e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:44,618 - mindformers[callback.py:317] - INFO -   65.2% |████████████████████████████████                  | 14.40 samples/s/p  0:12:05 }\n",
      "2023-11-17 10:20:46,857 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2448/ 3750], loss: 1.798, per_step_time: 555ms, lr: 1.8074174e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:46,858 - mindformers[callback.py:317] - INFO -   65.3% |████████████████████████████████                  | 14.40 samples/s/p  0:12:03 }\n",
      "2023-11-17 10:20:49,090 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2452/ 3750], loss: 2.225, per_step_time: 554ms, lr: 1.8021876e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:49,090 - mindformers[callback.py:317] - INFO -   65.4% |████████████████████████████████                  | 14.43 samples/s/p  0:11:59 }\n",
      "2023-11-17 10:20:51,324 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2456/ 3750], loss: 2.464, per_step_time: 554ms, lr: 1.7969585e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:51,324 - mindformers[callback.py:317] - INFO -   65.5% |████████████████████████████████                  | 14.43 samples/s/p  0:11:57 }\n",
      "2023-11-17 10:20:53,560 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2460/ 3750], loss: 3.125, per_step_time: 554ms, lr: 1.7917291e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:53,560 - mindformers[callback.py:317] - INFO -   65.6% |████████████████████████████████                  | 14.42 samples/s/p  0:11:55 }\n",
      "2023-11-17 10:20:55,795 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2464/ 3750], loss: 2.272, per_step_time: 554ms, lr: 1.7864993e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:55,795 - mindformers[callback.py:317] - INFO -   65.7% |████████████████████████████████                  | 14.43 samples/s/p  0:11:53 }\n",
      "2023-11-17 10:20:58,029 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2468/ 3750], loss: 3.033, per_step_time: 554ms, lr: 1.78127e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:20:58,029 - mindformers[callback.py:317] - INFO -   65.8% |████████████████████████████████                  | 14.43 samples/s/p  0:11:50 }\n",
      "2023-11-17 10:21:00,263 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2472/ 3750], loss: 2.891, per_step_time: 554ms, lr: 1.7760405e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:00,264 - mindformers[callback.py:317] - INFO -   65.9% |████████████████████████████████                  | 14.43 samples/s/p  0:11:48 }\n",
      "2023-11-17 10:21:02,498 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2476/ 3750], loss: 2.867, per_step_time: 554ms, lr: 1.7708115e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:02,499 - mindformers[callback.py:317] - INFO -   66.0% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:46 }\n",
      "INFO:root:Copy parallel total time cost: 42.69 seconds.\n",
      "2023-11-17 10:21:02,737 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.693680\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:21:02,862 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.100650\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:21:02,929 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039140\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:21:02,994 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.043306\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:21:04,733 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2480/ 3750], loss: 2.641, per_step_time: 554ms, lr: 1.7655817e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:04,733 - mindformers[callback.py:317] - INFO -   66.1% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:44 }\n",
      "2023-11-17 10:21:06,967 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2484/ 3750], loss: 2.850, per_step_time: 554ms, lr: 1.7603523e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:06,967 - mindformers[callback.py:317] - INFO -   66.2% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:41 }\n",
      "2023-11-17 10:21:09,201 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2488/ 3750], loss: 2.090, per_step_time: 554ms, lr: 1.7551229e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:09,202 - mindformers[callback.py:317] - INFO -   66.3% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:39 }\n",
      "2023-11-17 10:21:11,436 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2492/ 3750], loss: 1.780, per_step_time: 554ms, lr: 1.7498935e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:11,436 - mindformers[callback.py:317] - INFO -   66.5% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:37 }\n",
      "2023-11-17 10:21:13,671 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2496/ 3750], loss: 2.806, per_step_time: 554ms, lr: 1.7446639e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:13,671 - mindformers[callback.py:317] - INFO -   66.6% |█████████████████████████████████                 | 14.42 samples/s/p  0:11:35 }\n",
      "2023-11-17 10:21:15,905 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2500/ 3750], loss: 2.001, per_step_time: 554ms, lr: 1.7394344e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:15,906 - mindformers[callback.py:317] - INFO -   66.7% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:33 }\n",
      "2023-11-17 10:21:15,917 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.24 seconds.\n",
      "2023-11-17 10:21:45,265 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.243658\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 54268), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:21:48,924 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2504/ 3750], loss: 2.158, per_step_time: 570ms, lr: 1.734205e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:48,925 - mindformers[callback.py:317] - INFO -   66.8% |█████████████████████████████████                 | 14.03 samples/s/p  0:11:50 }\n",
      "2023-11-17 10:21:51,160 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2508/ 3750], loss: 1.983, per_step_time: 554ms, lr: 1.7289756e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:51,160 - mindformers[callback.py:317] - INFO -   66.9% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:28 }\n",
      "2023-11-17 10:21:53,401 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2512/ 3750], loss: 2.814, per_step_time: 556ms, lr: 1.723746e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:53,401 - mindformers[callback.py:317] - INFO -   67.0% |█████████████████████████████████                 | 14.39 samples/s/p  0:11:28 }\n",
      "2023-11-17 10:21:55,642 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2516/ 3750], loss: 3.052, per_step_time: 556ms, lr: 1.7185166e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:55,643 - mindformers[callback.py:317] - INFO -   67.1% |█████████████████████████████████                 | 14.38 samples/s/p  0:11:26 }\n",
      "2023-11-17 10:21:57,880 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2520/ 3750], loss: 2.543, per_step_time: 555ms, lr: 1.7132872e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:21:57,881 - mindformers[callback.py:317] - INFO -   67.2% |█████████████████████████████████                 | 14.40 samples/s/p  0:11:23 }\n",
      "2023-11-17 10:22:00,115 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2524/ 3750], loss: 3.215, per_step_time: 554ms, lr: 1.7080578e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:00,115 - mindformers[callback.py:317] - INFO -   67.3% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:19 }\n",
      "2023-11-17 10:22:02,349 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2528/ 3750], loss: 2.320, per_step_time: 554ms, lr: 1.7028282e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:02,350 - mindformers[callback.py:317] - INFO -   67.4% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:17 }\n",
      "2023-11-17 10:22:04,583 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2532/ 3750], loss: 3.438, per_step_time: 554ms, lr: 1.6975988e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:04,584 - mindformers[callback.py:317] - INFO -   67.5% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:15 }\n",
      "2023-11-17 10:22:06,818 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2536/ 3750], loss: 2.484, per_step_time: 554ms, lr: 1.6923694e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:06,818 - mindformers[callback.py:317] - INFO -   67.6% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:13 }\n",
      "2023-11-17 10:22:09,070 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2540/ 3750], loss: 2.771, per_step_time: 558ms, lr: 1.68714e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:09,071 - mindformers[callback.py:317] - INFO -   67.7% |█████████████████████████████████                 | 14.31 samples/s/p  0:11:16 }\n",
      "2023-11-17 10:22:11,304 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2544/ 3750], loss: 3.149, per_step_time: 554ms, lr: 1.6819102e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:11,304 - mindformers[callback.py:317] - INFO -   67.8% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:08 }\n",
      "2023-11-17 10:22:13,538 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2548/ 3750], loss: 2.784, per_step_time: 554ms, lr: 1.6766808e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:13,539 - mindformers[callback.py:317] - INFO -   67.9% |█████████████████████████████████                 | 14.43 samples/s/p  0:11:06 }\n",
      "2023-11-17 10:22:15,776 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2552/ 3750], loss: 2.345, per_step_time: 555ms, lr: 1.6714514e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:15,776 - mindformers[callback.py:317] - INFO -   68.1% |██████████████████████████████████                | 14.41 samples/s/p  0:11:05 }\n",
      "2023-11-17 10:22:18,010 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2556/ 3750], loss: 3.042, per_step_time: 554ms, lr: 1.666222e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:18,011 - mindformers[callback.py:317] - INFO -   68.2% |██████████████████████████████████                | 14.43 samples/s/p  0:11:01 }\n",
      "2023-11-17 10:22:20,244 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2560/ 3750], loss: 2.091, per_step_time: 554ms, lr: 1.6609925e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:20,245 - mindformers[callback.py:317] - INFO -   68.3% |██████████████████████████████████                | 14.43 samples/s/p  0:10:59 }\n",
      "2023-11-17 10:22:22,479 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2564/ 3750], loss: 2.753, per_step_time: 554ms, lr: 1.6557631e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:22,480 - mindformers[callback.py:317] - INFO -   68.4% |██████████████████████████████████                | 14.43 samples/s/p  0:10:57 }\n",
      "2023-11-17 10:22:24,715 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2568/ 3750], loss: 2.625, per_step_time: 554ms, lr: 1.6505337e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:24,716 - mindformers[callback.py:317] - INFO -   68.5% |██████████████████████████████████                | 14.43 samples/s/p  0:10:55 }\n",
      "2023-11-17 10:22:26,953 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2572/ 3750], loss: 2.590, per_step_time: 555ms, lr: 1.6453043e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:26,954 - mindformers[callback.py:317] - INFO -   68.6% |██████████████████████████████████                | 14.41 samples/s/p  0:10:53 }\n",
      "INFO:root:Copy parallel total time cost: 42.46 seconds.\n",
      "2023-11-17 10:22:29,153 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.458141\n",
      "2023-11-17 10:22:29,189 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2576/ 3750], loss: 2.487, per_step_time: 554ms, lr: 1.6400747e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:29,190 - mindformers[callback.py:317] - INFO -   68.7% |██████████████████████████████████                | 14.42 samples/s/p  0:10:51 }\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:22:29,280 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.099839\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:22:29,350 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.041447\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:22:29,393 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.019745\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:22:31,432 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2580/ 3750], loss: 2.228, per_step_time: 556ms, lr: 1.6348453e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:31,432 - mindformers[callback.py:317] - INFO -   68.8% |██████████████████████████████████                | 14.38 samples/s/p  0:10:50 }\n",
      "2023-11-17 10:22:33,669 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2584/ 3750], loss: 2.458, per_step_time: 555ms, lr: 1.6296159e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:33,670 - mindformers[callback.py:317] - INFO -   68.9% |██████████████████████████████████                | 14.41 samples/s/p  0:10:47 }\n",
      "2023-11-17 10:22:35,913 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2588/ 3750], loss: 3.305, per_step_time: 556ms, lr: 1.6243865e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:35,913 - mindformers[callback.py:317] - INFO -   69.0% |██████████████████████████████████                | 14.38 samples/s/p  0:10:46 }\n",
      "2023-11-17 10:22:38,152 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2592/ 3750], loss: 2.996, per_step_time: 555ms, lr: 1.6191569e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:38,152 - mindformers[callback.py:317] - INFO -   69.1% |██████████████████████████████████                | 14.40 samples/s/p  0:10:43 }\n",
      "2023-11-17 10:22:40,392 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2596/ 3750], loss: 2.885, per_step_time: 555ms, lr: 1.6139275e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:40,392 - mindformers[callback.py:317] - INFO -   69.2% |██████████████████████████████████                | 14.40 samples/s/p  0:10:41 }\n",
      "2023-11-17 10:22:42,627 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2600/ 3750], loss: 2.185, per_step_time: 554ms, lr: 1.6086979e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:22:42,628 - mindformers[callback.py:317] - INFO -   69.3% |██████████████████████████████████                | 14.42 samples/s/p  0:10:37 }\n",
      "2023-11-17 10:22:42,640 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.33 seconds.\n",
      "2023-11-17 10:23:11,748 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.327450\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51030), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.12 seconds.\n",
      "2023-11-17 10:23:15,478 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2604/ 3750], loss: 2.394, per_step_time: 569ms, lr: 1.6034686e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:15,479 - mindformers[callback.py:317] - INFO -   69.4% |██████████████████████████████████                | 14.04 samples/s/p  0:10:52 }\n",
      "2023-11-17 10:23:17,715 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2608/ 3750], loss: 3.136, per_step_time: 554ms, lr: 1.598239e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:17,715 - mindformers[callback.py:317] - INFO -   69.5% |██████████████████████████████████                | 14.43 samples/s/p  0:10:33 }\n",
      "2023-11-17 10:23:19,950 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2612/ 3750], loss: 2.845, per_step_time: 554ms, lr: 1.5930093e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:19,950 - mindformers[callback.py:317] - INFO -   69.7% |██████████████████████████████████                | 14.43 samples/s/p  0:10:31 }\n",
      "2023-11-17 10:23:22,185 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2616/ 3750], loss: 2.130, per_step_time: 554ms, lr: 1.5877802e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:22,185 - mindformers[callback.py:317] - INFO -   69.8% |██████████████████████████████████                | 14.43 samples/s/p  0:10:28 }\n",
      "2023-11-17 10:23:24,443 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2620/ 3750], loss: 2.989, per_step_time: 554ms, lr: 1.5825508e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:24,444 - mindformers[callback.py:317] - INFO -   69.9% |██████████████████████████████████                | 14.42 samples/s/p  0:10:26 }\n",
      "2023-11-17 10:23:26,678 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2624/ 3750], loss: 2.966, per_step_time: 554ms, lr: 1.577321e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:26,679 - mindformers[callback.py:317] - INFO -   70.0% |██████████████████████████████████                | 14.43 samples/s/p  0:10:24 }\n",
      "2023-11-17 10:23:28,913 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2628/ 3750], loss: 2.503, per_step_time: 554ms, lr: 1.5720916e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:28,913 - mindformers[callback.py:317] - INFO -   70.1% |███████████████████████████████████               | 14.43 samples/s/p  0:10:22 }\n",
      "2023-11-17 10:23:31,148 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2632/ 3750], loss: 3.451, per_step_time: 554ms, lr: 1.5668622e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:31,149 - mindformers[callback.py:317] - INFO -   70.2% |███████████████████████████████████               | 14.43 samples/s/p  0:10:19 }\n",
      "2023-11-17 10:23:33,385 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2636/ 3750], loss: 2.701, per_step_time: 554ms, lr: 1.5616328e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:33,385 - mindformers[callback.py:317] - INFO -   70.3% |███████████████████████████████████               | 14.42 samples/s/p  0:10:18 }\n",
      "2023-11-17 10:23:35,620 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2640/ 3750], loss: 2.780, per_step_time: 554ms, lr: 1.5564032e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:35,621 - mindformers[callback.py:317] - INFO -   70.4% |███████████████████████████████████               | 14.43 samples/s/p  0:10:15 }\n",
      "2023-11-17 10:23:37,855 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2644/ 3750], loss: 2.713, per_step_time: 554ms, lr: 1.551174e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:37,856 - mindformers[callback.py:317] - INFO -   70.5% |███████████████████████████████████               | 14.43 samples/s/p  0:10:13 }\n",
      "2023-11-17 10:23:40,090 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2648/ 3750], loss: 3.126, per_step_time: 554ms, lr: 1.5459444e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:40,090 - mindformers[callback.py:317] - INFO -   70.6% |███████████████████████████████████               | 14.43 samples/s/p  0:10:10 }\n",
      "2023-11-17 10:23:42,331 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2652/ 3750], loss: 3.632, per_step_time: 555ms, lr: 1.540715e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:42,332 - mindformers[callback.py:317] - INFO -   70.7% |███████████████████████████████████               | 14.39 samples/s/p  0:10:10 }\n",
      "2023-11-17 10:23:44,575 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2656/ 3750], loss: 2.693, per_step_time: 556ms, lr: 1.5354855e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:44,576 - mindformers[callback.py:317] - INFO -   70.8% |███████████████████████████████████               | 14.39 samples/s/p  0:10:08 }\n",
      "2023-11-17 10:23:46,810 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2660/ 3750], loss: 2.641, per_step_time: 554ms, lr: 1.5302561e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:46,811 - mindformers[callback.py:317] - INFO -   70.9% |███████████████████████████████████               | 14.43 samples/s/p  0:10:04 }\n",
      "2023-11-17 10:23:49,045 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2664/ 3750], loss: 3.001, per_step_time: 554ms, lr: 1.5250268e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:49,046 - mindformers[callback.py:317] - INFO -   71.0% |███████████████████████████████████               | 14.43 samples/s/p  0:10:02 }\n",
      "2023-11-17 10:23:51,283 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2668/ 3750], loss: 2.632, per_step_time: 555ms, lr: 1.5197974e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:51,284 - mindformers[callback.py:317] - INFO -   71.1% |███████████████████████████████████               | 14.41 samples/s/p  0:10:00 }\n",
      "2023-11-17 10:23:53,520 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2672/ 3750], loss: 2.149, per_step_time: 554ms, lr: 1.5145677e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:53,520 - mindformers[callback.py:317] - INFO -   71.3% |███████████████████████████████████               | 14.42 samples/s/p  0:09:57 }\n",
      "2023-11-17 10:23:55,757 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2676/ 3750], loss: 2.400, per_step_time: 554ms, lr: 1.5093383e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:55,757 - mindformers[callback.py:317] - INFO -   71.4% |███████████████████████████████████               | 14.42 samples/s/p  0:09:55 }\n",
      "INFO:root:Copy parallel total time cost: 42.61 seconds.\n",
      "2023-11-17 10:23:55,863 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.610159\n",
      "INFO:root:List OBS time cost: 0.08 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.11 seconds.\n",
      "2023-11-17 10:23:56,004 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.114110\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:23:56,066 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.037435\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:23:56,111 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020220\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:23:57,996 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2680/ 3750], loss: 2.919, per_step_time: 555ms, lr: 1.5041089e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:23:57,997 - mindformers[callback.py:317] - INFO -   71.5% |███████████████████████████████████               | 14.40 samples/s/p  0:09:54 }\n",
      "2023-11-17 10:24:00,239 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2684/ 3750], loss: 2.818, per_step_time: 556ms, lr: 1.4988793e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:00,239 - mindformers[callback.py:317] - INFO -   71.6% |███████████████████████████████████               | 14.39 samples/s/p  0:09:52 }\n",
      "2023-11-17 10:24:02,489 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2688/ 3750], loss: 2.512, per_step_time: 557ms, lr: 1.4936499e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:02,489 - mindformers[callback.py:317] - INFO -   71.7% |███████████████████████████████████               | 14.35 samples/s/p  0:09:51 }\n",
      "2023-11-17 10:24:04,726 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2692/ 3750], loss: 2.373, per_step_time: 554ms, lr: 1.4884205e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:04,726 - mindformers[callback.py:317] - INFO -   71.8% |███████████████████████████████████               | 14.42 samples/s/p  0:09:46 }\n",
      "2023-11-17 10:24:06,961 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2696/ 3750], loss: 3.211, per_step_time: 554ms, lr: 1.483191e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:06,962 - mindformers[callback.py:317] - INFO -   71.9% |███████████████████████████████████               | 14.43 samples/s/p  0:09:44 }\n",
      "2023-11-17 10:24:09,196 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2700/ 3750], loss: 2.698, per_step_time: 554ms, lr: 1.4779616e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:09,196 - mindformers[callback.py:317] - INFO -   72.0% |████████████████████████████████████              | 14.43 samples/s/p  0:09:42 }\n",
      "2023-11-17 10:24:09,209 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.54 seconds.\n",
      "2023-11-17 10:24:38,676 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.540700\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 41764), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:24:42,397 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2704/ 3750], loss: 3.317, per_step_time: 580ms, lr: 1.4727322e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:42,398 - mindformers[callback.py:317] - INFO -   72.1% |████████████████████████████████████              | 13.78 samples/s/p  0:10:07 }\n",
      "2023-11-17 10:24:44,656 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2708/ 3750], loss: 2.133, per_step_time: 555ms, lr: 1.4675025e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:44,656 - mindformers[callback.py:317] - INFO -   72.2% |████████████████████████████████████              | 14.41 samples/s/p  0:09:38 }\n",
      "2023-11-17 10:24:46,894 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2712/ 3750], loss: 2.638, per_step_time: 555ms, lr: 1.4622733e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:46,895 - mindformers[callback.py:317] - INFO -   72.3% |████████████████████████████████████              | 14.41 samples/s/p  0:09:36 }\n",
      "2023-11-17 10:24:49,137 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2716/ 3750], loss: 3.543, per_step_time: 555ms, lr: 1.4570439e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:49,138 - mindformers[callback.py:317] - INFO -   72.4% |████████████████████████████████████              | 14.40 samples/s/p  0:09:34 }\n",
      "2023-11-17 10:24:51,376 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2720/ 3750], loss: 2.346, per_step_time: 555ms, lr: 1.4518145e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:51,376 - mindformers[callback.py:317] - INFO -   72.5% |████████████████████████████████████              | 14.41 samples/s/p  0:09:31 }\n",
      "2023-11-17 10:24:53,615 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2724/ 3750], loss: 3.200, per_step_time: 555ms, lr: 1.4465849e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:53,615 - mindformers[callback.py:317] - INFO -   72.6% |████████████████████████████████████              | 14.41 samples/s/p  0:09:29 }\n",
      "2023-11-17 10:24:55,854 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2728/ 3750], loss: 3.253, per_step_time: 555ms, lr: 1.4413555e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:55,855 - mindformers[callback.py:317] - INFO -   72.7% |████████████████████████████████████              | 14.40 samples/s/p  0:09:27 }\n",
      "2023-11-17 10:24:58,094 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2732/ 3750], loss: 3.439, per_step_time: 555ms, lr: 1.4361261e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:24:58,095 - mindformers[callback.py:317] - INFO -   72.9% |████████████████████████████████████              | 14.40 samples/s/p  0:09:25 }\n",
      "2023-11-17 10:25:00,374 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2736/ 3750], loss: 3.053, per_step_time: 565ms, lr: 1.43089665e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:00,374 - mindformers[callback.py:317] - INFO -   73.0% |████████████████████████████████████              | 14.15 samples/s/p  0:09:33 }\n",
      "2023-11-17 10:25:02,665 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2740/ 3750], loss: 2.491, per_step_time: 568ms, lr: 1.42566705e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:02,665 - mindformers[callback.py:317] - INFO -   73.1% |████████████████████████████████████              | 14.08 samples/s/p  0:09:33 }\n",
      "2023-11-17 10:25:04,901 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2744/ 3750], loss: 2.790, per_step_time: 554ms, lr: 1.4204376e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:04,902 - mindformers[callback.py:317] - INFO -   73.2% |████████████████████████████████████              | 14.42 samples/s/p  0:09:18 }\n",
      "2023-11-17 10:25:07,137 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2748/ 3750], loss: 2.336, per_step_time: 554ms, lr: 1.4152082e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:07,138 - mindformers[callback.py:317] - INFO -   73.3% |████████████████████████████████████              | 14.43 samples/s/p  0:09:15 }\n",
      "2023-11-17 10:25:09,373 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2752/ 3750], loss: 2.469, per_step_time: 554ms, lr: 1.4099788e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:09,374 - mindformers[callback.py:317] - INFO -   73.4% |████████████████████████████████████              | 14.42 samples/s/p  0:09:13 }\n",
      "2023-11-17 10:25:11,608 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2756/ 3750], loss: 2.691, per_step_time: 554ms, lr: 1.40474895e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:11,609 - mindformers[callback.py:317] - INFO -   73.5% |████████████████████████████████████              | 14.43 samples/s/p  0:09:11 }\n",
      "2023-11-17 10:25:13,843 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2760/ 3750], loss: 2.446, per_step_time: 554ms, lr: 1.3995197e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:13,844 - mindformers[callback.py:317] - INFO -   73.6% |████████████████████████████████████              | 14.43 samples/s/p  0:09:08 }\n",
      "2023-11-17 10:25:16,079 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2764/ 3750], loss: 3.021, per_step_time: 554ms, lr: 1.3942903e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:16,080 - mindformers[callback.py:317] - INFO -   73.7% |████████████████████████████████████              | 14.43 samples/s/p  0:09:06 }\n",
      "2023-11-17 10:25:18,318 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2768/ 3750], loss: 2.309, per_step_time: 555ms, lr: 1.3890612e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:18,318 - mindformers[callback.py:317] - INFO -   73.8% |████████████████████████████████████              | 14.41 samples/s/p  0:09:05 }\n",
      "2023-11-17 10:25:20,553 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2772/ 3750], loss: 3.262, per_step_time: 554ms, lr: 1.3838313e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:20,554 - mindformers[callback.py:317] - INFO -   73.9% |████████████████████████████████████              | 14.43 samples/s/p  0:09:02 }\n",
      "2023-11-17 10:25:22,789 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2776/ 3750], loss: 2.879, per_step_time: 554ms, lr: 1.3786019e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:22,789 - mindformers[callback.py:317] - INFO -   74.0% |█████████████████████████████████████             | 14.43 samples/s/p  0:09:00 }\n",
      "INFO:root:Copy parallel total time cost: 43.55 seconds.\n",
      "2023-11-17 10:25:23,685 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.550048\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:25:23,812 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.101703\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:25:23,876 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.037639\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:25:23,936 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.021683\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "2023-11-17 10:25:25,024 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2780/ 3750], loss: 2.666, per_step_time: 554ms, lr: 1.3733725e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:25,025 - mindformers[callback.py:317] - INFO -   74.1% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:57 }\n",
      "2023-11-17 10:25:27,261 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2784/ 3750], loss: 2.920, per_step_time: 554ms, lr: 1.3681432e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:27,261 - mindformers[callback.py:317] - INFO -   74.2% |█████████████████████████████████████             | 14.42 samples/s/p  0:08:55 }\n",
      "2023-11-17 10:25:29,504 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2788/ 3750], loss: 2.633, per_step_time: 556ms, lr: 1.36291355e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:29,504 - mindformers[callback.py:317] - INFO -   74.3% |█████████████████████████████████████             | 14.38 samples/s/p  0:08:55 }\n",
      "2023-11-17 10:25:31,745 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2792/ 3750], loss: 3.031, per_step_time: 555ms, lr: 1.3576841e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:31,745 - mindformers[callback.py:317] - INFO -   74.5% |█████████████████████████████████████             | 14.39 samples/s/p  0:08:52 }\n",
      "2023-11-17 10:25:33,980 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2796/ 3750], loss: 2.921, per_step_time: 554ms, lr: 1.3524547e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:33,980 - mindformers[callback.py:317] - INFO -   74.6% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:48 }\n",
      "2023-11-17 10:25:36,215 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2800/ 3750], loss: 3.259, per_step_time: 554ms, lr: 1.3472254e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:25:36,215 - mindformers[callback.py:317] - INFO -   74.7% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:46 }\n",
      "2023-11-17 10:25:36,228 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.66 seconds.\n",
      "2023-11-17 10:26:06,618 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.656843\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 41666), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:26:10,206 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2804/ 3750], loss: 2.147, per_step_time: 568ms, lr: 1.3419957e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:10,207 - mindformers[callback.py:317] - INFO -   74.8% |█████████████████████████████████████             | 14.07 samples/s/p  0:08:57 }\n",
      "2023-11-17 10:26:12,442 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2808/ 3750], loss: 2.479, per_step_time: 554ms, lr: 1.3367661e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:12,443 - mindformers[callback.py:317] - INFO -   74.9% |█████████████████████████████████████             | 14.44 samples/s/p  0:08:42 }\n",
      "2023-11-17 10:26:14,678 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2812/ 3750], loss: 1.905, per_step_time: 554ms, lr: 1.3315369e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:14,678 - mindformers[callback.py:317] - INFO -   75.0% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:40 }\n",
      "2023-11-17 10:26:16,913 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2816/ 3750], loss: 2.464, per_step_time: 554ms, lr: 1.3263075e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:16,913 - mindformers[callback.py:317] - INFO -   75.1% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:37 }\n",
      "2023-11-17 10:26:19,149 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2820/ 3750], loss: 2.721, per_step_time: 554ms, lr: 1.3210779e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:19,149 - mindformers[callback.py:317] - INFO -   75.2% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:35 }\n",
      "2023-11-17 10:26:21,386 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2824/ 3750], loss: 2.578, per_step_time: 554ms, lr: 1.3158485e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:21,386 - mindformers[callback.py:317] - INFO -   75.3% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:33 }\n",
      "2023-11-17 10:26:23,622 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2828/ 3750], loss: 1.901, per_step_time: 554ms, lr: 1.3106191e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:23,623 - mindformers[callback.py:317] - INFO -   75.4% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:31 }\n",
      "2023-11-17 10:26:25,857 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2832/ 3750], loss: 2.233, per_step_time: 554ms, lr: 1.30538965e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:25,858 - mindformers[callback.py:317] - INFO -   75.5% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:28 }\n",
      "2023-11-17 10:26:28,093 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2836/ 3750], loss: 2.466, per_step_time: 554ms, lr: 1.30016e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:28,094 - mindformers[callback.py:317] - INFO -   75.6% |█████████████████████████████████████             | 14.43 samples/s/p  0:08:26 }\n",
      "2023-11-17 10:26:30,331 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2840/ 3750], loss: 2.499, per_step_time: 554ms, lr: 1.2949307e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:30,331 - mindformers[callback.py:317] - INFO -   75.7% |█████████████████████████████████████             | 14.42 samples/s/p  0:08:24 }\n",
      "2023-11-17 10:26:32,573 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2844/ 3750], loss: 2.605, per_step_time: 555ms, lr: 1.2897013e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:32,573 - mindformers[callback.py:317] - INFO -   75.8% |█████████████████████████████████████             | 14.39 samples/s/p  0:08:23 }\n",
      "2023-11-17 10:26:34,814 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2848/ 3750], loss: 2.819, per_step_time: 555ms, lr: 1.2844717e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:34,814 - mindformers[callback.py:317] - INFO -   75.9% |█████████████████████████████████████             | 14.39 samples/s/p  0:08:21 }\n",
      "2023-11-17 10:26:37,050 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2852/ 3750], loss: 2.448, per_step_time: 554ms, lr: 1.2792421e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:37,050 - mindformers[callback.py:317] - INFO -   76.1% |██████████████████████████████████████            | 14.43 samples/s/p  0:08:17 }\n",
      "2023-11-17 10:26:39,285 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2856/ 3750], loss: 2.148, per_step_time: 554ms, lr: 1.2740129e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:39,286 - mindformers[callback.py:317] - INFO -   76.2% |██████████████████████████████████████            | 14.43 samples/s/p  0:08:15 }\n",
      "2023-11-17 10:26:41,523 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2860/ 3750], loss: 2.938, per_step_time: 554ms, lr: 1.2687835e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:41,524 - mindformers[callback.py:317] - INFO -   76.3% |██████████████████████████████████████            | 14.42 samples/s/p  0:08:13 }\n",
      "2023-11-17 10:26:43,763 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2864/ 3750], loss: 2.899, per_step_time: 554ms, lr: 1.2635539e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:43,763 - mindformers[callback.py:317] - INFO -   76.4% |██████████████████████████████████████            | 14.43 samples/s/p  0:08:11 }\n",
      "2023-11-17 10:26:45,999 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2868/ 3750], loss: 2.981, per_step_time: 554ms, lr: 1.2583244e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:46,000 - mindformers[callback.py:317] - INFO -   76.5% |██████████████████████████████████████            | 14.43 samples/s/p  0:08:09 }\n",
      "2023-11-17 10:26:48,234 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2872/ 3750], loss: 2.986, per_step_time: 554ms, lr: 1.253095e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:48,235 - mindformers[callback.py:317] - INFO -   76.6% |██████████████████████████████████████            | 14.43 samples/s/p  0:08:06 }\n",
      "2023-11-17 10:26:50,471 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2876/ 3750], loss: 2.841, per_step_time: 554ms, lr: 1.24786575e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:50,471 - mindformers[callback.py:317] - INFO -   76.7% |██████████████████████████████████████            | 14.42 samples/s/p  0:08:04 }\n",
      "INFO:root:Copy parallel total time cost: 42.79 seconds.\n",
      "2023-11-17 10:26:50,756 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.787097\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:26:50,882 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.100526\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:26:50,947 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039462\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:26:51,024 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.023724\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:26:52,707 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2880/ 3750], loss: 2.422, per_step_time: 554ms, lr: 1.24263615e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:52,707 - mindformers[callback.py:317] - INFO -   76.8% |██████████████████████████████████████            | 14.43 samples/s/p  0:08:02 }\n",
      "2023-11-17 10:26:54,943 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2884/ 3750], loss: 2.035, per_step_time: 554ms, lr: 1.2374064e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:54,944 - mindformers[callback.py:317] - INFO -   76.9% |██████████████████████████████████████            | 14.42 samples/s/p  0:08:00 }\n",
      "2023-11-17 10:26:57,180 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2888/ 3750], loss: 2.646, per_step_time: 554ms, lr: 1.2321772e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:57,180 - mindformers[callback.py:317] - INFO -   77.0% |██████████████████████████████████████            | 14.43 samples/s/p  0:07:57 }\n",
      "2023-11-17 10:26:59,416 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2892/ 3750], loss: 3.278, per_step_time: 554ms, lr: 1.2269478e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:26:59,416 - mindformers[callback.py:317] - INFO -   77.1% |██████████████████████████████████████            | 14.43 samples/s/p  0:07:55 }\n",
      "2023-11-17 10:27:01,651 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2896/ 3750], loss: 2.881, per_step_time: 554ms, lr: 1.2217185e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:01,652 - mindformers[callback.py:317] - INFO -   77.2% |██████████████████████████████████████            | 14.43 samples/s/p  0:07:53 }\n",
      "2023-11-17 10:27:03,889 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2900/ 3750], loss: 2.710, per_step_time: 554ms, lr: 1.2164886e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:03,889 - mindformers[callback.py:317] - INFO -   77.3% |██████████████████████████████████████            | 14.42 samples/s/p  0:07:51 }\n",
      "2023-11-17 10:27:03,902 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.07 seconds.\n",
      "2023-11-17 10:27:33,118 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.067688\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51864), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.12 seconds.\n",
      "2023-11-17 10:27:36,714 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2904/ 3750], loss: 2.602, per_step_time: 571ms, lr: 1.2112594e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:36,714 - mindformers[callback.py:317] - INFO -   77.4% |██████████████████████████████████████            | 14.00 samples/s/p  0:08:03 }\n",
      "2023-11-17 10:27:38,951 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2908/ 3750], loss: 2.593, per_step_time: 554ms, lr: 1.2060301e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:38,951 - mindformers[callback.py:317] - INFO -   77.5% |██████████████████████████████████████            | 14.43 samples/s/p  0:07:46 }\n",
      "2023-11-17 10:27:41,187 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2912/ 3750], loss: 2.789, per_step_time: 554ms, lr: 1.2008006e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:41,187 - mindformers[callback.py:317] - INFO -   77.7% |██████████████████████████████████████            | 14.43 samples/s/p  0:07:44 }\n",
      "2023-11-17 10:27:43,425 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2916/ 3750], loss: 2.590, per_step_time: 554ms, lr: 1.1955709e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:43,425 - mindformers[callback.py:317] - INFO -   77.8% |██████████████████████████████████████            | 14.43 samples/s/p  0:07:42 }\n",
      "2023-11-17 10:27:45,661 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2920/ 3750], loss: 2.402, per_step_time: 554ms, lr: 1.1903416e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:45,662 - mindformers[callback.py:317] - INFO -   77.9% |██████████████████████████████████████            | 14.43 samples/s/p  0:07:40 }\n",
      "2023-11-17 10:27:47,897 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2924/ 3750], loss: 2.226, per_step_time: 554ms, lr: 1.1851121e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:47,898 - mindformers[callback.py:317] - INFO -   78.0% |██████████████████████████████████████            | 14.43 samples/s/p  0:07:37 }\n",
      "2023-11-17 10:27:50,134 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2928/ 3750], loss: 2.523, per_step_time: 554ms, lr: 1.17988275e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:50,135 - mindformers[callback.py:317] - INFO -   78.1% |███████████████████████████████████████           | 14.43 samples/s/p  0:07:35 }\n",
      "2023-11-17 10:27:52,371 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2932/ 3750], loss: 2.059, per_step_time: 554ms, lr: 1.1746531e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:52,372 - mindformers[callback.py:317] - INFO -   78.2% |███████████████████████████████████████           | 14.42 samples/s/p  0:07:33 }\n",
      "2023-11-17 10:27:54,608 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2936/ 3750], loss: 2.349, per_step_time: 554ms, lr: 1.16942365e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:54,608 - mindformers[callback.py:317] - INFO -   78.3% |███████████████████████████████████████           | 14.43 samples/s/p  0:07:31 }\n",
      "2023-11-17 10:27:56,843 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2940/ 3750], loss: 3.124, per_step_time: 554ms, lr: 1.1641942e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:56,844 - mindformers[callback.py:317] - INFO -   78.4% |███████████████████████████████████████           | 14.43 samples/s/p  0:07:29 }\n",
      "2023-11-17 10:27:59,080 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2944/ 3750], loss: 2.772, per_step_time: 554ms, lr: 1.158965e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:27:59,080 - mindformers[callback.py:317] - INFO -   78.5% |███████████████████████████████████████           | 14.43 samples/s/p  0:07:26 }\n",
      "2023-11-17 10:28:01,317 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2948/ 3750], loss: 2.985, per_step_time: 554ms, lr: 1.1537352e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:01,317 - mindformers[callback.py:317] - INFO -   78.6% |███████████████████████████████████████           | 14.42 samples/s/p  0:07:24 }\n",
      "2023-11-17 10:28:03,555 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2952/ 3750], loss: 2.417, per_step_time: 554ms, lr: 1.1485058e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:03,555 - mindformers[callback.py:317] - INFO -   78.7% |███████████████████████████████████████           | 14.42 samples/s/p  0:07:22 }\n",
      "2023-11-17 10:28:05,794 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2956/ 3750], loss: 2.307, per_step_time: 555ms, lr: 1.1432766e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:05,794 - mindformers[callback.py:317] - INFO -   78.8% |███████████████████████████████████████           | 14.41 samples/s/p  0:07:20 }\n",
      "2023-11-17 10:28:08,032 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2960/ 3750], loss: 2.881, per_step_time: 554ms, lr: 1.138047e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:08,033 - mindformers[callback.py:317] - INFO -   78.9% |███████████████████████████████████████           | 14.41 samples/s/p  0:07:18 }\n",
      "2023-11-17 10:28:10,273 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2964/ 3750], loss: 2.291, per_step_time: 555ms, lr: 1.1328178e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:10,273 - mindformers[callback.py:317] - INFO -   79.0% |███████████████████████████████████████           | 14.40 samples/s/p  0:07:16 }\n",
      "2023-11-17 10:28:12,512 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2968/ 3750], loss: 2.788, per_step_time: 555ms, lr: 1.127588e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:12,512 - mindformers[callback.py:317] - INFO -   79.1% |███████████████████████████████████████           | 14.41 samples/s/p  0:07:14 }\n",
      "2023-11-17 10:28:14,748 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2972/ 3750], loss: 2.705, per_step_time: 554ms, lr: 1.1223587e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:14,748 - mindformers[callback.py:317] - INFO -   79.3% |███████████████████████████████████████           | 14.43 samples/s/p  0:07:11 }\n",
      "INFO:root:Copy parallel total time cost: 42.51 seconds.\n",
      "2023-11-17 10:28:16,976 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.508269\n",
      "2023-11-17 10:28:16,983 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2976/ 3750], loss: 2.559, per_step_time: 554ms, lr: 1.1171293e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:16,984 - mindformers[callback.py:317] - INFO -   79.4% |███████████████████████████████████████           | 14.43 samples/s/p  0:07:08 }\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:28:17,103 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.100240\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:28:17,168 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.041289\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:28:17,215 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.021459\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:28:19,219 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2980/ 3750], loss: 2.584, per_step_time: 554ms, lr: 1.1118999e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:19,220 - mindformers[callback.py:317] - INFO -   79.5% |███████████████████████████████████████           | 14.43 samples/s/p  0:07:06 }\n",
      "2023-11-17 10:28:21,456 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2984/ 3750], loss: 2.660, per_step_time: 554ms, lr: 1.1066702e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:21,456 - mindformers[callback.py:317] - INFO -   79.6% |███████████████████████████████████████           | 14.43 samples/s/p  0:07:04 }\n",
      "2023-11-17 10:28:23,699 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2988/ 3750], loss: 2.971, per_step_time: 556ms, lr: 1.1014408e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:23,700 - mindformers[callback.py:317] - INFO -   79.7% |███████████████████████████████████████           | 14.38 samples/s/p  0:07:03 }\n",
      "2023-11-17 10:28:25,944 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2992/ 3750], loss: 1.984, per_step_time: 556ms, lr: 1.0962115e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:25,944 - mindformers[callback.py:317] - INFO -   79.8% |███████████████████████████████████████           | 14.38 samples/s/p  0:07:01 }\n",
      "2023-11-17 10:28:28,181 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 2996/ 3750], loss: 2.679, per_step_time: 554ms, lr: 1.0909821e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:28,181 - mindformers[callback.py:317] - INFO -   79.9% |███████████████████████████████████████           | 14.43 samples/s/p  0:06:58 }\n",
      "2023-11-17 10:28:30,417 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3000/ 3750], loss: 3.003, per_step_time: 554ms, lr: 1.0857524e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:28:30,417 - mindformers[callback.py:317] - INFO -   80.0% |████████████████████████████████████████          | 14.43 samples/s/p  0:06:55 }\n",
      "2023-11-17 10:28:30,431 - mindformers[callback.py:553] - INFO - ......Saving ckpt......\n",
      "[WARNING] GE_ADPT(17499,ffff9167b0b0,python):2023-11-17-10:28:30.432.380 [mindspore/ccsrc/transform/graph_ir/graph_runner.cc:128] RunGraph] Get graph form DfGraphManager failed!\n",
      "[WARNING] DEVICE(17499,ffff9167b0b0,python):2023-11-17-10:28:30.432.426 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ascend_deprecated_interface.cc:172] DoExecNonInputGraph] Exec graph:save.25767_25758_12920_1_mindspore_train_dataset_helper__DataWrapper_construct_5465 failed\n",
      "WARNING:root:Retry=9, Wait=0.1, Timestamp=1700188112.946576, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57126), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=8, Wait=0.2, Timestamp=1700188113.062256, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57128), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=7, Wait=0.4, Timestamp=1700188113.2707863, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57130), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=6, Wait=0.8, Timestamp=1700188113.6793602, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57132), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=5, Wait=1.6, Timestamp=1700188114.488299, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57134), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=4, Wait=3.2, Timestamp=1700188116.0982914, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57136), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=3, Wait=6.4, Timestamp=1700188119.3073986, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57138), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=2, Wait=12.8, Timestamp=1700188125.719435, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 38274), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "WARNING:root:Retry=1, Wait=25.6, Timestamp=1700188138.5394487, Function=uploadFile, args=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True), kwargs={}, request-id=\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=128, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 55880), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "ERROR:root:Failed to call:\n",
      "\tfunc=<bound method ObsClient.uploadFile of <moxing.framework.file.file_io_obs_wrapper.ObsClientDecorated object at 0xfffe4032a130>>\n",
      "\targs=('lxy-guiyang1-output', 'glm2-2.2-output/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt', 10485760, 32, True)\n",
      "\tkwargs={}\n",
      "WARNING:root:skip file not found: /cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-500_4.ckpt\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=186, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57102), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=159, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46912), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=182, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46926), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=184, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46924), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=179, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57104), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=163, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57090), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=175, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46834), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=158, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 49544), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=185, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46932), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=160, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46910), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=165, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46832), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=189, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46930), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=183, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46840), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=168, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57106), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=181, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46844), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=164, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35638), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=166, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46836), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=172, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46920), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=169, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46918), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=173, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 49546), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=174, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46914), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=180, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 40032), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=162, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 40030), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=171, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35640), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=167, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46838), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=176, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35642), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=156, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 49542), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=187, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46928), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=127, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 40028), raddr=('100.125.6.131', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=125, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57086), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=161, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57088), raddr=('100.125.7.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/util/runtime.py:64: ResourceWarning: unclosed <ssl.SSLSocket fd=177, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46842), raddr=('100.125.6.3', 443)>\n",
      "  signal.signal(signal.SIGALRM, _handle_timeout)\n",
      "INFO:root:Copy parallel total time cost: 67.11 seconds.\n",
      "2023-11-17 10:29:24,353 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 67.111278\n",
      "2023-11-17 10:30:25,717 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 48988), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.12 seconds.\n",
      "2023-11-17 10:30:28,935 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3004/ 3750], loss: 2.668, per_step_time: 577ms, lr: 1.080523e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:28,935 - mindformers[callback.py:317] - INFO -   80.1% |████████████████████████████████████████          | 13.86 samples/s/p  0:07:10 }\n",
      "2023-11-17 10:30:31,208 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3008/ 3750], loss: 2.250, per_step_time: 555ms, lr: 1.0752936e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:31,208 - mindformers[callback.py:317] - INFO -   80.2% |████████████████████████████████████████          | 14.41 samples/s/p  0:06:51 }\n",
      "2023-11-17 10:30:33,449 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3012/ 3750], loss: 3.010, per_step_time: 554ms, lr: 1.0700642e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:33,450 - mindformers[callback.py:317] - INFO -   80.3% |████████████████████████████████████████          | 14.42 samples/s/p  0:06:49 }\n",
      "2023-11-17 10:30:35,693 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3016/ 3750], loss: 3.085, per_step_time: 555ms, lr: 1.0648345e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:35,693 - mindformers[callback.py:317] - INFO -   80.4% |████████████████████████████████████████          | 14.40 samples/s/p  0:06:47 }\n",
      "2023-11-17 10:30:37,950 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3020/ 3750], loss: 2.439, per_step_time: 559ms, lr: 1.0596053e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:37,950 - mindformers[callback.py:317] - INFO -   80.5% |████████████████████████████████████████          | 14.31 samples/s/p  0:06:48 }\n",
      "2023-11-17 10:30:40,190 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3024/ 3750], loss: 3.360, per_step_time: 554ms, lr: 1.0543758e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:40,191 - mindformers[callback.py:317] - INFO -   80.6% |████████████████████████████████████████          | 14.42 samples/s/p  0:06:42 }\n",
      "2023-11-17 10:30:42,431 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3028/ 3750], loss: 2.684, per_step_time: 554ms, lr: 1.0491464e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:42,432 - mindformers[callback.py:317] - INFO -   80.7% |████████████████████████████████████████          | 14.42 samples/s/p  0:06:40 }\n",
      "2023-11-17 10:30:44,675 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3032/ 3750], loss: 2.568, per_step_time: 555ms, lr: 1.04391665e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:44,675 - mindformers[callback.py:317] - INFO -   80.9% |████████████████████████████████████████          | 14.40 samples/s/p  0:06:38 }\n",
      "2023-11-17 10:30:46,920 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3036/ 3750], loss: 2.684, per_step_time: 555ms, lr: 1.0386872e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:46,921 - mindformers[callback.py:317] - INFO -   81.0% |████████████████████████████████████████          | 14.40 samples/s/p  0:06:36 }\n",
      "2023-11-17 10:30:49,169 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3040/ 3750], loss: 2.521, per_step_time: 555ms, lr: 1.033458e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:49,169 - mindformers[callback.py:317] - INFO -   81.1% |████████████████████████████████████████          | 14.40 samples/s/p  0:06:34 }\n",
      "2023-11-17 10:30:51,414 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3044/ 3750], loss: 2.651, per_step_time: 556ms, lr: 1.0282284e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:51,414 - mindformers[callback.py:317] - INFO -   81.2% |████████████████████████████████████████          | 14.38 samples/s/p  0:06:32 }\n",
      "2023-11-17 10:30:53,656 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3048/ 3750], loss: 2.516, per_step_time: 555ms, lr: 1.0229989e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:53,656 - mindformers[callback.py:317] - INFO -   81.3% |████████████████████████████████████████          | 14.40 samples/s/p  0:06:29 }\n",
      "2023-11-17 10:30:55,909 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3052/ 3750], loss: 2.857, per_step_time: 558ms, lr: 1.0177696e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:55,910 - mindformers[callback.py:317] - INFO -   81.4% |████████████████████████████████████████          | 14.33 samples/s/p  0:06:29 }\n",
      "2023-11-17 10:30:58,159 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3056/ 3750], loss: 2.131, per_step_time: 557ms, lr: 1.0125402e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:30:58,159 - mindformers[callback.py:317] - INFO -   81.5% |████████████████████████████████████████          | 14.35 samples/s/p  0:06:26 }\n",
      "2023-11-17 10:31:00,397 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3060/ 3750], loss: 2.419, per_step_time: 554ms, lr: 1.0073108e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:31:00,398 - mindformers[callback.py:317] - INFO -   81.6% |████████████████████████████████████████          | 14.42 samples/s/p  0:06:22 }\n",
      "2023-11-17 10:31:02,637 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3064/ 3750], loss: 2.865, per_step_time: 555ms, lr: 1.0020811e-05, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:31:02,638 - mindformers[callback.py:317] - INFO -   81.7% |████████████████████████████████████████          | 14.41 samples/s/p  0:06:20 }\n",
      "2023-11-17 10:31:04,878 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3068/ 3750], loss: 2.924, per_step_time: 555ms, lr: 9.968518e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:31:04,878 - mindformers[callback.py:317] - INFO -   81.8% |████████████████████████████████████████          | 14.41 samples/s/p  0:06:18 }\n",
      "2023-11-17 10:31:07,118 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3072/ 3750], loss: 2.712, per_step_time: 555ms, lr: 9.916224e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:31:07,118 - mindformers[callback.py:317] - INFO -   81.9% |████████████████████████████████████████          | 14.41 samples/s/p  0:06:16 }\n",
      "2023-11-17 10:31:09,366 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3076/ 3750], loss: 2.149, per_step_time: 557ms, lr: 9.863928e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:31:09,367 - mindformers[callback.py:317] - INFO -   82.0% |█████████████████████████████████████████         | 14.36 samples/s/p  0:06:15 }\n",
      "INFO:root:Copy parallel total time cost: 43.63 seconds.\n",
      "2023-11-17 10:31:10,295 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.631701\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:31:10,424 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.104445\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:31:10,490 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.042615\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:31:10,537 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.019428\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:31:11,616 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3080/ 3750], loss: 3.066, per_step_time: 557ms, lr: 9.8116325e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:31:11,617 - mindformers[callback.py:317] - INFO -   82.1% |█████████████████████████████████████████         | 14.36 samples/s/p  0:06:13 }\n",
      "2023-11-17 10:31:13,863 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3084/ 3750], loss: 2.322, per_step_time: 556ms, lr: 9.759338e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:31:13,863 - mindformers[callback.py:317] - INFO -   82.2% |█████████████████████████████████████████         | 14.38 samples/s/p  0:06:10 }\n",
      "2023-11-17 10:31:16,101 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3088/ 3750], loss: 2.625, per_step_time: 554ms, lr: 9.707044e-06, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:31:16,101 - mindformers[callback.py:317] - INFO -   82.3% |█████████████████████████████████████████         | 14.43 samples/s/p  0:06:07 }\n",
      "2023-11-17 10:31:18,344 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3092/ 3750], loss: 2.873, per_step_time: 555ms, lr: 9.65475e-06, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:31:18,344 - mindformers[callback.py:317] - INFO -   82.5% |█████████████████████████████████████████         | 14.39 samples/s/p  0:06:05 }\n",
      "2023-11-17 10:31:20,596 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3096/ 3750], loss: 2.760, per_step_time: 557ms, lr: 9.602454e-06, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:31:20,596 - mindformers[callback.py:317] - INFO -   82.6% |█████████████████████████████████████████         | 14.34 samples/s/p  0:06:04 }\n",
      "2023-11-17 10:31:22,834 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3100/ 3750], loss: 2.342, per_step_time: 554ms, lr: 9.550161e-06, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:31:22,834 - mindformers[callback.py:317] - INFO -   82.7% |█████████████████████████████████████████         | 14.43 samples/s/p  0:06:00 }\n",
      "2023-11-17 10:31:22,849 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.52 seconds.\n",
      "2023-11-17 10:31:53,087 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.519328\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 52312), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:31:56,766 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3104/ 3750], loss: 2.661, per_step_time: 577ms, lr: 9.497867e-06, overflow cond: False, loss_scale: 32768.0\n",
      "2023-11-17 10:31:56,767 - mindformers[callback.py:317] - INFO -   82.8% |█████████████████████████████████████████         | 13.86 samples/s/p  0:06:12 }\n",
      "2023-11-17 10:31:59,010 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3108/ 3750], loss: 3.361, per_step_time: 555ms, lr: 9.458645e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:31:59,010 - mindformers[callback.py:317] - INFO -   82.9% |█████████████████████████████████████████         | 14.40 samples/s/p  0:05:56 }\n",
      "2023-11-17 10:32:01,255 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3112/ 3750], loss: 3.223, per_step_time: 555ms, lr: 9.406351e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:01,256 - mindformers[callback.py:317] - INFO -   83.0% |█████████████████████████████████████████         | 14.40 samples/s/p  0:05:54 }\n",
      "2023-11-17 10:32:03,499 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3116/ 3750], loss: 2.663, per_step_time: 555ms, lr: 9.354055e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:03,499 - mindformers[callback.py:317] - INFO -   83.1% |█████████████████████████████████████████         | 14.39 samples/s/p  0:05:52 }\n",
      "2023-11-17 10:32:05,740 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3120/ 3750], loss: 2.660, per_step_time: 555ms, lr: 9.301762e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:05,740 - mindformers[callback.py:317] - INFO -   83.2% |█████████████████████████████████████████         | 14.40 samples/s/p  0:05:49 }\n",
      "2023-11-17 10:32:07,980 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3124/ 3750], loss: 2.059, per_step_time: 555ms, lr: 9.249467e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:07,981 - mindformers[callback.py:317] - INFO -   83.3% |█████████████████████████████████████████         | 14.41 samples/s/p  0:05:47 }\n",
      "2023-11-17 10:32:10,220 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3128/ 3750], loss: 2.924, per_step_time: 555ms, lr: 9.197173e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:10,221 - mindformers[callback.py:317] - INFO -   83.4% |█████████████████████████████████████████         | 14.41 samples/s/p  0:05:45 }\n",
      "2023-11-17 10:32:12,460 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3132/ 3750], loss: 2.493, per_step_time: 555ms, lr: 9.144876e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:12,461 - mindformers[callback.py:317] - INFO -   83.5% |█████████████████████████████████████████         | 14.41 samples/s/p  0:05:43 }\n",
      "2023-11-17 10:32:14,703 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3136/ 3750], loss: 2.803, per_step_time: 555ms, lr: 9.092583e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:14,703 - mindformers[callback.py:317] - INFO -   83.6% |█████████████████████████████████████████         | 14.39 samples/s/p  0:05:41 }\n",
      "2023-11-17 10:32:17,000 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3140/ 3750], loss: 2.461, per_step_time: 569ms, lr: 9.040289e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:17,000 - mindformers[callback.py:317] - INFO -   83.7% |█████████████████████████████████████████         | 14.05 samples/s/p  0:05:47 }\n",
      "2023-11-17 10:32:19,293 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3144/ 3750], loss: 2.720, per_step_time: 568ms, lr: 8.987995e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:19,294 - mindformers[callback.py:317] - INFO -   83.8% |█████████████████████████████████████████         | 14.07 samples/s/p  0:05:44 }\n",
      "2023-11-17 10:32:21,531 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3148/ 3750], loss: 2.892, per_step_time: 554ms, lr: 8.935702e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:21,531 - mindformers[callback.py:317] - INFO -   83.9% |█████████████████████████████████████████         | 14.43 samples/s/p  0:05:33 }\n",
      "2023-11-17 10:32:23,768 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3152/ 3750], loss: 2.522, per_step_time: 554ms, lr: 8.883404e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:23,768 - mindformers[callback.py:317] - INFO -   84.1% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:31 }\n",
      "2023-11-17 10:32:26,004 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3156/ 3750], loss: 2.795, per_step_time: 554ms, lr: 8.831111e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:26,004 - mindformers[callback.py:317] - INFO -   84.2% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:29 }\n",
      "2023-11-17 10:32:28,241 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3160/ 3750], loss: 2.582, per_step_time: 554ms, lr: 8.778817e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:28,241 - mindformers[callback.py:317] - INFO -   84.3% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:27 }\n",
      "2023-11-17 10:32:30,478 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3164/ 3750], loss: 2.911, per_step_time: 554ms, lr: 8.726523e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:30,479 - mindformers[callback.py:317] - INFO -   84.4% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:24 }\n",
      "2023-11-17 10:32:32,716 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3168/ 3750], loss: 2.770, per_step_time: 554ms, lr: 8.674227e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:32,717 - mindformers[callback.py:317] - INFO -   84.5% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:22 }\n",
      "2023-11-17 10:32:34,953 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3172/ 3750], loss: 3.329, per_step_time: 554ms, lr: 8.6219325e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:34,954 - mindformers[callback.py:317] - INFO -   84.6% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:20 }\n",
      "INFO:root:Copy parallel total time cost: 42.06 seconds.\n",
      "2023-11-17 10:32:36,555 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.061506\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.12 seconds.\n",
      "2023-11-17 10:32:36,701 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.116888\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:32:36,775 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.040941\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:32:36,827 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.025199\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:32:37,190 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3176/ 3750], loss: 2.691, per_step_time: 554ms, lr: 8.569638e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:37,191 - mindformers[callback.py:317] - INFO -   84.7% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:18 }\n",
      "2023-11-17 10:32:39,428 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3180/ 3750], loss: 2.290, per_step_time: 554ms, lr: 8.517345e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:39,428 - mindformers[callback.py:317] - INFO -   84.8% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:16 }\n",
      "2023-11-17 10:32:41,665 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3184/ 3750], loss: 2.404, per_step_time: 554ms, lr: 8.465047e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:41,665 - mindformers[callback.py:317] - INFO -   84.9% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:13 }\n",
      "2023-11-17 10:32:43,902 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3188/ 3750], loss: 3.080, per_step_time: 554ms, lr: 8.412753e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:43,903 - mindformers[callback.py:317] - INFO -   85.0% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:11 }\n",
      "2023-11-17 10:32:46,139 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3192/ 3750], loss: 3.099, per_step_time: 554ms, lr: 8.360459e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:46,140 - mindformers[callback.py:317] - INFO -   85.1% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:09 }\n",
      "2023-11-17 10:32:48,380 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3196/ 3750], loss: 2.526, per_step_time: 555ms, lr: 8.308167e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:48,380 - mindformers[callback.py:317] - INFO -   85.2% |██████████████████████████████████████████        | 14.41 samples/s/p  0:05:07 }\n",
      "2023-11-17 10:32:50,623 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3200/ 3750], loss: 3.021, per_step_time: 555ms, lr: 8.25587e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:32:50,623 - mindformers[callback.py:317] - INFO -   85.3% |██████████████████████████████████████████        | 14.39 samples/s/p  0:05:05 }\n",
      "2023-11-17 10:32:50,651 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.03 seconds.\n",
      "2023-11-17 10:33:18,888 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.034159\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43402), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.11 seconds.\n",
      "2023-11-17 10:33:22,487 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3204/ 3750], loss: 3.160, per_step_time: 574ms, lr: 8.203575e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:22,488 - mindformers[callback.py:317] - INFO -   85.4% |██████████████████████████████████████████        | 13.93 samples/s/p  0:05:13 }\n",
      "2023-11-17 10:33:24,726 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3208/ 3750], loss: 2.701, per_step_time: 554ms, lr: 8.151282e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:24,726 - mindformers[callback.py:317] - INFO -   85.5% |██████████████████████████████████████████        | 14.43 samples/s/p  0:05:00 }\n",
      "2023-11-17 10:33:26,964 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3212/ 3750], loss: 3.241, per_step_time: 554ms, lr: 8.0989885e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:26,965 - mindformers[callback.py:317] - INFO -   85.7% |██████████████████████████████████████████        | 14.42 samples/s/p  0:04:58 }\n",
      "2023-11-17 10:33:29,204 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3216/ 3750], loss: 2.464, per_step_time: 554ms, lr: 8.046692e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:29,205 - mindformers[callback.py:317] - INFO -   85.8% |██████████████████████████████████████████        | 14.43 samples/s/p  0:04:56 }\n",
      "2023-11-17 10:33:31,442 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3220/ 3750], loss: 2.707, per_step_time: 554ms, lr: 7.994396e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:31,443 - mindformers[callback.py:317] - INFO -   85.9% |██████████████████████████████████████████        | 14.43 samples/s/p  0:04:53 }\n",
      "2023-11-17 10:33:33,681 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3224/ 3750], loss: 2.447, per_step_time: 554ms, lr: 7.942104e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:33,681 - mindformers[callback.py:317] - INFO -   86.0% |██████████████████████████████████████████        | 14.42 samples/s/p  0:04:51 }\n",
      "2023-11-17 10:33:35,919 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3228/ 3750], loss: 3.249, per_step_time: 554ms, lr: 7.88981e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:35,919 - mindformers[callback.py:317] - INFO -   86.1% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:49 }\n",
      "2023-11-17 10:33:38,157 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3232/ 3750], loss: 2.847, per_step_time: 554ms, lr: 7.837512e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:38,157 - mindformers[callback.py:317] - INFO -   86.2% |███████████████████████████████████████████       | 14.42 samples/s/p  0:04:47 }\n",
      "2023-11-17 10:33:40,437 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3236/ 3750], loss: 1.895, per_step_time: 564ms, lr: 7.785219e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:40,437 - mindformers[callback.py:317] - INFO -   86.3% |███████████████████████████████████████████       | 14.16 samples/s/p  0:04:50 }\n",
      "2023-11-17 10:33:42,701 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3240/ 3750], loss: 2.426, per_step_time: 561ms, lr: 7.732926e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:42,701 - mindformers[callback.py:317] - INFO -   86.4% |███████████████████████████████████████████       | 14.26 samples/s/p  0:04:46 }\n",
      "2023-11-17 10:33:44,938 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3244/ 3750], loss: 2.838, per_step_time: 554ms, lr: 7.680632e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:44,938 - mindformers[callback.py:317] - INFO -   86.5% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:40 }\n",
      "2023-11-17 10:33:47,180 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3248/ 3750], loss: 2.334, per_step_time: 555ms, lr: 7.628335e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:47,180 - mindformers[callback.py:317] - INFO -   86.6% |███████████████████████████████████████████       | 14.40 samples/s/p  0:04:38 }\n",
      "2023-11-17 10:33:49,442 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3252/ 3750], loss: 2.270, per_step_time: 557ms, lr: 7.576041e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:49,442 - mindformers[callback.py:317] - INFO -   86.7% |███████████████████████████████████████████       | 14.35 samples/s/p  0:04:37 }\n",
      "2023-11-17 10:33:51,680 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3256/ 3750], loss: 3.330, per_step_time: 554ms, lr: 7.523747e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:51,680 - mindformers[callback.py:317] - INFO -   86.8% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:33 }\n",
      "2023-11-17 10:33:53,917 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3260/ 3750], loss: 2.909, per_step_time: 554ms, lr: 7.471453e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:53,918 - mindformers[callback.py:317] - INFO -   86.9% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:31 }\n",
      "2023-11-17 10:33:56,155 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3264/ 3750], loss: 2.287, per_step_time: 554ms, lr: 7.4191553e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:56,156 - mindformers[callback.py:317] - INFO -   87.0% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:29 }\n",
      "2023-11-17 10:33:58,393 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3268/ 3750], loss: 2.434, per_step_time: 554ms, lr: 7.3668634e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:33:58,393 - mindformers[callback.py:317] - INFO -   87.1% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:27 }\n",
      "2023-11-17 10:34:00,631 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3272/ 3750], loss: 3.217, per_step_time: 554ms, lr: 7.3145693e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:00,632 - mindformers[callback.py:317] - INFO -   87.3% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:25 }\n",
      "INFO:root:Copy parallel total time cost: 42.52 seconds.\n",
      "2023-11-17 10:34:02,772 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.522990\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "2023-11-17 10:34:02,871 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3276/ 3750], loss: 2.867, per_step_time: 554ms, lr: 7.262275e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:02,871 - mindformers[callback.py:317] - INFO -   87.4% |███████████████████████████████████████████       | 14.42 samples/s/p  0:04:22 }\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:34:02,900 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.104269\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:34:02,958 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.035749\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:34:03,004 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.019512\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:34:05,109 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3280/ 3750], loss: 2.045, per_step_time: 554ms, lr: 7.209978e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:05,109 - mindformers[callback.py:317] - INFO -   87.5% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:20 }\n",
      "2023-11-17 10:34:07,347 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3284/ 3750], loss: 2.412, per_step_time: 554ms, lr: 7.1576833e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:07,347 - mindformers[callback.py:317] - INFO -   87.6% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:18 }\n",
      "2023-11-17 10:34:09,585 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3288/ 3750], loss: 2.786, per_step_time: 554ms, lr: 7.1053896e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:09,585 - mindformers[callback.py:317] - INFO -   87.7% |███████████████████████████████████████████       | 14.43 samples/s/p  0:04:16 }\n",
      "2023-11-17 10:34:11,829 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3292/ 3750], loss: 3.129, per_step_time: 556ms, lr: 7.0530973e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:11,830 - mindformers[callback.py:317] - INFO -   87.8% |███████████████████████████████████████████       | 14.38 samples/s/p  0:04:14 }\n",
      "2023-11-17 10:34:14,068 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3296/ 3750], loss: 2.174, per_step_time: 554ms, lr: 7.0008e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:14,068 - mindformers[callback.py:317] - INFO -   87.9% |███████████████████████████████████████████       | 14.42 samples/s/p  0:04:11 }\n",
      "2023-11-17 10:34:16,320 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3300/ 3750], loss: 2.625, per_step_time: 558ms, lr: 6.948507e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:16,320 - mindformers[callback.py:317] - INFO -   88.0% |████████████████████████████████████████████      | 14.34 samples/s/p  0:04:11 }\n",
      "2023-11-17 10:34:16,335 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 41.76 seconds.\n",
      "2023-11-17 10:34:44,793 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 41.763713\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 38116), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.10 seconds.\n",
      "2023-11-17 10:34:48,064 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3304/ 3750], loss: 2.718, per_step_time: 571ms, lr: 6.8962117e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:48,065 - mindformers[callback.py:317] - INFO -   88.1% |████████████████████████████████████████████      | 14.00 samples/s/p  0:04:14 }\n",
      "2023-11-17 10:34:50,303 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3308/ 3750], loss: 2.375, per_step_time: 554ms, lr: 6.843919e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:50,303 - mindformers[callback.py:317] - INFO -   88.2% |████████████████████████████████████████████      | 14.43 samples/s/p  0:04:05 }\n",
      "2023-11-17 10:34:52,541 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3312/ 3750], loss: 2.647, per_step_time: 554ms, lr: 6.791621e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:52,541 - mindformers[callback.py:317] - INFO -   88.3% |████████████████████████████████████████████      | 14.43 samples/s/p  0:04:02 }\n",
      "2023-11-17 10:34:54,778 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3316/ 3750], loss: 3.072, per_step_time: 554ms, lr: 6.7393285e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:54,779 - mindformers[callback.py:317] - INFO -   88.4% |████████████████████████████████████████████      | 14.43 samples/s/p  0:04:00 }\n",
      "2023-11-17 10:34:57,020 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3320/ 3750], loss: 3.606, per_step_time: 555ms, lr: 6.687034e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:57,021 - mindformers[callback.py:317] - INFO -   88.5% |████████████████████████████████████████████      | 14.40 samples/s/p  0:03:58 }\n",
      "2023-11-17 10:34:59,263 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3324/ 3750], loss: 2.936, per_step_time: 554ms, lr: 6.63474e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:34:59,264 - mindformers[callback.py:317] - INFO -   88.6% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:56 }\n",
      "2023-11-17 10:35:01,502 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3328/ 3750], loss: 3.130, per_step_time: 554ms, lr: 6.5824424e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:01,502 - mindformers[callback.py:317] - INFO -   88.7% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:54 }\n",
      "2023-11-17 10:35:03,740 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3332/ 3750], loss: 2.143, per_step_time: 554ms, lr: 6.5301497e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:03,740 - mindformers[callback.py:317] - INFO -   88.9% |████████████████████████████████████████████      | 14.42 samples/s/p  0:03:51 }\n",
      "2023-11-17 10:35:05,978 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3336/ 3750], loss: 2.559, per_step_time: 554ms, lr: 6.4778555e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:05,978 - mindformers[callback.py:317] - INFO -   89.0% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:49 }\n",
      "2023-11-17 10:35:08,215 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3340/ 3750], loss: 2.408, per_step_time: 554ms, lr: 6.4255614e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:08,216 - mindformers[callback.py:317] - INFO -   89.1% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:47 }\n",
      "2023-11-17 10:35:10,453 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3344/ 3750], loss: 2.876, per_step_time: 554ms, lr: 6.373266e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:10,454 - mindformers[callback.py:317] - INFO -   89.2% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:45 }\n",
      "2023-11-17 10:35:12,694 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3348/ 3750], loss: 2.583, per_step_time: 555ms, lr: 6.320972e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:12,695 - mindformers[callback.py:317] - INFO -   89.3% |████████████████████████████████████████████      | 14.41 samples/s/p  0:03:43 }\n",
      "2023-11-17 10:35:14,940 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3352/ 3750], loss: 2.029, per_step_time: 556ms, lr: 6.2686777e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:14,940 - mindformers[callback.py:317] - INFO -   89.4% |████████████████████████████████████████████      | 14.38 samples/s/p  0:03:41 }\n",
      "2023-11-17 10:35:17,180 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3356/ 3750], loss: 2.740, per_step_time: 555ms, lr: 6.2163826e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:17,180 - mindformers[callback.py:317] - INFO -   89.5% |████████████████████████████████████████████      | 14.41 samples/s/p  0:03:38 }\n",
      "2023-11-17 10:35:19,417 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3360/ 3750], loss: 3.356, per_step_time: 554ms, lr: 6.1640862e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:19,418 - mindformers[callback.py:317] - INFO -   89.6% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:36 }\n",
      "2023-11-17 10:35:21,655 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3364/ 3750], loss: 2.097, per_step_time: 554ms, lr: 6.111792e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:21,655 - mindformers[callback.py:317] - INFO -   89.7% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:34 }\n",
      "2023-11-17 10:35:23,893 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3368/ 3750], loss: 2.062, per_step_time: 554ms, lr: 6.0595e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:23,894 - mindformers[callback.py:317] - INFO -   89.8% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:31 }\n",
      "2023-11-17 10:35:26,131 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3372/ 3750], loss: 2.360, per_step_time: 554ms, lr: 6.0072057e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:26,131 - mindformers[callback.py:317] - INFO -   89.9% |████████████████████████████████████████████      | 14.43 samples/s/p  0:03:29 }\n",
      "2023-11-17 10:35:28,371 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3376/ 3750], loss: 2.908, per_step_time: 555ms, lr: 5.954909e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:28,372 - mindformers[callback.py:317] - INFO -   90.0% |█████████████████████████████████████████████     | 14.41 samples/s/p  0:03:27 }\n",
      "INFO:root:Copy parallel total time cost: 43.66 seconds.\n",
      "2023-11-17 10:35:29,495 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.662934\n",
      "INFO:root:List OBS time cost: 0.08 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.13 seconds.\n",
      "2023-11-17 10:35:29,658 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.134155\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:35:29,722 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.039773\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:35:29,772 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.024627\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:35:30,612 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3380/ 3750], loss: 2.536, per_step_time: 555ms, lr: 5.9026147e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:30,613 - mindformers[callback.py:317] - INFO -   90.1% |█████████████████████████████████████████████     | 14.41 samples/s/p  0:03:25 }\n",
      "2023-11-17 10:35:32,852 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3384/ 3750], loss: 2.104, per_step_time: 554ms, lr: 5.850321e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:32,852 - mindformers[callback.py:317] - INFO -   90.2% |█████████████████████████████████████████████     | 14.42 samples/s/p  0:03:23 }\n",
      "2023-11-17 10:35:35,090 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3388/ 3750], loss: 2.402, per_step_time: 554ms, lr: 5.7980274e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:35,090 - mindformers[callback.py:317] - INFO -   90.3% |█████████████████████████████████████████████     | 14.43 samples/s/p  0:03:20 }\n",
      "2023-11-17 10:35:37,328 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3392/ 3750], loss: 2.423, per_step_time: 554ms, lr: 5.74573e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:37,328 - mindformers[callback.py:317] - INFO -   90.5% |█████████████████████████████████████████████     | 14.43 samples/s/p  0:03:18 }\n",
      "2023-11-17 10:35:39,565 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3396/ 3750], loss: 2.650, per_step_time: 554ms, lr: 5.6934364e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:39,565 - mindformers[callback.py:317] - INFO -   90.6% |█████████████████████████████████████████████     | 14.43 samples/s/p  0:03:16 }\n",
      "2023-11-17 10:35:41,803 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3400/ 3750], loss: 2.027, per_step_time: 554ms, lr: 5.6411423e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:35:41,803 - mindformers[callback.py:317] - INFO -   90.7% |█████████████████████████████████████████████     | 14.43 samples/s/p  0:03:14 }\n",
      "2023-11-17 10:35:41,819 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 43.56 seconds.\n",
      "2023-11-17 10:36:13,359 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 43.556939\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 42254), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:36:16,625 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3404/ 3750], loss: 3.216, per_step_time: 569ms, lr: 5.5888486e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:16,626 - mindformers[callback.py:317] - INFO -   90.8% |█████████████████████████████████████████████     | 14.06 samples/s/p  0:03:16 }\n",
      "2023-11-17 10:36:18,868 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3408/ 3750], loss: 2.482, per_step_time: 554ms, lr: 5.5365545e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:18,868 - mindformers[callback.py:317] - INFO -   90.9% |█████████████████████████████████████████████     | 14.42 samples/s/p  0:03:09 }\n",
      "2023-11-17 10:36:21,155 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3412/ 3750], loss: 2.126, per_step_time: 566ms, lr: 5.484258e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:21,155 - mindformers[callback.py:317] - INFO -   91.0% |█████████████████████████████████████████████     | 14.13 samples/s/p  0:03:11 }\n",
      "2023-11-17 10:36:23,409 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3416/ 3750], loss: 2.137, per_step_time: 558ms, lr: 5.431965e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:23,410 - mindformers[callback.py:317] - INFO -   91.1% |█████████████████████████████████████████████     | 14.33 samples/s/p  0:03:06 }\n",
      "2023-11-17 10:36:25,649 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3420/ 3750], loss: 3.005, per_step_time: 554ms, lr: 5.379671e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:25,650 - mindformers[callback.py:317] - INFO -   91.2% |█████████████████████████████████████████████     | 14.42 samples/s/p  0:03:03 }\n",
      "2023-11-17 10:36:27,891 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3424/ 3750], loss: 2.412, per_step_time: 555ms, lr: 5.327377e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:27,892 - mindformers[callback.py:317] - INFO -   91.3% |█████████████████████████████████████████████     | 14.41 samples/s/p  0:03:00 }\n",
      "2023-11-17 10:36:30,134 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3428/ 3750], loss: 2.217, per_step_time: 555ms, lr: 5.2750806e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:30,135 - mindformers[callback.py:317] - INFO -   91.4% |█████████████████████████████████████████████     | 14.40 samples/s/p  0:02:58 }\n",
      "2023-11-17 10:36:32,376 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3432/ 3750], loss: 2.037, per_step_time: 555ms, lr: 5.222786e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:32,377 - mindformers[callback.py:317] - INFO -   91.5% |█████████████████████████████████████████████     | 14.41 samples/s/p  0:02:56 }\n",
      "2023-11-17 10:36:34,615 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3436/ 3750], loss: 2.771, per_step_time: 554ms, lr: 5.1704924e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:34,615 - mindformers[callback.py:317] - INFO -   91.6% |█████████████████████████████████████████████     | 14.43 samples/s/p  0:02:54 }\n",
      "2023-11-17 10:36:36,853 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3440/ 3750], loss: 2.212, per_step_time: 554ms, lr: 5.1181987e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:36,854 - mindformers[callback.py:317] - INFO -   91.7% |█████████████████████████████████████████████     | 14.43 samples/s/p  0:02:51 }\n",
      "2023-11-17 10:36:39,093 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3444/ 3750], loss: 2.731, per_step_time: 554ms, lr: 5.0659023e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:39,093 - mindformers[callback.py:317] - INFO -   91.8% |█████████████████████████████████████████████     | 14.43 samples/s/p  0:02:49 }\n",
      "2023-11-17 10:36:41,339 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3448/ 3750], loss: 2.184, per_step_time: 555ms, lr: 5.013608e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:41,339 - mindformers[callback.py:317] - INFO -   91.9% |█████████████████████████████████████████████     | 14.41 samples/s/p  0:02:47 }\n",
      "2023-11-17 10:36:43,583 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3452/ 3750], loss: 2.645, per_step_time: 555ms, lr: 4.9613136e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:43,583 - mindformers[callback.py:317] - INFO -   92.1% |██████████████████████████████████████████████    | 14.39 samples/s/p  0:02:45 }\n",
      "2023-11-17 10:36:45,821 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3456/ 3750], loss: 2.399, per_step_time: 554ms, lr: 4.9090204e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:45,822 - mindformers[callback.py:317] - INFO -   92.2% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:43 }\n",
      "2023-11-17 10:36:48,060 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3460/ 3750], loss: 2.510, per_step_time: 554ms, lr: 4.8567235e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:48,060 - mindformers[callback.py:317] - INFO -   92.3% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:40 }\n",
      "2023-11-17 10:36:50,303 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3464/ 3750], loss: 3.732, per_step_time: 555ms, lr: 4.80443e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:50,304 - mindformers[callback.py:317] - INFO -   92.4% |██████████████████████████████████████████████    | 14.39 samples/s/p  0:02:38 }\n",
      "2023-11-17 10:36:52,547 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3468/ 3750], loss: 1.841, per_step_time: 555ms, lr: 4.7521357e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:52,548 - mindformers[callback.py:317] - INFO -   92.5% |██████████████████████████████████████████████    | 14.40 samples/s/p  0:02:36 }\n",
      "2023-11-17 10:36:54,787 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3472/ 3750], loss: 2.932, per_step_time: 554ms, lr: 4.6998416e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:54,787 - mindformers[callback.py:317] - INFO -   92.6% |██████████████████████████████████████████████    | 14.42 samples/s/p  0:02:34 }\n",
      "INFO:root:Copy parallel total time cost: 42.49 seconds.\n",
      "2023-11-17 10:36:56,882 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.487731\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.10 seconds.\n",
      "2023-11-17 10:36:57,010 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.103759\n",
      "2023-11-17 10:36:57,030 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3476/ 3750], loss: 2.525, per_step_time: 555ms, lr: 4.647545e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:57,030 - mindformers[callback.py:317] - INFO -   92.7% |██████████████████████████████████████████████    | 14.40 samples/s/p  0:02:32 }\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/modelarts/model_analysis_results.json'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:36:57,075 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.038538\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:36:57,126 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.023494\n",
      "INFO:root:List OBS time cost: 0.03 seconds.\n",
      "2023-11-17 10:36:59,267 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3480/ 3750], loss: 3.047, per_step_time: 554ms, lr: 4.595252e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:36:59,267 - mindformers[callback.py:317] - INFO -   92.8% |██████████████████████████████████████████████    | 14.44 samples/s/p  0:02:29 }\n",
      "2023-11-17 10:37:01,505 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3484/ 3750], loss: 2.991, per_step_time: 554ms, lr: 4.542957e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:01,505 - mindformers[callback.py:317] - INFO -   92.9% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:27 }\n",
      "2023-11-17 10:37:03,743 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3488/ 3750], loss: 2.616, per_step_time: 554ms, lr: 4.4906637e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:03,743 - mindformers[callback.py:317] - INFO -   93.0% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:25 }\n",
      "2023-11-17 10:37:05,998 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3492/ 3750], loss: 2.381, per_step_time: 554ms, lr: 4.4383673e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:05,999 - mindformers[callback.py:317] - INFO -   93.1% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:23 }\n",
      "2023-11-17 10:37:08,236 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3496/ 3750], loss: 1.969, per_step_time: 554ms, lr: 4.3860723e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:08,236 - mindformers[callback.py:317] - INFO -   93.2% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:20 }\n",
      "2023-11-17 10:37:10,475 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3500/ 3750], loss: 2.389, per_step_time: 554ms, lr: 4.3337795e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:10,475 - mindformers[callback.py:317] - INFO -   93.3% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:18 }\n",
      "2023-11-17 10:37:10,492 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 42.32 seconds.\n",
      "2023-11-17 10:37:39,479 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 42.322031\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34146), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.13 seconds.\n",
      "2023-11-17 10:37:42,746 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3504/ 3750], loss: 2.698, per_step_time: 572ms, lr: 4.2814854e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:42,746 - mindformers[callback.py:317] - INFO -   93.4% |██████████████████████████████████████████████    | 13.97 samples/s/p  0:02:20 }\n",
      "2023-11-17 10:37:44,987 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3508/ 3750], loss: 2.645, per_step_time: 554ms, lr: 4.229189e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:44,987 - mindformers[callback.py:317] - INFO -   93.5% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:14 }\n",
      "2023-11-17 10:37:47,226 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3512/ 3750], loss: 2.927, per_step_time: 554ms, lr: 4.176895e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:47,227 - mindformers[callback.py:317] - INFO -   93.7% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:11 }\n",
      "2023-11-17 10:37:49,471 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3516/ 3750], loss: 2.996, per_step_time: 555ms, lr: 4.124601e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:49,472 - mindformers[callback.py:317] - INFO -   93.8% |██████████████████████████████████████████████    | 14.39 samples/s/p  0:02:10 }\n",
      "2023-11-17 10:37:51,711 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3520/ 3750], loss: 2.542, per_step_time: 554ms, lr: 4.0723075e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:51,712 - mindformers[callback.py:317] - INFO -   93.9% |██████████████████████████████████████████████    | 14.43 samples/s/p  0:02:07 }\n",
      "2023-11-17 10:37:53,959 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3524/ 3750], loss: 2.530, per_step_time: 556ms, lr: 4.020011e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:53,959 - mindformers[callback.py:317] - INFO -   94.0% |██████████████████████████████████████████████    | 14.37 samples/s/p  0:02:05 }\n",
      "2023-11-17 10:37:56,207 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3528/ 3750], loss: 2.576, per_step_time: 556ms, lr: 3.967717e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:56,208 - mindformers[callback.py:317] - INFO -   94.1% |███████████████████████████████████████████████   | 14.37 samples/s/p  0:02:03 }\n",
      "2023-11-17 10:37:58,458 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3532/ 3750], loss: 2.571, per_step_time: 557ms, lr: 3.9154224e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:37:58,459 - mindformers[callback.py:317] - INFO -   94.2% |███████████████████████████████████████████████   | 14.35 samples/s/p  0:02:01 }\n",
      "2023-11-17 10:38:00,708 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3536/ 3750], loss: 3.146, per_step_time: 557ms, lr: 3.8631288e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:00,708 - mindformers[callback.py:317] - INFO -   94.3% |███████████████████████████████████████████████   | 14.36 samples/s/p  0:01:59 }\n",
      "2023-11-17 10:38:02,947 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3540/ 3750], loss: 2.422, per_step_time: 554ms, lr: 3.8108321e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:02,947 - mindformers[callback.py:317] - INFO -   94.4% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:56 }\n",
      "2023-11-17 10:38:05,186 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3544/ 3750], loss: 2.199, per_step_time: 554ms, lr: 3.758538e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:05,186 - mindformers[callback.py:317] - INFO -   94.5% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:54 }\n",
      "2023-11-17 10:38:07,427 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3548/ 3750], loss: 2.931, per_step_time: 555ms, lr: 3.7062446e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:07,427 - mindformers[callback.py:317] - INFO -   94.6% |███████████████████████████████████████████████   | 14.41 samples/s/p  0:01:52 }\n",
      "2023-11-17 10:38:09,668 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3552/ 3750], loss: 2.206, per_step_time: 554ms, lr: 3.653951e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:09,668 - mindformers[callback.py:317] - INFO -   94.7% |███████████████████████████████████████████████   | 14.42 samples/s/p  0:01:49 }\n",
      "2023-11-17 10:38:11,909 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3556/ 3750], loss: 2.979, per_step_time: 555ms, lr: 3.6016538e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:11,910 - mindformers[callback.py:317] - INFO -   94.8% |███████████████████████████████████████████████   | 14.41 samples/s/p  0:01:47 }\n",
      "2023-11-17 10:38:14,151 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3560/ 3750], loss: 3.109, per_step_time: 555ms, lr: 3.5493597e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:14,151 - mindformers[callback.py:317] - INFO -   94.9% |███████████████████████████████████████████████   | 14.41 samples/s/p  0:01:45 }\n",
      "2023-11-17 10:38:16,390 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3564/ 3750], loss: 2.244, per_step_time: 554ms, lr: 3.4970662e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:16,391 - mindformers[callback.py:317] - INFO -   95.0% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:43 }\n",
      "2023-11-17 10:38:18,630 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3568/ 3750], loss: 2.116, per_step_time: 554ms, lr: 3.4447726e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:18,631 - mindformers[callback.py:317] - INFO -   95.1% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:40 }\n",
      "2023-11-17 10:38:20,874 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3572/ 3750], loss: 2.408, per_step_time: 554ms, lr: 3.3924762e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:20,875 - mindformers[callback.py:317] - INFO -   95.3% |███████████████████████████████████████████████   | 14.42 samples/s/p  0:01:38 }\n",
      "2023-11-17 10:38:23,113 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3576/ 3750], loss: 2.141, per_step_time: 554ms, lr: 3.3401816e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:23,114 - mindformers[callback.py:317] - INFO -   95.4% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:36 }\n",
      "2023-11-17 10:38:25,352 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3580/ 3750], loss: 3.186, per_step_time: 554ms, lr: 3.2878881e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:25,353 - mindformers[callback.py:317] - INFO -   95.5% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:34 }\n",
      "INFO:root:Copy parallel total time cost: 45.97 seconds.\n",
      "2023-11-17 10:38:26,470 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 45.966785\n",
      "INFO:root:List OBS time cost: 0.11 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.15 seconds.\n",
      "2023-11-17 10:38:26,642 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.145849\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.05 seconds.\n",
      "2023-11-17 10:38:26,712 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.046212\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:38:26,758 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020267\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:38:27,591 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3584/ 3750], loss: 2.279, per_step_time: 554ms, lr: 3.235594e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:27,591 - mindformers[callback.py:317] - INFO -   95.6% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:32 }\n",
      "2023-11-17 10:38:29,830 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3588/ 3750], loss: 2.433, per_step_time: 554ms, lr: 3.1832976e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:29,830 - mindformers[callback.py:317] - INFO -   95.7% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:29 }\n",
      "2023-11-17 10:38:32,069 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3592/ 3750], loss: 2.652, per_step_time: 554ms, lr: 3.1310035e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:32,070 - mindformers[callback.py:317] - INFO -   95.8% |███████████████████████████████████████████████   | 14.43 samples/s/p  0:01:27 }\n",
      "2023-11-17 10:38:34,311 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3596/ 3750], loss: 2.064, per_step_time: 555ms, lr: 3.07871e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:34,312 - mindformers[callback.py:317] - INFO -   95.9% |███████████████████████████████████████████████   | 14.41 samples/s/p  0:01:25 }\n",
      "2023-11-17 10:38:36,553 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3600/ 3750], loss: 3.177, per_step_time: 555ms, lr: 3.026416e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:38:36,554 - mindformers[callback.py:317] - INFO -   96.0% |████████████████████████████████████████████████  | 14.41 samples/s/p  0:01:23 }\n",
      "2023-11-17 10:38:36,570 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 44.53 seconds.\n",
      "2023-11-17 10:39:11,313 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 44.528177\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 40088), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.09 seconds.\n",
      "2023-11-17 10:39:14,938 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3604/ 3750], loss: 2.981, per_step_time: 570ms, lr: 2.9741195e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:14,939 - mindformers[callback.py:317] - INFO -   96.1% |████████████████████████████████████████████████  | 14.03 samples/s/p  0:01:23 }\n",
      "2023-11-17 10:39:17,179 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3608/ 3750], loss: 2.826, per_step_time: 554ms, lr: 2.9218259e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:17,180 - mindformers[callback.py:317] - INFO -   96.2% |████████████████████████████████████████████████  | 14.43 samples/s/p  0:01:18 }\n",
      "2023-11-17 10:39:19,418 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3612/ 3750], loss: 2.370, per_step_time: 554ms, lr: 2.8695317e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:19,419 - mindformers[callback.py:317] - INFO -   96.3% |████████████████████████████████████████████████  | 14.43 samples/s/p  0:01:16 }\n",
      "2023-11-17 10:39:21,658 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3616/ 3750], loss: 2.857, per_step_time: 554ms, lr: 2.8172376e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:21,659 - mindformers[callback.py:317] - INFO -   96.4% |████████████████████████████████████████████████  | 14.42 samples/s/p  0:01:14 }\n",
      "2023-11-17 10:39:23,901 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3620/ 3750], loss: 2.937, per_step_time: 555ms, lr: 2.7649407e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:23,902 - mindformers[callback.py:317] - INFO -   96.5% |████████████████████████████████████████████████  | 14.41 samples/s/p  0:01:12 }\n",
      "2023-11-17 10:39:26,144 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3624/ 3750], loss: 3.604, per_step_time: 555ms, lr: 2.712647e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:26,145 - mindformers[callback.py:317] - INFO -   96.6% |████████████████████████████████████████████████  | 14.40 samples/s/p  0:01:09 }\n",
      "2023-11-17 10:39:28,386 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3628/ 3750], loss: 2.348, per_step_time: 555ms, lr: 2.6603534e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:28,386 - mindformers[callback.py:317] - INFO -   96.7% |████████████████████████████████████████████████  | 14.41 samples/s/p  0:01:07 }\n",
      "2023-11-17 10:39:30,629 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3632/ 3750], loss: 2.350, per_step_time: 555ms, lr: 2.6080593e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:30,629 - mindformers[callback.py:317] - INFO -   96.9% |████████████████████████████████████████████████  | 14.41 samples/s/p  0:01:05 }\n",
      "2023-11-17 10:39:32,871 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3636/ 3750], loss: 1.683, per_step_time: 555ms, lr: 2.5557624e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:32,871 - mindformers[callback.py:317] - INFO -   97.0% |████████████████████████████████████████████████  | 14.41 samples/s/p  0:01:03 }\n",
      "2023-11-17 10:39:35,114 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3640/ 3750], loss: 2.459, per_step_time: 555ms, lr: 2.5034687e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:35,114 - mindformers[callback.py:317] - INFO -   97.1% |████████████████████████████████████████████████  | 14.41 samples/s/p  0:01:01 }\n",
      "2023-11-17 10:39:37,354 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3644/ 3750], loss: 2.693, per_step_time: 554ms, lr: 2.451175e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:37,354 - mindformers[callback.py:317] - INFO -   97.2% |████████████████████████████████████████████████  | 14.42 samples/s/p  0:00:58 }\n",
      "2023-11-17 10:39:39,595 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3648/ 3750], loss: 3.030, per_step_time: 554ms, lr: 2.3988812e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:39,595 - mindformers[callback.py:317] - INFO -   97.3% |████████████████████████████████████████████████  | 14.43 samples/s/p  0:00:56 }\n",
      "2023-11-17 10:39:41,834 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3652/ 3750], loss: 2.241, per_step_time: 554ms, lr: 2.3465846e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:41,835 - mindformers[callback.py:317] - INFO -   97.4% |████████████████████████████████████████████████  | 14.43 samples/s/p  0:00:54 }\n",
      "2023-11-17 10:39:44,076 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3656/ 3750], loss: 2.411, per_step_time: 555ms, lr: 2.2942907e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:44,077 - mindformers[callback.py:317] - INFO -   97.5% |████████████████████████████████████████████████  | 14.41 samples/s/p  0:00:52 }\n",
      "2023-11-17 10:39:46,316 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3660/ 3750], loss: 2.905, per_step_time: 554ms, lr: 2.2419965e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:46,316 - mindformers[callback.py:317] - INFO -   97.6% |████████████████████████████████████████████████  | 14.43 samples/s/p  0:00:49 }\n",
      "2023-11-17 10:39:48,564 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3664/ 3750], loss: 2.608, per_step_time: 556ms, lr: 2.189703e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:48,564 - mindformers[callback.py:317] - INFO -   97.7% |████████████████████████████████████████████████  | 14.37 samples/s/p  0:00:47 }\n",
      "2023-11-17 10:39:50,814 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3668/ 3750], loss: 3.023, per_step_time: 557ms, lr: 2.1374092e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:50,815 - mindformers[callback.py:317] - INFO -   97.8% |████████████████████████████████████████████████  | 14.36 samples/s/p  0:00:45 }\n",
      "2023-11-17 10:39:53,061 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3672/ 3750], loss: 2.494, per_step_time: 556ms, lr: 2.0851123e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:53,062 - mindformers[callback.py:317] - INFO -   97.9% |████████████████████████████████████████████████  | 14.38 samples/s/p  0:00:43 }\n",
      "2023-11-17 10:39:55,306 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3676/ 3750], loss: 2.588, per_step_time: 555ms, lr: 2.0328184e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:55,307 - mindformers[callback.py:317] - INFO -   98.0% |█████████████████████████████████████████████████ | 14.40 samples/s/p  0:00:41 }\n",
      "INFO:root:Copy parallel total time cost: 42.88 seconds.\n",
      "2023-11-17 10:39:55,587 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 42.882304\n",
      "INFO:root:List OBS time cost: 0.06 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.09 seconds.\n",
      "2023-11-17 10:39:55,701 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.091972\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.04 seconds.\n",
      "2023-11-17 10:39:55,767 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.037862\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:39:55,815 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.022562\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "2023-11-17 10:39:57,544 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3680/ 3750], loss: 2.343, per_step_time: 554ms, lr: 1.9805248e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:57,544 - mindformers[callback.py:317] - INFO -   98.1% |█████████████████████████████████████████████████ | 14.44 samples/s/p  0:00:38 }\n",
      "2023-11-17 10:39:59,784 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3684/ 3750], loss: 3.003, per_step_time: 554ms, lr: 1.9282309e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:39:59,784 - mindformers[callback.py:317] - INFO -   98.2% |█████████████████████████████████████████████████ | 14.43 samples/s/p  0:00:36 }\n",
      "2023-11-17 10:40:02,040 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3688/ 3750], loss: 2.947, per_step_time: 554ms, lr: 1.875934e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:02,040 - mindformers[callback.py:317] - INFO -   98.3% |█████████████████████████████████████████████████ | 14.43 samples/s/p  0:00:34 }\n",
      "2023-11-17 10:40:04,279 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3692/ 3750], loss: 3.092, per_step_time: 554ms, lr: 1.8236402e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:04,279 - mindformers[callback.py:317] - INFO -   98.5% |█████████████████████████████████████████████████ | 14.44 samples/s/p  0:00:32 }\n",
      "2023-11-17 10:40:06,517 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3696/ 3750], loss: 2.576, per_step_time: 554ms, lr: 1.7713464e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:06,518 - mindformers[callback.py:317] - INFO -   98.6% |█████████████████████████████████████████████████ | 14.44 samples/s/p  0:00:29 }\n",
      "2023-11-17 10:40:08,756 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3700/ 3750], loss: 2.528, per_step_time: 554ms, lr: 1.7190525e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:08,757 - mindformers[callback.py:317] - INFO -   98.7% |█████████████████████████████████████████████████ | 14.43 samples/s/p  0:00:27 }\n",
      "2023-11-17 10:40:08,774 - mindformers[cloud_adapter.py:84] - INFO - Starting upload output file to obs!\n",
      "INFO:root:Copy parallel total time cost: 43.81 seconds.\n",
      "2023-11-17 10:40:39,650 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 43.808478\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 38042), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.11 seconds.\n",
      "2023-11-17 10:40:43,267 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3704/ 3750], loss: 3.479, per_step_time: 574ms, lr: 1.6667559e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:43,268 - mindformers[callback.py:317] - INFO -   98.8% |█████████████████████████████████████████████████ | 13.94 samples/s/p  0:00:26 }\n",
      "2023-11-17 10:40:45,508 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3708/ 3750], loss: 2.032, per_step_time: 554ms, lr: 1.614462e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:45,509 - mindformers[callback.py:317] - INFO -   98.9% |█████████████████████████████████████████████████ | 14.44 samples/s/p  0:00:23 }\n",
      "2023-11-17 10:40:47,748 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3712/ 3750], loss: 3.287, per_step_time: 554ms, lr: 1.5621682e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:47,748 - mindformers[callback.py:317] - INFO -   99.0% |█████████████████████████████████████████████████ | 14.43 samples/s/p  0:00:21 }\n",
      "2023-11-17 10:40:49,987 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3716/ 3750], loss: 2.268, per_step_time: 554ms, lr: 1.5098743e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:49,987 - mindformers[callback.py:317] - INFO -   99.1% |█████████████████████████████████████████████████ | 14.44 samples/s/p  0:00:18 }\n",
      "2023-11-17 10:40:52,226 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3720/ 3750], loss: 2.533, per_step_time: 554ms, lr: 1.4575776e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:52,226 - mindformers[callback.py:317] - INFO -   99.2% |█████████████████████████████████████████████████ | 14.44 samples/s/p  0:00:16 }\n",
      "2023-11-17 10:40:54,464 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3724/ 3750], loss: 2.727, per_step_time: 554ms, lr: 1.4052837e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:54,465 - mindformers[callback.py:317] - INFO -   99.3% |█████████████████████████████████████████████████ | 14.43 samples/s/p  0:00:14 }\n",
      "2023-11-17 10:40:56,703 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3728/ 3750], loss: 2.116, per_step_time: 554ms, lr: 1.3529898e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:56,704 - mindformers[callback.py:317] - INFO -   99.4% |█████████████████████████████████████████████████ | 14.43 samples/s/p  0:00:12 }\n",
      "2023-11-17 10:40:58,947 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3732/ 3750], loss: 2.819, per_step_time: 555ms, lr: 1.3006961e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:40:58,947 - mindformers[callback.py:317] - INFO -   99.5% |█████████████████████████████████████████████████ | 14.40 samples/s/p  0:00:09 }\n",
      "2023-11-17 10:41:01,190 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3736/ 3750], loss: 2.764, per_step_time: 555ms, lr: 1.2483994e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:41:01,191 - mindformers[callback.py:317] - INFO -   99.6% |█████████████████████████████████████████████████ | 14.40 samples/s/p  0:00:07 }\n",
      "2023-11-17 10:41:03,432 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3740/ 3750], loss: 2.438, per_step_time: 554ms, lr: 1.1961055e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:41:03,433 - mindformers[callback.py:317] - INFO -   99.7% |█████████████████████████████████████████████████ | 14.42 samples/s/p  0:00:05 }\n",
      "2023-11-17 10:41:05,675 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3744/ 3750], loss: 3.350, per_step_time: 555ms, lr: 1.1438116e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:41:05,675 - mindformers[callback.py:317] - INFO -   99.8% |█████████████████████████████████████████████████ | 14.41 samples/s/p  0:00:03 }\n",
      "2023-11-17 10:41:07,914 - mindformers[callback.py:307] - INFO - { Epoch:[  1/  1], step:[ 3748/ 3750], loss: 2.809, per_step_time: 554ms, lr: 1.0915179e-06, overflow cond: False, loss_scale: 16384.0\n",
      "2023-11-17 10:41:07,915 - mindformers[callback.py:317] - INFO -   99.9% |█████████████████████████████████████████████████ | 14.43 samples/s/p  0:00:01 }\n",
      "2023-11-17 10:41:07,940 - mindformers[callback.py:553] - INFO - ......Saving ckpt......\n",
      "[WARNING] GE_ADPT(17499,ffff9167b0b0,python):2023-11-17-10:41:09.759.427 [mindspore/ccsrc/transform/graph_ir/graph_runner.cc:128] RunGraph] Get graph form DfGraphManager failed!\n",
      "[WARNING] DEVICE(17499,ffff9167b0b0,python):2023-11-17-10:41:09.759.482 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ascend_deprecated_interface.cc:172] DoExecNonInputGraph] Exec graph:save.25767_25758_12920_1_mindspore_train_dataset_helper__DataWrapper_construct_5465 failed\n",
      "INFO:root:Copy parallel total time cost: 43.40 seconds.\n",
      "2023-11-17 10:41:24,418 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint_network/rank_0 success, cost time: 43.398456\n",
      "INFO:root:List OBS time cost: 0.07 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 0.11 seconds.\n",
      "2023-11-17 10:41:24,555 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/log success, cost time: 0.106450\n",
      "INFO:root:List OBS time cost: 0.03 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.05 seconds.\n",
      "2023-11-17 10:41:24,634 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/modelarts success, cost time: 0.053113\n",
      "INFO:root:No files to copy.\n",
      "2023-11-17 10:41:24,675 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/strategy success, cost time: 0.020931\n",
      "INFO:root:List OBS time cost: 0.02 seconds.\n",
      "INFO:root:Copy parallel total time cost: 0.18 seconds.\n",
      "2023-11-17 10:41:24,885 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output/checkpoint/rank_0 success, cost time: 0.181282\n",
      "2023-11-17 10:43:12,879 - mindformers[base_trainer.py:667] - INFO - .........Training Over!.............\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=122, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 52498), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.34 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 90.13 seconds.\n",
      "2023-11-17 10:44:43,039 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-output success, cost time: 90.130156\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=134, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39598), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=150, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57678), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=181, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39620), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=169, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57684), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=187, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 42014), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=179, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39618), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=156, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 42006), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=147, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 42004), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=174, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57688), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=186, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 54658), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=123, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57672), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=135, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39600), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=153, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46050), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=152, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39606), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=142, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 54874), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=184, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39622), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=157, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55202), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=144, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 54872), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=180, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 54656), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=185, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 42012), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=129, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 45260), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=161, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 45952), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=140, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 45264), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=183, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 54660), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=127, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 45954), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=171, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39614), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=167, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 42010), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=133, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 47402), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=154, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57680), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=145, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 52990), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=158, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 45262), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=124, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 52992), raddr=('100.125.7.3', 443)>\n"
     ]
    }
   ],
   "source": [
    "!MS_ENABLE_FORMAT_MODE=1 GE_USE_STATIC_MEMORY=2 HCCL_CONNECT_TIMEOUT=6000 MS_ASCEND_CHECK_OVERFLOW_MODE=\"INFNAN_MODE\" DEVICE_ID=0 RUN_STATUS=\"finetune\" python run_mindformer.py --config=\"configs/glm2/run_glm2_6b_lora_910b.yaml\" --use_parallel=False --run_mode=\"finetune\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655be91-e0d8-4475-aac7-af85484b212d",
   "metadata": {},
   "source": [
    "# 模型验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece3153-eabd-4ff1-a24e-bfcf69a0a9af",
   "metadata": {},
   "source": [
    "可以看到，微调后的所有模型已经保存在配置的obs桶中，其中，最后一个模型同时也可以在配置中的`output_dir`本地路径看到。我们可以对微调后的模型进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c33e1d88-3c9a-4a24-88b4-49920f367e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13G\n",
      "-r-------- 1 ma-user  13G Nov 17 10:42 glm2-6b-lora_rank_0-937_4.ckpt\n",
      "-rw------- 1 ma-user 6.4M Nov 17 09:47 glm2-6b-lora_rank_0-graph.meta\n"
     ]
    }
   ],
   "source": [
    "%ll /home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/output/checkpoint/rank_0/ -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58f6179a-7fe2-4290-baef-af77fa2a9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_model_path = \"/home/ma-user/work/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-937_4.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e841137-d560-4132-bfb2-c06c18d496cb",
   "metadata": {},
   "source": [
    "将tokenizer.model拷贝至默认文件夹下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af0e115e-64e0-44eb-b8b4-3a783018ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $chatglm2_6b_lora_path/mindformers/scripts/mf_standalone/checkpoint_download/glm2\n",
    "!cp $chatglm2_6b_lora_path/tokenizer.model $chatglm2_6b_lora_path/mindformers/scripts/mf_standalone/checkpoint_download/glm2/tokenizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda0cc3-00ef-4b0b-b091-0ae54cabc68c",
   "metadata": {},
   "source": [
    "修改eval配置文件，我们使用的配置文件路径为：`$chatglm2_6b_lora_path/mindformers/scripts/mf_standalone/configs/glm2/run_glm2_6b_lora_eval.yaml`\n",
    "\n",
    "主要需要修改的字段：\n",
    "\n",
    "```yaml\n",
    "load_checkpoint: '/home/ma-user/work/glm2_6b.ckpt'\n",
    "# 必须，配置为obs路径\n",
    "remote_save_url: \"obs://ai-l/competition/glm2/eval-output-test/\"\n",
    "\n",
    "eval_dataset: &eval_dataset\n",
    "  data_loader:\n",
    "    dataset_dir: \"/home/ma-user/work/chatglm2_6b_lora/train.jsonl\"  # 修改为实际数据路径\n",
    "    origin_columns: [\"input\", \"output\"]  # 根据实际数据使用的字段进行修改\n",
    "  tokenizer:\n",
    "    type: ChatGLM2Tokenizer\n",
    "    vocab_file: \"/home/ma-user/work/chatglm2_6b_lora/tokenizer.model\"  # 实际使用的tokenizer.model路径，我们在微调代码$chatglm2_6b_lora_path中预置了一个tokenizer.model，如需使用自定义tokenizer model也可配置相关参数\n",
    "    \n",
    "```\n",
    "eval阶段较为耗时，建议准备的验证数据为10条左右，若验证数据少于batch_size，请将batch_size改小。\n",
    "\n",
    "修改后，运行eval命令，**注意**修改ckpt为实际微调后checkpoint路径 `--load_checkpoint=$finetuning_model_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4de818f2-18bd-4975-8035-ba35b956e642",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \\L\n",
      "  if not dirpath.find(\"AppData\\Local\\Temp\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \\B\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(__version__) >= LooseVersion(\"1.17.0\"):\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n",
      "2023-11-17 10:49:07,055 - mindformers[run_mindformer.py:93] - INFO - .........Build context config..........\n",
      "2023-11-17 10:49:07,056 - mindformers[parallel_config.py:38] - INFO - initial moe_config from dict: {'expert_num': 1, 'capacity_factor': 1.05, 'aux_loss_factor': 0.05, 'num_experts_chosen': 1}\n",
      "2023-11-17 10:49:07,056 - mindformers[parallel_config.py:44] - INFO - initial recompute_config from dict: {'recompute': True, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}\n",
      "2023-11-17 10:49:07,056 - mindformers[parallel_config.py:50] - INFO - initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'expert_parallel': 1, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}\n",
      "2023-11-17 10:49:07,057 - mindformers[run_mindformer.py:100] - INFO - context config is: [ParallelConfig]\n",
      "_recompute:[ParallelConfig]\n",
      "_recompute:True\n",
      "_select_recompute:False\n",
      "_parallel_optimizer_comm_recompute:False\n",
      "_mp_comm_recompute:True\n",
      "_recompute_slice_activation:True\n",
      "\n",
      "select_recompute:False\n",
      "use_seq_parallel:False\n",
      "_gradient_aggregation_group:4\n",
      "_embed_dp_mp_config:[ParallelConfig]\n",
      "_dp_mp_config:[ParallelConfig]\n",
      "_data_parallel:8\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_vocab_emb_dp:True\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_pp_config:[ParallelConfig]\n",
      "_pipeline_stage:1\n",
      "_micro_batch_num:1\n",
      "\n",
      "_moe_config:[ParallelConfig]\n",
      "_dpmp:[ParallelConfig]\n",
      "_data_parallel:8\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_expert_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "\n",
      "2023-11-17 10:49:07,057 - mindformers[run_mindformer.py:101] - INFO - moe config is: <mindformers.modules.transformer.moe.MoEConfig object at 0xffffac8f4fd0>\n",
      "2023-11-17 10:49:07,057 - mindformers[run_mindformer.py:117] - INFO - remote_save_url is obs://lxy-guiyang1-output/glm2-2.2-eval-output, the output file will be uploaded to here.\n",
      "2023-11-17 10:49:07,058 - mindformers[base_trainer.py:78] - INFO - Now Running Task is: text_generation, Model is: glm2_6b_lora\n",
      "2023-11-17 10:49:07,058 - mindformers[base_trainer.py:215] - INFO - The current parallel mode is stand_alone, batch size per card will not be changed: batch_size_per_card = 8\n",
      "2023-11-17 10:49:07,058 - mindformers[base_trainer.py:218] - INFO - global_batch_size = batch_size_per_card * device_num = 8 * 1 = 8\n",
      "2023-11-17 10:49:07,059 - mindformers[base_trainer.py:228] - INFO - parallel_config will be change to default config: [ParallelConfig]\n",
      "_recompute:[ParallelConfig]\n",
      "_recompute:True\n",
      "_select_recompute:False\n",
      "_parallel_optimizer_comm_recompute:False\n",
      "_mp_comm_recompute:True\n",
      "_recompute_slice_activation:True\n",
      "\n",
      "select_recompute:False\n",
      "use_seq_parallel:False\n",
      "_gradient_aggregation_group:4\n",
      "_embed_dp_mp_config:[ParallelConfig]\n",
      "_dp_mp_config:[ParallelConfig]\n",
      "_data_parallel:1\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_vocab_emb_dp:True\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_pp_config:[ParallelConfig]\n",
      "_pipeline_stage:1\n",
      "_micro_batch_num:1\n",
      "\n",
      "_moe_config:[ParallelConfig]\n",
      "_dpmp:[ParallelConfig]\n",
      "_data_parallel:1\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_expert_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      ".\n",
      "2023-11-17 10:49:07,059 - mindformers[causal_language_modeling.py:172] - INFO - .........Build Dataset For Evaluate..........\n",
      "2023-11-17 10:49:07,059 - mindformers[base_trainer.py:293] - INFO - .........Build Dataset From Config..........\n",
      "2023-11-17 10:49:07,059 - mindformers[keyword_gen_dataset.py:63] - INFO - Now Create Keyword Generation Dataset.\n",
      "2023-11-17 10:49:07,109 - mindformers[adgen_dataloader.py:145] - INFO - Loading 15 data success.\n",
      "2023-11-17 10:49:07,110 - mindformers[adgen_dataloader.py:82] - INFO - [DATASET] shuffle status is False, phase is eval.\n",
      "2023-11-17 10:49:07,120 - mindformers[keyword_gen_dataset.py:103] - INFO - Start tokenize on the dataset using tokenizer: {'type': 'ChatGLM2Tokenizer', 'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}\n",
      "2023-11-17 10:49:07,122 - mindformers[base_trainer.py:307] - INFO - .........Build Network From Config..........\n",
      "[WARNING] DEVICE(113099,ffffb9d2d0b0,python):2023-11-17-10:49:10.403.376 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_runtime_manager.cc:46] GetAscendRuntime] No ascend runtime creator for AscendVM with device id 0\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "2023-11-17 10:50:53,443 - mindformers[base_model.py:121] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.969.802 [mindspore/train/serialization.py:1317] For 'load_param_into_net', 56 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.224 [mindspore/train/serialization.py:1322] transformer.encoder.layers.0.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.331 [mindspore/train/serialization.py:1322] transformer.encoder.layers.0.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.399 [mindspore/train/serialization.py:1322] transformer.encoder.layers.1.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.462 [mindspore/train/serialization.py:1322] transformer.encoder.layers.1.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.521 [mindspore/train/serialization.py:1322] transformer.encoder.layers.2.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.578 [mindspore/train/serialization.py:1322] transformer.encoder.layers.2.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.634 [mindspore/train/serialization.py:1322] transformer.encoder.layers.3.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.690 [mindspore/train/serialization.py:1322] transformer.encoder.layers.3.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.744 [mindspore/train/serialization.py:1322] transformer.encoder.layers.4.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.798 [mindspore/train/serialization.py:1322] transformer.encoder.layers.4.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.852 [mindspore/train/serialization.py:1322] transformer.encoder.layers.5.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.906 [mindspore/train/serialization.py:1322] transformer.encoder.layers.5.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.970.960 [mindspore/train/serialization.py:1322] transformer.encoder.layers.6.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.013 [mindspore/train/serialization.py:1322] transformer.encoder.layers.6.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.065 [mindspore/train/serialization.py:1322] transformer.encoder.layers.7.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.117 [mindspore/train/serialization.py:1322] transformer.encoder.layers.7.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.170 [mindspore/train/serialization.py:1322] transformer.encoder.layers.8.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.221 [mindspore/train/serialization.py:1322] transformer.encoder.layers.8.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.272 [mindspore/train/serialization.py:1322] transformer.encoder.layers.9.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.324 [mindspore/train/serialization.py:1322] transformer.encoder.layers.9.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.374 [mindspore/train/serialization.py:1322] transformer.encoder.layers.10.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.426 [mindspore/train/serialization.py:1322] transformer.encoder.layers.10.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.478 [mindspore/train/serialization.py:1322] transformer.encoder.layers.11.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.531 [mindspore/train/serialization.py:1322] transformer.encoder.layers.11.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.582 [mindspore/train/serialization.py:1322] transformer.encoder.layers.12.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.634 [mindspore/train/serialization.py:1322] transformer.encoder.layers.12.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.686 [mindspore/train/serialization.py:1322] transformer.encoder.layers.13.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.737 [mindspore/train/serialization.py:1322] transformer.encoder.layers.13.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.788 [mindspore/train/serialization.py:1322] transformer.encoder.layers.14.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.839 [mindspore/train/serialization.py:1322] transformer.encoder.layers.14.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.890 [mindspore/train/serialization.py:1322] transformer.encoder.layers.15.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.941 [mindspore/train/serialization.py:1322] transformer.encoder.layers.15.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.971.991 [mindspore/train/serialization.py:1322] transformer.encoder.layers.16.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.042 [mindspore/train/serialization.py:1322] transformer.encoder.layers.16.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.092 [mindspore/train/serialization.py:1322] transformer.encoder.layers.17.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.143 [mindspore/train/serialization.py:1322] transformer.encoder.layers.17.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.194 [mindspore/train/serialization.py:1322] transformer.encoder.layers.18.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.245 [mindspore/train/serialization.py:1322] transformer.encoder.layers.18.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.295 [mindspore/train/serialization.py:1322] transformer.encoder.layers.19.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.346 [mindspore/train/serialization.py:1322] transformer.encoder.layers.19.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.397 [mindspore/train/serialization.py:1322] transformer.encoder.layers.20.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.447 [mindspore/train/serialization.py:1322] transformer.encoder.layers.20.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.499 [mindspore/train/serialization.py:1322] transformer.encoder.layers.21.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.549 [mindspore/train/serialization.py:1322] transformer.encoder.layers.21.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.600 [mindspore/train/serialization.py:1322] transformer.encoder.layers.22.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.651 [mindspore/train/serialization.py:1322] transformer.encoder.layers.22.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.701 [mindspore/train/serialization.py:1322] transformer.encoder.layers.23.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.753 [mindspore/train/serialization.py:1322] transformer.encoder.layers.23.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.803 [mindspore/train/serialization.py:1322] transformer.encoder.layers.24.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.853 [mindspore/train/serialization.py:1322] transformer.encoder.layers.24.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.903 [mindspore/train/serialization.py:1322] transformer.encoder.layers.25.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.972.954 [mindspore/train/serialization.py:1322] transformer.encoder.layers.25.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.973.003 [mindspore/train/serialization.py:1322] transformer.encoder.layers.26.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.973.053 [mindspore/train/serialization.py:1322] transformer.encoder.layers.26.value_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.973.104 [mindspore/train/serialization.py:1322] transformer.encoder.layers.27.key_past is not loaded.\n",
      "[WARNING] ME(113099:281473799344304,MainProcess):2023-11-17-10:52:18.973.155 [mindspore/train/serialization.py:1322] transformer.encoder.layers.27.value_past is not loaded.\n",
      "2023-11-17 10:52:18,973 - mindformers[base_model.py:116] - INFO - weights in /cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-937_4.ckpt are loaded\n",
      "[INFO] 2023-11-17 10:52:18,974 [113099] [SDK] : Start to freeze model for delta, mode: lora, include list: None, exclude list: None\n",
      "[INFO] 2023-11-17 10:52:18,975 [113099] [SDK] : Start to freeze model, include list: ['*'], exclude list: ['*mindpet_delta_lora*']\n",
      "[INFO] 2023-11-17 10:52:18,981 [113099] [SDK] : End to freeze model.\n",
      "[INFO] 2023-11-17 10:52:18,981 [113099] [SDK] : End to freeze model for delta.\n",
      "2023-11-17 10:52:18,992 - mindformers[base_trainer.py:464] - INFO - Network Parameters: 1 M.\n",
      "2023-11-17 10:52:18,992 - mindformers[causal_language_modeling.py:186] - INFO - .........Build Compute Metrics For Evaluate..........\n",
      "2023-11-17 10:52:18,992 - mindformers[causal_language_modeling.py:192] - INFO - .........Build tokenizer For Evaluate..........\n",
      "2023-11-17 10:52:19,053 - mindformers[causal_language_modeling.py:196] - INFO - .........Starting Init Evaluate Model..........\n",
      "2023-11-17 10:52:19,053 - mindformers[causal_language_modeling.py:206] - INFO - .........Starting Evaluate Model..........\n",
      "{'auto_trans_ckpt': False,\n",
      " 'auto_tune': False,\n",
      " 'autotune_per_step': 10,\n",
      " 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),\n",
      "               OrderedDict([('type', 'CheckpointMointor'),\n",
      "                            ('prefix', 'glm2-6b-lora'),\n",
      "                            ('save_checkpoint_steps', 1000),\n",
      "                            ('keep_checkpoint_max', 1),\n",
      "                            ('integrated_save', False),\n",
      "                            ('async_save', False)]),\n",
      "               OrderedDict([('type', 'ObsMonitor'), ('keep_last', False)])],\n",
      " 'context': {'device_id': 0,\n",
      "             'device_target': 'Ascend',\n",
      "             'enable_graph_kernel': False,\n",
      "             'graph_kernel_flags': '--disable_expand_ops=Softmax,Dropout '\n",
      "                                   '--enable_parallel_fusion=true '\n",
      "                                   '--reduce_fuse_depth=8 '\n",
      "                                   '--enable_auto_tensor_inplace=true',\n",
      "             'max_call_depth': 10000,\n",
      "             'save_graphs': False},\n",
      " 'device_num': 1,\n",
      " 'do_eval': False,\n",
      " 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor'),\n",
      "                                 ('keep_last', False)])],\n",
      " 'eval_dataset': {'auto_tune': False,\n",
      "                  'autotune_per_step': 10,\n",
      "                  'batch_size': 8,\n",
      "                  'data_loader': {'origin_columns': ['questions', 'answers'],\n",
      "                                  'phase': 'eval',\n",
      "                                  'shuffle': False,\n",
      "                                  'type': 'ADGenDataLoader',\n",
      "                                  'version': 2},\n",
      "                  'device_num': 1,\n",
      "                  'do_eval': True,\n",
      "                  'drop_remainder': True,\n",
      "                  'filepath_prefix': './autotune',\n",
      "                  'ignore_pad_token_for_loss': True,\n",
      "                  'input_columns': ['input_ids', 'labels'],\n",
      "                  'max_source_length': 256,\n",
      "                  'max_target_length': 256,\n",
      "                  'num_parallel_workers': 8,\n",
      "                  'numa_enable': False,\n",
      "                  'prefetch_size': 1,\n",
      "                  'profile': False,\n",
      "                  'python_multiprocessing': False,\n",
      "                  'rank_id': 0,\n",
      "                  'repeat': 1,\n",
      "                  'seed': 0,\n",
      "                  'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}},\n",
      " 'eval_dataset_task': {'dataset_config': {'auto_tune': False,\n",
      "                                          'autotune_per_step': 10,\n",
      "                                          'batch_size': 8,\n",
      "                                          'data_loader': {'origin_columns': ['questions',\n",
      "                                                                             'answers'],\n",
      "                                                          'phase': 'eval',\n",
      "                                                          'shuffle': False,\n",
      "                                                          'type': 'ADGenDataLoader',\n",
      "                                                          'version': 2},\n",
      "                                          'device_num': 1,\n",
      "                                          'do_eval': True,\n",
      "                                          'drop_remainder': True,\n",
      "                                          'filepath_prefix': './autotune',\n",
      "                                          'ignore_pad_token_for_loss': True,\n",
      "                                          'input_columns': ['input_ids',\n",
      "                                                            'labels'],\n",
      "                                          'max_source_length': 256,\n",
      "                                          'max_target_length': 256,\n",
      "                                          'num_parallel_workers': 8,\n",
      "                                          'numa_enable': False,\n",
      "                                          'prefetch_size': 1,\n",
      "                                          'profile': False,\n",
      "                                          'python_multiprocessing': False,\n",
      "                                          'rank_id': 0,\n",
      "                                          'repeat': 1,\n",
      "                                          'seed': 0,\n",
      "                                          'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                                        'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}},\n",
      "                       'type': 'KeyWordGenDataset'},\n",
      " 'eval_epoch_interval': -1,\n",
      " 'eval_step_interval': 500,\n",
      " 'filepath_prefix': './autotune',\n",
      " 'init_start_profile': True,\n",
      " 'layer_decay': 0.65,\n",
      " 'layer_scale': False,\n",
      " 'load_checkpoint': None,\n",
      " 'local_rank': 0,\n",
      " 'lr_scale': False,\n",
      " 'lr_scale_factor': 256,\n",
      " 'lr_schedule': {'learning_rate': 5e-05,\n",
      "                 'lr_end': 1e-06,\n",
      "                 'total_steps': -1,\n",
      "                 'type': 'polynomial',\n",
      "                 'warmup_steps': 0},\n",
      " 'metric': {'type': 'ADGENMetric'},\n",
      " 'micro_batch_interleave_num': 1,\n",
      " 'model': {'arch': {'type': 'ChatGLM2ForConditionalGeneration'},\n",
      "           'model_config': {'add_bias_linear': False,\n",
      "                            'add_qkv_bias': True,\n",
      "                            'apply_query_key_layer_scaling': True,\n",
      "                            'apply_residual_connection_post_layernorm': False,\n",
      "                            'attention_dropout': 0.0,\n",
      "                            'attention_softmax_in_fp32': True,\n",
      "                            'batch_size': 8,\n",
      "                            'bias_dropout_fusion': True,\n",
      "                            'checkpoint_name_or_path': None,\n",
      "                            'compute_dtype': 'float16',\n",
      "                            'do_sample': True,\n",
      "                            'eos_token_id': 2,\n",
      "                            'ffn_hidden_size': 13696,\n",
      "                            'fp32_residual_connection': False,\n",
      "                            'hidden_dropout': 0.0,\n",
      "                            'hidden_size': 4096,\n",
      "                            'kv_channels': 128,\n",
      "                            'layernorm_compute_type': 'float32',\n",
      "                            'layernorm_epsilon': '1e-5',\n",
      "                            'max_decode_length': 256,\n",
      "                            'multi_query_attention': True,\n",
      "                            'multi_query_group_num': 2,\n",
      "                            'num_attention_heads': 32,\n",
      "                            'num_layers': 28,\n",
      "                            'pad_token_id': 0,\n",
      "                            'padded_vocab_size': 65024,\n",
      "                            'param_init_type': 'float16',\n",
      "                            'pet_config': {'lora_alpha': 32,\n",
      "                                           'lora_dropout': 0.1,\n",
      "                                           'lora_rank': 8,\n",
      "                                           'pet_type': 'lora',\n",
      "                                           'target_modules': '.*query_key_value*'},\n",
      "                            'post_layer_norm': True,\n",
      "                            'pre_seq_len': 'None',\n",
      "                            'prefix_projection': False,\n",
      "                            'quantization_bit': 0,\n",
      "                            'repetition_penalty': 1.0,\n",
      "                            'rmsnorm': True,\n",
      "                            'seq_length': 256,\n",
      "                            'top_k': 1,\n",
      "                            'top_p': 1,\n",
      "                            'type': 'ChatGLM2Config',\n",
      "                            'use_past': True}},\n",
      " 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0xffffac8f4fd0>,\n",
      " 'only_save_strategy': False,\n",
      " 'optimizer': {'beta1': 0.9,\n",
      "               'beta2': 0.95,\n",
      "               'eps': 1e-08,\n",
      "               'type': 'FP32StateAdamWeightDecay',\n",
      "               'weight_decay': 0.1},\n",
      " 'output_dir': './output',\n",
      " 'parallel': {'enable_alltoall': False,\n",
      "              'enable_parallel_optimizer': True,\n",
      "              'full_batch': True,\n",
      "              'gradients_mean': False,\n",
      "              'loss_repeated_mean': True,\n",
      "              'parallel_mode': 1,\n",
      "              'search_mode': 'sharding_propagation',\n",
      "              'strategy_ckpt_config': {'only_trainable_params': False,\n",
      "                                       'save_file': './ckpt_strategy.ckpt'},\n",
      "              'strategy_ckpt_save_file': '/cache/ma-user-work/strategy/ckpt_strategy_rank_0.ckpt'},\n",
      " 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0xfffec85ae790>,\n",
      " 'processor': {'return_tensors': 'ms',\n",
      "               'tokenizer': {'bos_token': '<sop>',\n",
      "                             'end_token': '</s>',\n",
      "                             'eos_token': '<eop>',\n",
      "                             'gmask_token': '[gMASK]',\n",
      "                             'mask_token': '[MASK]',\n",
      "                             'pad_token': '<pad>',\n",
      "                             'type': 'ChatGLM2Tokenizer',\n",
      "                             'unk_token': '<unk>',\n",
      "                             'vocab_file': './checkpoint_download/glm2/tokenizer.model'},\n",
      "               'type': 'GLMProcessor'},\n",
      " 'profile': False,\n",
      " 'profile_communication': True,\n",
      " 'profile_memory': True,\n",
      " 'profile_start_step': 1,\n",
      " 'profile_stop_step': 10,\n",
      " 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0xffffac8f4fa0>,\n",
      " 'remote_save_url': 'obs://lxy-guiyang1-output/glm2-2.2-eval-output',\n",
      " 'resume_training': False,\n",
      " 'run_mode': 'eval',\n",
      " 'runner_config': {'batch_size': 8,\n",
      "                   'epochs': 1,\n",
      "                   'sink_mode': True,\n",
      "                   'sink_size': 4},\n",
      " 'runner_wrapper': {'scale_sense': {'loss_scale_value': 65536,\n",
      "                                    'scale_factor': 2,\n",
      "                                    'scale_window': 1000,\n",
      "                                    'type': 'DynamicLossScaleUpdateCell'},\n",
      "                    'type': 'MFTrainOneStepCell',\n",
      "                    'use_clip_grad': True},\n",
      " 'seed': 0,\n",
      " 'train_dataset': {'auto_tune': False,\n",
      "                   'autotune_per_step': 10,\n",
      "                   'batch_size': 8,\n",
      "                   'data_loader': {'dataset_dir': '/path/to/AdvertiseGen/train.json',\n",
      "                                   'origin_columns': ['content', 'summary'],\n",
      "                                   'phase': 'train',\n",
      "                                   'shuffle': True,\n",
      "                                   'type': 'ADGenDataLoader',\n",
      "                                   'version': 2},\n",
      "                   'do_eval': False,\n",
      "                   'drop_remainder': True,\n",
      "                   'filepath_prefix': './autotune',\n",
      "                   'ignore_pad_token_for_loss': True,\n",
      "                   'input_columns': ['input_ids', 'labels'],\n",
      "                   'max_source_length': 64,\n",
      "                   'max_target_length': 128,\n",
      "                   'num_parallel_workers': 8,\n",
      "                   'numa_enable': False,\n",
      "                   'prefetch_size': 1,\n",
      "                   'profile': False,\n",
      "                   'python_multiprocessing': False,\n",
      "                   'repeat': 1,\n",
      "                   'seed': 0,\n",
      "                   'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                 'vocab_file': '/path/to/tokenizer.model'}},\n",
      " 'train_dataset_task': {'dataset_config': {'auto_tune': False,\n",
      "                                           'autotune_per_step': 10,\n",
      "                                           'batch_size': 8,\n",
      "                                           'data_loader': {'dataset_dir': '/path/to/AdvertiseGen/train.json',\n",
      "                                                           'origin_columns': ['content',\n",
      "                                                                              'summary'],\n",
      "                                                           'phase': 'train',\n",
      "                                                           'shuffle': True,\n",
      "                                                           'type': 'ADGenDataLoader',\n",
      "                                                           'version': 2},\n",
      "                                           'do_eval': False,\n",
      "                                           'drop_remainder': True,\n",
      "                                           'filepath_prefix': './autotune',\n",
      "                                           'ignore_pad_token_for_loss': True,\n",
      "                                           'input_columns': ['input_ids',\n",
      "                                                             'labels'],\n",
      "                                           'max_source_length': 64,\n",
      "                                           'max_target_length': 128,\n",
      "                                           'num_parallel_workers': 8,\n",
      "                                           'numa_enable': False,\n",
      "                                           'prefetch_size': 1,\n",
      "                                           'profile': False,\n",
      "                                           'python_multiprocessing': False,\n",
      "                                           'repeat': 1,\n",
      "                                           'seed': 0,\n",
      "                                           'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                                         'vocab_file': '/path/to/tokenizer.model'}},\n",
      "                        'type': 'KeyWordGenDataset'},\n",
      " 'trainer': {'model_name': 'glm2_6b_lora',\n",
      "             'type': 'CausalLanguageModelingTrainer'},\n",
      " 'use_parallel': False}\n",
      "2023-11-17 10:52:19,075 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 10:52:19,075 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 10:52:19,075 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 10:52:19,076 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 10:52:19,076 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "[WARNING] DEVICE(113099,ffffb9d2d0b0,python):2023-11-17-10:53:29.865.537 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_graph_executor.cc:1258] RunInitGraph] Can not find init_subgraph.kernel_graph_0 sub graph, don't need data init subgraph in INFER mode.\n",
      "[WARNING] DEVICE(113099,ffffb9d2d0b0,python):2023-11-17-10:55:46.958.419 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_graph_executor.cc:1258] RunInitGraph] Can not find init_subgraph.kernel_graph_1 sub graph, don't need data init subgraph in INFER mode.\n",
      "2023-11-17 10:56:53,784 - mindformers[text_generator.py:434] - INFO - total time: 274.6883656978607 s; generated tokens: 542 tokens; generate speed: 1.973145089793008 tokens/s\n",
      "2023-11-17 10:56:53,788 - mindformers[causal_language_modeling.py:253] - INFO - Step[1/2], cost time 274.6958s, every example cost time is 34.3370, generate speed: 1.9731 tokens/s, avg speed: 0.0000 tokens/s, remaining time: 0:00:00\n",
      "pred is:\n",
      " 牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；牵引治疗；\n",
      " label is:\n",
      " 颈椎定点伸引手法；补阳还五汤；后路减压植入物内固定治疗；加味补阳还五汤；围手术期护理；银质针导热疗法\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.421 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.421 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n",
      "pred is:\n",
      " 利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；利尿剂；血管扩张剂；\n",
      " label is:\n",
      " 左西孟旦；新活素\n",
      "pred is:\n",
      " 针灸；针灸治疗\n",
      " label is:\n",
      " 脾胃调理\n",
      "pred is:\n",
      " 腹部超声\n",
      " label is:\n",
      " 超声造影检查\n",
      "pred is:\n",
      " 淋巴结；肺；肝；肾；脑\n",
      " label is:\n",
      " 肝\n",
      "pred is:\n",
      " 精囊炎\n",
      " label is:\n",
      " 血精；男性不育症\n",
      "pred is:\n",
      " 年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄；年龄\n",
      " label is:\n",
      " 侵入性操作；糖尿病；机械通气\n",
      "pred is:\n",
      " 脑电图；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；脑电图检查；脑脊液检查；\n",
      " label is:\n",
      " 脑脊液氨基酸；颅脑CT检查；血常规；血液电解质检查；便常规；颅脑MRI检查\n",
      "2023-11-17 10:57:04,720 - mindformers[text_generator.py:434] - INFO - total time: 9.470386028289795 s; generated tokens: 213 tokens; generate speed: 22.491163439772105 tokens/s\n",
      "2023-11-17 10:57:04,724 - mindformers[causal_language_modeling.py:253] - INFO - Step[2/2], cost time 9.4778s, every example cost time is 1.1847, generate speed: 22.4735 tokens/s, avg speed: 22.4733 tokens/s, remaining time: 0:00:00\n",
      "pred is:\n",
      " 角化囊肿；角化囊肿；角化囊肿性皮肤病；角化囊肿性皮肤病；角化囊肿\n",
      " label is:\n",
      " 畏光；视力障碍；唇皱褶增多畸形；结膜充血\n",
      "pred is:\n",
      " 肝细胞；肝细胞核\n",
      " label is:\n",
      " 口腔黏膜；OSCC组织\n",
      "pred is:\n",
      " 1.2%\n",
      " label is:\n",
      " 41.97％和18.14％；22.8％和7.1％\n",
      "pred is:\n",
      " 多排螺旋CT\n",
      " label is:\n",
      " CT冠状动脉图像\n",
      "pred is:\n",
      " 认知功能测试；认知功能问卷；认知功能测试量表；认知功能问卷量表；认知功能测试；认知功能问卷量表\n",
      " label is:\n",
      " 听觉词语学习测验\n",
      "pred is:\n",
      " 老年人\n",
      " label is:\n",
      " 儿童\n",
      "pred is:\n",
      " 阿奇霉素；头孢曲松钠；头孢曲松；头孢曲松；头孢曲松；头孢曲松钠；头孢曲松；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；头孢曲松钠片；\n",
      " label is:\n",
      " 特利加压素；垂体后叶素\n",
      "pred is:\n",
      " 腹腔镜治疗\n",
      " label is:\n",
      " 保守性治疗\n",
      "metric: ADGENMetric\n",
      "rouge-1: 18.3107\n",
      "rouge-2: 0.9992\n",
      "rouge-l: 13.1502\n",
      "bleu-4:  4.4355\n",
      "2023-11-17 10:57:04,757 - mindformers[causal_language_modeling.py:267] - INFO - ...........Evaluate Over!...............\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=121, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 52196), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.58 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 85.29 seconds.\n",
      "2023-11-17 10:58:33,773 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-eval-output success, cost time: 85.287628\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/run_mindformer.py\", line 340, in <module>\n",
      "    main(config_)\n",
      "  File \"/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/mindformers/tools/cloud_adapter/cloud_monitor.py\", line 45, in wrapper\n",
      "    _last_transform(local_id, log)\n",
      "  File \"/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/mindformers/tools/cloud_adapter/cloud_monitor.py\", line 72, in _last_transform\n",
      "    os.mknod(LAST_TRANSFORM_LOCK_PATH)\n",
      "FileExistsError: [Errno 17] File exists\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=159, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 44752), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=177, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 53146), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=173, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55982), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=169, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 53136), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=160, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 53128), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=184, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55994), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=179, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55990), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=151, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35140), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=150, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 33998), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=161, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55978), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=171, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 53138), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=133, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 53790), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=181, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35144), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=143, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 58990), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=131, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 45456), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=153, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55976), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=178, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46126), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=158, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35142), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=155, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46128), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=162, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 53132), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=166, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46122), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=139, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 45454), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=152, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 58404), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=122, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 53124), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=165, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34000), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=145, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 53792), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=138, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 58988), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=164, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46124), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=129, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 58406), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=148, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 58402), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=123, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 40760), raddr=('100.125.7.3', 443)>\n"
     ]
    }
   ],
   "source": [
    "!python run_mindformer.py --config=configs/glm2/run_glm2_6b_lora_eval.yaml --use_parallel=False --run_mode=\"eval\" --device_id=0 --load_checkpoint=$finetuning_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc1a6c-2c42-4070-bc26-37f0453ca5b2",
   "metadata": {},
   "source": [
    "eval会打印验证数据的推理结果，如需对比微调前后效果，可以对预训练模型也进行验证，**此步骤不是必须的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c6aa580-5989-47c2-bc61-a6dd4381e5f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \\L\n",
      "  if not dirpath.find(\"AppData\\Local\\Temp\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \\B\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(__version__) >= LooseVersion(\"1.17.0\"):\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/data/protos/string_int_label_map_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n",
      "2023-11-17 11:00:02,765 - mindformers[run_mindformer.py:93] - INFO - .........Build context config..........\n",
      "2023-11-17 11:00:02,765 - mindformers[parallel_config.py:38] - INFO - initial moe_config from dict: {'expert_num': 1, 'capacity_factor': 1.05, 'aux_loss_factor': 0.05, 'num_experts_chosen': 1}\n",
      "2023-11-17 11:00:02,766 - mindformers[parallel_config.py:44] - INFO - initial recompute_config from dict: {'recompute': True, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}\n",
      "2023-11-17 11:00:02,766 - mindformers[parallel_config.py:50] - INFO - initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'expert_parallel': 1, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}\n",
      "2023-11-17 11:00:02,766 - mindformers[run_mindformer.py:100] - INFO - context config is: [ParallelConfig]\n",
      "_recompute:[ParallelConfig]\n",
      "_recompute:True\n",
      "_select_recompute:False\n",
      "_parallel_optimizer_comm_recompute:False\n",
      "_mp_comm_recompute:True\n",
      "_recompute_slice_activation:True\n",
      "\n",
      "select_recompute:False\n",
      "use_seq_parallel:False\n",
      "_gradient_aggregation_group:4\n",
      "_embed_dp_mp_config:[ParallelConfig]\n",
      "_dp_mp_config:[ParallelConfig]\n",
      "_data_parallel:8\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_vocab_emb_dp:True\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_pp_config:[ParallelConfig]\n",
      "_pipeline_stage:1\n",
      "_micro_batch_num:1\n",
      "\n",
      "_moe_config:[ParallelConfig]\n",
      "_dpmp:[ParallelConfig]\n",
      "_data_parallel:8\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_expert_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "\n",
      "2023-11-17 11:00:02,767 - mindformers[run_mindformer.py:101] - INFO - moe config is: <mindformers.modules.transformer.moe.MoEConfig object at 0xffff9c359520>\n",
      "2023-11-17 11:00:02,767 - mindformers[run_mindformer.py:117] - INFO - remote_save_url is obs://lxy-guiyang1-output/glm2-2.2-eval-output, the output file will be uploaded to here.\n",
      "2023-11-17 11:00:02,767 - mindformers[base_trainer.py:78] - INFO - Now Running Task is: text_generation, Model is: glm2_6b_lora\n",
      "2023-11-17 11:00:02,768 - mindformers[base_trainer.py:215] - INFO - The current parallel mode is stand_alone, batch size per card will not be changed: batch_size_per_card = 8\n",
      "2023-11-17 11:00:02,768 - mindformers[base_trainer.py:218] - INFO - global_batch_size = batch_size_per_card * device_num = 8 * 1 = 8\n",
      "2023-11-17 11:00:02,768 - mindformers[base_trainer.py:228] - INFO - parallel_config will be change to default config: [ParallelConfig]\n",
      "_recompute:[ParallelConfig]\n",
      "_recompute:True\n",
      "_select_recompute:False\n",
      "_parallel_optimizer_comm_recompute:False\n",
      "_mp_comm_recompute:True\n",
      "_recompute_slice_activation:True\n",
      "\n",
      "select_recompute:False\n",
      "use_seq_parallel:False\n",
      "_gradient_aggregation_group:4\n",
      "_embed_dp_mp_config:[ParallelConfig]\n",
      "_dp_mp_config:[ParallelConfig]\n",
      "_data_parallel:1\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_vocab_emb_dp:True\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_pp_config:[ParallelConfig]\n",
      "_pipeline_stage:1\n",
      "_micro_batch_num:1\n",
      "\n",
      "_moe_config:[ParallelConfig]\n",
      "_dpmp:[ParallelConfig]\n",
      "_data_parallel:1\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_expert_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      ".\n",
      "2023-11-17 11:00:02,769 - mindformers[causal_language_modeling.py:172] - INFO - .........Build Dataset For Evaluate..........\n",
      "2023-11-17 11:00:02,769 - mindformers[base_trainer.py:293] - INFO - .........Build Dataset From Config..........\n",
      "2023-11-17 11:00:02,769 - mindformers[keyword_gen_dataset.py:63] - INFO - Now Create Keyword Generation Dataset.\n",
      "2023-11-17 11:00:02,822 - mindformers[adgen_dataloader.py:145] - INFO - Loading 15 data success.\n",
      "2023-11-17 11:00:02,822 - mindformers[adgen_dataloader.py:82] - INFO - [DATASET] shuffle status is False, phase is eval.\n",
      "2023-11-17 11:00:02,832 - mindformers[keyword_gen_dataset.py:103] - INFO - Start tokenize on the dataset using tokenizer: {'type': 'ChatGLM2Tokenizer', 'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}\n",
      "2023-11-17 11:00:02,835 - mindformers[base_trainer.py:307] - INFO - .........Build Network From Config..........\n",
      "[WARNING] DEVICE(131255,ffffa969d0b0,python):2023-11-17-11:00:06.166.227 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_runtime_manager.cc:46] GetAscendRuntime] No ascend runtime creator for AscendVM with device id 0\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "2023-11-17 11:01:47,506 - mindformers[base_model.py:121] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:00.114.387 [mindspore/train/serialization.py:172] The type of transformer.embedding.embedding_table:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:02.695.190 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.0.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:02.779.593 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.0.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:03.159.473 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.1.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:03.242.235 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.1.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:03.645.515 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.2.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:03.727.902 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.2.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:04.111.377 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.3.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:04.194.151 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.3.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:04.829.422 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.4.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:04.912.604 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.4.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:05.299.752 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.5.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:05.388.564 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.5.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:05.771.694 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.6.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:05.859.776 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.6.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:06.239.532 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.7.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:06.326.810 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.7.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:07.761.26 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.8.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:07.158.823 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.8.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:07.536.993 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.9.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:07.672.852 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.9.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:08.924.55 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.10.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:08.176.002 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.10.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:08.569.600 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.11.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:08.653.420 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.11.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:09.687.02 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.12.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:09.158.184 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.12.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:09.780.342 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.13.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:09.868.702 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.13.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:10.605.291 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.14.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:10.688.531 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.14.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:11.734.02 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.15.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:11.155.961 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.15.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:11.917.546 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.16.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:12.985. [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.16.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:12.423.059 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.17.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:12.509.579 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.17.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:12.896.862 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.18.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:12.979.859 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.18.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:13.366.141 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.19.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:13.455.795 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.19.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:13.839.619 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.20.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:13.932.833 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.20.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:14.678.656 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.21.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:14.761.778 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.21.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:15.139.696 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.22.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:15.222.883 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.22.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:15.615.335 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.23.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:15.698.735 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.23.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:16.265.126 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.24.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:16.348.269 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.24.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:16.902.540 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.25.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:16.997.939 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.25.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:17.742.559 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.26.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:17.825.675 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.26.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:18.213.449 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.27.input_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:18.296.864 [mindspore/train/serialization.py:172] The type of transformer.encoder.layers.27.post_attention_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:18.685.000 [mindspore/train/serialization.py:172] The type of transformer.encoder.final_layernorm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.299.496 [mindspore/train/serialization.py:1317] For 'load_param_into_net', 112 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.299.683 [mindspore/train/serialization.py:1322] transformer.encoder.layers.0.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.299.756 [mindspore/train/serialization.py:1322] transformer.encoder.layers.0.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.299.817 [mindspore/train/serialization.py:1322] transformer.encoder.layers.0.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.299.878 [mindspore/train/serialization.py:1322] transformer.encoder.layers.0.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.299.936 [mindspore/train/serialization.py:1322] transformer.encoder.layers.1.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.299.992 [mindspore/train/serialization.py:1322] transformer.encoder.layers.1.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.046 [mindspore/train/serialization.py:1322] transformer.encoder.layers.1.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.100 [mindspore/train/serialization.py:1322] transformer.encoder.layers.1.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.153 [mindspore/train/serialization.py:1322] transformer.encoder.layers.2.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.205 [mindspore/train/serialization.py:1322] transformer.encoder.layers.2.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.259 [mindspore/train/serialization.py:1322] transformer.encoder.layers.2.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.311 [mindspore/train/serialization.py:1322] transformer.encoder.layers.2.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.362 [mindspore/train/serialization.py:1322] transformer.encoder.layers.3.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.412 [mindspore/train/serialization.py:1322] transformer.encoder.layers.3.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.462 [mindspore/train/serialization.py:1322] transformer.encoder.layers.3.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.512 [mindspore/train/serialization.py:1322] transformer.encoder.layers.3.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.562 [mindspore/train/serialization.py:1322] transformer.encoder.layers.4.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.612 [mindspore/train/serialization.py:1322] transformer.encoder.layers.4.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.663 [mindspore/train/serialization.py:1322] transformer.encoder.layers.4.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.712 [mindspore/train/serialization.py:1322] transformer.encoder.layers.4.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.761 [mindspore/train/serialization.py:1322] transformer.encoder.layers.5.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.811 [mindspore/train/serialization.py:1322] transformer.encoder.layers.5.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.861 [mindspore/train/serialization.py:1322] transformer.encoder.layers.5.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.911 [mindspore/train/serialization.py:1322] transformer.encoder.layers.5.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.300.961 [mindspore/train/serialization.py:1322] transformer.encoder.layers.6.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.011 [mindspore/train/serialization.py:1322] transformer.encoder.layers.6.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.060 [mindspore/train/serialization.py:1322] transformer.encoder.layers.6.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.110 [mindspore/train/serialization.py:1322] transformer.encoder.layers.6.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.160 [mindspore/train/serialization.py:1322] transformer.encoder.layers.7.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.210 [mindspore/train/serialization.py:1322] transformer.encoder.layers.7.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.259 [mindspore/train/serialization.py:1322] transformer.encoder.layers.7.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.309 [mindspore/train/serialization.py:1322] transformer.encoder.layers.7.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.359 [mindspore/train/serialization.py:1322] transformer.encoder.layers.8.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.408 [mindspore/train/serialization.py:1322] transformer.encoder.layers.8.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.457 [mindspore/train/serialization.py:1322] transformer.encoder.layers.8.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.507 [mindspore/train/serialization.py:1322] transformer.encoder.layers.8.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.556 [mindspore/train/serialization.py:1322] transformer.encoder.layers.9.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.604 [mindspore/train/serialization.py:1322] transformer.encoder.layers.9.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.653 [mindspore/train/serialization.py:1322] transformer.encoder.layers.9.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.702 [mindspore/train/serialization.py:1322] transformer.encoder.layers.9.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.751 [mindspore/train/serialization.py:1322] transformer.encoder.layers.10.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.801 [mindspore/train/serialization.py:1322] transformer.encoder.layers.10.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.849 [mindspore/train/serialization.py:1322] transformer.encoder.layers.10.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.898 [mindspore/train/serialization.py:1322] transformer.encoder.layers.10.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.949 [mindspore/train/serialization.py:1322] transformer.encoder.layers.11.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.301.998 [mindspore/train/serialization.py:1322] transformer.encoder.layers.11.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.047 [mindspore/train/serialization.py:1322] transformer.encoder.layers.11.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.098 [mindspore/train/serialization.py:1322] transformer.encoder.layers.11.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.147 [mindspore/train/serialization.py:1322] transformer.encoder.layers.12.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.197 [mindspore/train/serialization.py:1322] transformer.encoder.layers.12.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.262 [mindspore/train/serialization.py:1322] transformer.encoder.layers.12.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.315 [mindspore/train/serialization.py:1322] transformer.encoder.layers.12.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.366 [mindspore/train/serialization.py:1322] transformer.encoder.layers.13.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.415 [mindspore/train/serialization.py:1322] transformer.encoder.layers.13.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.465 [mindspore/train/serialization.py:1322] transformer.encoder.layers.13.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.515 [mindspore/train/serialization.py:1322] transformer.encoder.layers.13.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.564 [mindspore/train/serialization.py:1322] transformer.encoder.layers.14.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.614 [mindspore/train/serialization.py:1322] transformer.encoder.layers.14.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.663 [mindspore/train/serialization.py:1322] transformer.encoder.layers.14.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.713 [mindspore/train/serialization.py:1322] transformer.encoder.layers.14.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.762 [mindspore/train/serialization.py:1322] transformer.encoder.layers.15.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.811 [mindspore/train/serialization.py:1322] transformer.encoder.layers.15.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.860 [mindspore/train/serialization.py:1322] transformer.encoder.layers.15.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.909 [mindspore/train/serialization.py:1322] transformer.encoder.layers.15.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.302.958 [mindspore/train/serialization.py:1322] transformer.encoder.layers.16.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.007 [mindspore/train/serialization.py:1322] transformer.encoder.layers.16.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.058 [mindspore/train/serialization.py:1322] transformer.encoder.layers.16.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.108 [mindspore/train/serialization.py:1322] transformer.encoder.layers.16.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.158 [mindspore/train/serialization.py:1322] transformer.encoder.layers.17.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.208 [mindspore/train/serialization.py:1322] transformer.encoder.layers.17.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.257 [mindspore/train/serialization.py:1322] transformer.encoder.layers.17.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.306 [mindspore/train/serialization.py:1322] transformer.encoder.layers.17.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.355 [mindspore/train/serialization.py:1322] transformer.encoder.layers.18.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.404 [mindspore/train/serialization.py:1322] transformer.encoder.layers.18.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.452 [mindspore/train/serialization.py:1322] transformer.encoder.layers.18.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.501 [mindspore/train/serialization.py:1322] transformer.encoder.layers.18.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.550 [mindspore/train/serialization.py:1322] transformer.encoder.layers.19.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.599 [mindspore/train/serialization.py:1322] transformer.encoder.layers.19.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.647 [mindspore/train/serialization.py:1322] transformer.encoder.layers.19.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.696 [mindspore/train/serialization.py:1322] transformer.encoder.layers.19.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.746 [mindspore/train/serialization.py:1322] transformer.encoder.layers.20.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.794 [mindspore/train/serialization.py:1322] transformer.encoder.layers.20.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.842 [mindspore/train/serialization.py:1322] transformer.encoder.layers.20.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.892 [mindspore/train/serialization.py:1322] transformer.encoder.layers.20.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.941 [mindspore/train/serialization.py:1322] transformer.encoder.layers.21.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.303.991 [mindspore/train/serialization.py:1322] transformer.encoder.layers.21.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.040 [mindspore/train/serialization.py:1322] transformer.encoder.layers.21.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.089 [mindspore/train/serialization.py:1322] transformer.encoder.layers.21.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.138 [mindspore/train/serialization.py:1322] transformer.encoder.layers.22.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.187 [mindspore/train/serialization.py:1322] transformer.encoder.layers.22.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.236 [mindspore/train/serialization.py:1322] transformer.encoder.layers.22.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.285 [mindspore/train/serialization.py:1322] transformer.encoder.layers.22.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.333 [mindspore/train/serialization.py:1322] transformer.encoder.layers.23.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.382 [mindspore/train/serialization.py:1322] transformer.encoder.layers.23.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.431 [mindspore/train/serialization.py:1322] transformer.encoder.layers.23.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.479 [mindspore/train/serialization.py:1322] transformer.encoder.layers.23.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.528 [mindspore/train/serialization.py:1322] transformer.encoder.layers.24.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.577 [mindspore/train/serialization.py:1322] transformer.encoder.layers.24.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.625 [mindspore/train/serialization.py:1322] transformer.encoder.layers.24.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.673 [mindspore/train/serialization.py:1322] transformer.encoder.layers.24.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.723 [mindspore/train/serialization.py:1322] transformer.encoder.layers.25.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.772 [mindspore/train/serialization.py:1322] transformer.encoder.layers.25.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.820 [mindspore/train/serialization.py:1322] transformer.encoder.layers.25.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.869 [mindspore/train/serialization.py:1322] transformer.encoder.layers.25.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.918 [mindspore/train/serialization.py:1322] transformer.encoder.layers.26.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.304.966 [mindspore/train/serialization.py:1322] transformer.encoder.layers.26.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.305.015 [mindspore/train/serialization.py:1322] transformer.encoder.layers.26.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.305.063 [mindspore/train/serialization.py:1322] transformer.encoder.layers.26.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.305.114 [mindspore/train/serialization.py:1322] transformer.encoder.layers.27.key_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.305.162 [mindspore/train/serialization.py:1322] transformer.encoder.layers.27.value_past is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.305.211 [mindspore/train/serialization.py:1322] transformer.encoder.layers.27.self_attention.query_key_value.mindpet_delta_lora_a is not loaded.\n",
      "[WARNING] ME(131255:281473524027568,MainProcess):2023-11-17-11:03:19.305.260 [mindspore/train/serialization.py:1322] transformer.encoder.layers.27.self_attention.query_key_value.mindpet_delta_lora_b is not loaded.\n",
      "2023-11-17 11:03:19,305 - mindformers[base_model.py:116] - INFO - weights in /cache/glm2_6b.ckpt are loaded\n",
      "[INFO] 2023-11-17 11:03:19,307 [131255] [SDK] : Start to freeze model for delta, mode: lora, include list: None, exclude list: None\n",
      "[INFO] 2023-11-17 11:03:19,307 [131255] [SDK] : Start to freeze model, include list: ['*'], exclude list: ['*mindpet_delta_lora*']\n",
      "[INFO] 2023-11-17 11:03:19,313 [131255] [SDK] : End to freeze model.\n",
      "[INFO] 2023-11-17 11:03:19,313 [131255] [SDK] : End to freeze model for delta.\n",
      "2023-11-17 11:03:20,464 - mindformers[base_trainer.py:464] - INFO - Network Parameters: 1 M.\n",
      "2023-11-17 11:03:20,465 - mindformers[causal_language_modeling.py:186] - INFO - .........Build Compute Metrics For Evaluate..........\n",
      "2023-11-17 11:03:20,465 - mindformers[causal_language_modeling.py:192] - INFO - .........Build tokenizer For Evaluate..........\n",
      "2023-11-17 11:03:20,523 - mindformers[causal_language_modeling.py:196] - INFO - .........Starting Init Evaluate Model..........\n",
      "2023-11-17 11:03:20,524 - mindformers[causal_language_modeling.py:206] - INFO - .........Starting Evaluate Model..........\n",
      "{'auto_trans_ckpt': False,\n",
      " 'auto_tune': False,\n",
      " 'autotune_per_step': 10,\n",
      " 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),\n",
      "               OrderedDict([('type', 'CheckpointMointor'),\n",
      "                            ('prefix', 'glm2-6b-lora'),\n",
      "                            ('save_checkpoint_steps', 1000),\n",
      "                            ('keep_checkpoint_max', 1),\n",
      "                            ('integrated_save', False),\n",
      "                            ('async_save', False)]),\n",
      "               OrderedDict([('type', 'ObsMonitor'), ('keep_last', False)])],\n",
      " 'context': {'device_id': 0,\n",
      "             'device_target': 'Ascend',\n",
      "             'enable_graph_kernel': False,\n",
      "             'graph_kernel_flags': '--disable_expand_ops=Softmax,Dropout '\n",
      "                                   '--enable_parallel_fusion=true '\n",
      "                                   '--reduce_fuse_depth=8 '\n",
      "                                   '--enable_auto_tensor_inplace=true',\n",
      "             'max_call_depth': 10000,\n",
      "             'save_graphs': False},\n",
      " 'device_num': 1,\n",
      " 'do_eval': False,\n",
      " 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor'),\n",
      "                                 ('keep_last', False)])],\n",
      " 'eval_dataset': {'auto_tune': False,\n",
      "                  'autotune_per_step': 10,\n",
      "                  'batch_size': 8,\n",
      "                  'data_loader': {'origin_columns': ['questions', 'answers'],\n",
      "                                  'phase': 'eval',\n",
      "                                  'shuffle': False,\n",
      "                                  'type': 'ADGenDataLoader',\n",
      "                                  'version': 2},\n",
      "                  'device_num': 1,\n",
      "                  'do_eval': True,\n",
      "                  'drop_remainder': True,\n",
      "                  'filepath_prefix': './autotune',\n",
      "                  'ignore_pad_token_for_loss': True,\n",
      "                  'input_columns': ['input_ids', 'labels'],\n",
      "                  'max_source_length': 256,\n",
      "                  'max_target_length': 256,\n",
      "                  'num_parallel_workers': 8,\n",
      "                  'numa_enable': False,\n",
      "                  'prefetch_size': 1,\n",
      "                  'profile': False,\n",
      "                  'python_multiprocessing': False,\n",
      "                  'rank_id': 0,\n",
      "                  'repeat': 1,\n",
      "                  'seed': 0,\n",
      "                  'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}},\n",
      " 'eval_dataset_task': {'dataset_config': {'auto_tune': False,\n",
      "                                          'autotune_per_step': 10,\n",
      "                                          'batch_size': 8,\n",
      "                                          'data_loader': {'origin_columns': ['questions',\n",
      "                                                                             'answers'],\n",
      "                                                          'phase': 'eval',\n",
      "                                                          'shuffle': False,\n",
      "                                                          'type': 'ADGenDataLoader',\n",
      "                                                          'version': 2},\n",
      "                                          'device_num': 1,\n",
      "                                          'do_eval': True,\n",
      "                                          'drop_remainder': True,\n",
      "                                          'filepath_prefix': './autotune',\n",
      "                                          'ignore_pad_token_for_loss': True,\n",
      "                                          'input_columns': ['input_ids',\n",
      "                                                            'labels'],\n",
      "                                          'max_source_length': 256,\n",
      "                                          'max_target_length': 256,\n",
      "                                          'num_parallel_workers': 8,\n",
      "                                          'numa_enable': False,\n",
      "                                          'prefetch_size': 1,\n",
      "                                          'profile': False,\n",
      "                                          'python_multiprocessing': False,\n",
      "                                          'rank_id': 0,\n",
      "                                          'repeat': 1,\n",
      "                                          'seed': 0,\n",
      "                                          'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                                        'vocab_file': '/home/ma-user/work/chatglm2_6b_lora/tokenizer.model'}},\n",
      "                       'type': 'KeyWordGenDataset'},\n",
      " 'eval_epoch_interval': -1,\n",
      " 'eval_step_interval': 500,\n",
      " 'filepath_prefix': './autotune',\n",
      " 'init_start_profile': True,\n",
      " 'layer_decay': 0.65,\n",
      " 'layer_scale': False,\n",
      " 'load_checkpoint': None,\n",
      " 'local_rank': 0,\n",
      " 'lr_scale': False,\n",
      " 'lr_scale_factor': 256,\n",
      " 'lr_schedule': {'learning_rate': 5e-05,\n",
      "                 'lr_end': 1e-06,\n",
      "                 'total_steps': -1,\n",
      "                 'type': 'polynomial',\n",
      "                 'warmup_steps': 0},\n",
      " 'metric': {'type': 'ADGENMetric'},\n",
      " 'micro_batch_interleave_num': 1,\n",
      " 'model': {'arch': {'type': 'ChatGLM2ForConditionalGeneration'},\n",
      "           'model_config': {'add_bias_linear': False,\n",
      "                            'add_qkv_bias': True,\n",
      "                            'apply_query_key_layer_scaling': True,\n",
      "                            'apply_residual_connection_post_layernorm': False,\n",
      "                            'attention_dropout': 0.0,\n",
      "                            'attention_softmax_in_fp32': True,\n",
      "                            'batch_size': 8,\n",
      "                            'bias_dropout_fusion': True,\n",
      "                            'checkpoint_name_or_path': None,\n",
      "                            'compute_dtype': 'float16',\n",
      "                            'do_sample': True,\n",
      "                            'eos_token_id': 2,\n",
      "                            'ffn_hidden_size': 13696,\n",
      "                            'fp32_residual_connection': False,\n",
      "                            'hidden_dropout': 0.0,\n",
      "                            'hidden_size': 4096,\n",
      "                            'kv_channels': 128,\n",
      "                            'layernorm_compute_type': 'float32',\n",
      "                            'layernorm_epsilon': '1e-5',\n",
      "                            'max_decode_length': 256,\n",
      "                            'multi_query_attention': True,\n",
      "                            'multi_query_group_num': 2,\n",
      "                            'num_attention_heads': 32,\n",
      "                            'num_layers': 28,\n",
      "                            'pad_token_id': 0,\n",
      "                            'padded_vocab_size': 65024,\n",
      "                            'param_init_type': 'float16',\n",
      "                            'pet_config': {'lora_alpha': 32,\n",
      "                                           'lora_dropout': 0.1,\n",
      "                                           'lora_rank': 8,\n",
      "                                           'pet_type': 'lora',\n",
      "                                           'target_modules': '.*query_key_value*'},\n",
      "                            'post_layer_norm': True,\n",
      "                            'pre_seq_len': 'None',\n",
      "                            'prefix_projection': False,\n",
      "                            'quantization_bit': 0,\n",
      "                            'repetition_penalty': 1.0,\n",
      "                            'rmsnorm': True,\n",
      "                            'seq_length': 256,\n",
      "                            'top_k': 1,\n",
      "                            'top_p': 1,\n",
      "                            'type': 'ChatGLM2Config',\n",
      "                            'use_past': True}},\n",
      " 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0xffff9c359520>,\n",
      " 'only_save_strategy': False,\n",
      " 'optimizer': {'beta1': 0.9,\n",
      "               'beta2': 0.95,\n",
      "               'eps': 1e-08,\n",
      "               'type': 'FP32StateAdamWeightDecay',\n",
      "               'weight_decay': 0.1},\n",
      " 'output_dir': './output',\n",
      " 'parallel': {'enable_alltoall': False,\n",
      "              'enable_parallel_optimizer': True,\n",
      "              'full_batch': True,\n",
      "              'gradients_mean': False,\n",
      "              'loss_repeated_mean': True,\n",
      "              'parallel_mode': 1,\n",
      "              'search_mode': 'sharding_propagation',\n",
      "              'strategy_ckpt_config': {'only_trainable_params': False,\n",
      "                                       'save_file': './ckpt_strategy.ckpt'},\n",
      "              'strategy_ckpt_save_file': '/cache/ma-user-work/strategy/ckpt_strategy_rank_0.ckpt'},\n",
      " 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0xfffeb8e59820>,\n",
      " 'processor': {'return_tensors': 'ms',\n",
      "               'tokenizer': {'bos_token': '<sop>',\n",
      "                             'end_token': '</s>',\n",
      "                             'eos_token': '<eop>',\n",
      "                             'gmask_token': '[gMASK]',\n",
      "                             'mask_token': '[MASK]',\n",
      "                             'pad_token': '<pad>',\n",
      "                             'type': 'ChatGLM2Tokenizer',\n",
      "                             'unk_token': '<unk>',\n",
      "                             'vocab_file': './checkpoint_download/glm2/tokenizer.model'},\n",
      "               'type': 'GLMProcessor'},\n",
      " 'profile': False,\n",
      " 'profile_communication': True,\n",
      " 'profile_memory': True,\n",
      " 'profile_start_step': 1,\n",
      " 'profile_stop_step': 10,\n",
      " 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0xfffeb8e597c0>,\n",
      " 'remote_save_url': 'obs://lxy-guiyang1-output/glm2-2.2-eval-output',\n",
      " 'resume_training': False,\n",
      " 'run_mode': 'eval',\n",
      " 'runner_config': {'batch_size': 8,\n",
      "                   'epochs': 1,\n",
      "                   'sink_mode': True,\n",
      "                   'sink_size': 4},\n",
      " 'runner_wrapper': {'scale_sense': {'loss_scale_value': 65536,\n",
      "                                    'scale_factor': 2,\n",
      "                                    'scale_window': 1000,\n",
      "                                    'type': 'DynamicLossScaleUpdateCell'},\n",
      "                    'type': 'MFTrainOneStepCell',\n",
      "                    'use_clip_grad': True},\n",
      " 'seed': 0,\n",
      " 'train_dataset': {'auto_tune': False,\n",
      "                   'autotune_per_step': 10,\n",
      "                   'batch_size': 8,\n",
      "                   'data_loader': {'dataset_dir': '/path/to/AdvertiseGen/train.json',\n",
      "                                   'origin_columns': ['content', 'summary'],\n",
      "                                   'phase': 'train',\n",
      "                                   'shuffle': True,\n",
      "                                   'type': 'ADGenDataLoader',\n",
      "                                   'version': 2},\n",
      "                   'do_eval': False,\n",
      "                   'drop_remainder': True,\n",
      "                   'filepath_prefix': './autotune',\n",
      "                   'ignore_pad_token_for_loss': True,\n",
      "                   'input_columns': ['input_ids', 'labels'],\n",
      "                   'max_source_length': 64,\n",
      "                   'max_target_length': 128,\n",
      "                   'num_parallel_workers': 8,\n",
      "                   'numa_enable': False,\n",
      "                   'prefetch_size': 1,\n",
      "                   'profile': False,\n",
      "                   'python_multiprocessing': False,\n",
      "                   'repeat': 1,\n",
      "                   'seed': 0,\n",
      "                   'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                 'vocab_file': '/path/to/tokenizer.model'}},\n",
      " 'train_dataset_task': {'dataset_config': {'auto_tune': False,\n",
      "                                           'autotune_per_step': 10,\n",
      "                                           'batch_size': 8,\n",
      "                                           'data_loader': {'dataset_dir': '/path/to/AdvertiseGen/train.json',\n",
      "                                                           'origin_columns': ['content',\n",
      "                                                                              'summary'],\n",
      "                                                           'phase': 'train',\n",
      "                                                           'shuffle': True,\n",
      "                                                           'type': 'ADGenDataLoader',\n",
      "                                                           'version': 2},\n",
      "                                           'do_eval': False,\n",
      "                                           'drop_remainder': True,\n",
      "                                           'filepath_prefix': './autotune',\n",
      "                                           'ignore_pad_token_for_loss': True,\n",
      "                                           'input_columns': ['input_ids',\n",
      "                                                             'labels'],\n",
      "                                           'max_source_length': 64,\n",
      "                                           'max_target_length': 128,\n",
      "                                           'num_parallel_workers': 8,\n",
      "                                           'numa_enable': False,\n",
      "                                           'prefetch_size': 1,\n",
      "                                           'profile': False,\n",
      "                                           'python_multiprocessing': False,\n",
      "                                           'repeat': 1,\n",
      "                                           'seed': 0,\n",
      "                                           'tokenizer': {'type': 'ChatGLM2Tokenizer',\n",
      "                                                         'vocab_file': '/path/to/tokenizer.model'}},\n",
      "                        'type': 'KeyWordGenDataset'},\n",
      " 'trainer': {'model_name': 'glm2_6b_lora',\n",
      "             'type': 'CausalLanguageModelingTrainer'},\n",
      " 'use_parallel': False}\n",
      "2023-11-17 11:03:20,546 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 11:03:20,547 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 11:03:20,547 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 11:03:20,548 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 11:03:20,548 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2023-11-17 11:03:20,549 - mindformers[base_tokenizer.py:2286] - WARNING - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "[WARNING] DEVICE(131255,ffffa969d0b0,python):2023-11-17-11:04:34.207.883 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_graph_executor.cc:1258] RunInitGraph] Can not find init_subgraph.kernel_graph_0 sub graph, don't need data init subgraph in INFER mode.\n",
      "[WARNING] DEVICE(131255,ffffa969d0b0,python):2023-11-17-11:06:42.815.148 [mindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_graph_executor.cc:1258] RunInitGraph] Can not find init_subgraph.kernel_graph_1 sub graph, don't need data init subgraph in INFER mode.\n",
      "2023-11-17 11:08:03,497 - mindformers[text_generator.py:434] - INFO - total time: 282.92456459999084 s; generated tokens: 1791 tokens; generate speed: 6.330309291214008 tokens/s\n",
      "2023-11-17 11:08:03,501 - mindformers[causal_language_modeling.py:253] - INFO - Step[1/2], cost time 282.9318s, every example cost time is 35.3665, generate speed: 6.3301 tokens/s, avg speed: 0.0000 tokens/s, remaining time: 0:00:00\n",
      "pred is:\n",
      " 颈椎管狭窄症是指颈椎管内径缩小,导致颈椎内容物(如脊髓和神经)受压或刺激而引起的症状,常见的包括颈部疼痛、僵硬、头晕、手臂无力、下肢肌肉无力等。目前尚无彻底治愈颈椎管狭窄症的方法,但是一些辅助治疗可以帮助缓解症状和提高生活质量。以下是一些常见的颈椎管狭窄症辅助治疗措施:\n",
      "\n",
      "1. 物理疗法:包括按摩、牵引、热敷或冷敷、理疗和针灸等治疗方法,可以缓解颈椎管狭窄症的症状,减轻疼痛和僵硬,并促进血液循环和神经恢复。\n",
      "\n",
      "2. 药物治疗:可以使用非甾体抗炎药物(如布洛芬或阿司匹林)缓解疼痛和减轻炎症,还可以使用肌肉松弛剂和神经阻滞剂来减轻肌肉痉挛和疼痛。\n",
      "\n",
      "3. 运动疗法:适当的体育锻炼可以增强颈部肌肉和韧带的稳定性,缓解颈椎管狭窄症的症状,并促进神经恢复。\n",
      "\n",
      "4. 心理治疗:颈椎管狭窄症可能会导致患者焦虑、抑郁和失眠等心理\n",
      " label is:\n",
      " 颈椎定点伸引手法；补阳还五汤；后路减压植入物内固定治疗；加味补阳还五汤；围手术期护理；银质针导热疗法\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.308 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.308 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n",
      "pred is:\n",
      " \n",
      "\n",
      "老年性顽固性心力衰竭是指老年人患有心力衰竭,经治疗后仍难以缓解或复发的情况。针对这种病情,目前推荐使用以下药物:\n",
      "\n",
      "1. 利尿剂:利尿剂可以帮助老年人减少体内的水分和盐分,从而减轻心脏负担,改善心力衰竭的症状。\n",
      "\n",
      "2. ACEI类药物:ACEI类药物可以抑制血管紧张素转化酶,从而降低血压,改善心脏功能。\n",
      "\n",
      "3. ARB类药物:ARB类药物可以抑制血管紧张素转化酶,从而降低血压,改善心脏功能。\n",
      "\n",
      "4. β受体阻滞剂:β受体阻滞剂可以减轻心脏负担,降低心率和血压,改善心力衰竭的症状。\n",
      "\n",
      "5. 醛固酮受体拮抗剂:醛固酮受体拮抗剂可以抑制醛固酮的作用,从而减轻心脏负担,改善心力衰竭的症状。\n",
      "\n",
      "6. 维生素B12和叶酸:维生素B12和叶酸可以帮助老年人改善心脏健康,减少心力衰竭的风险。\n",
      "\n",
      "不过,\n",
      " label is:\n",
      " 左西孟旦；新活素\n",
      "pred is:\n",
      " \n",
      "\n",
      "气血津液是中医理论中的概念,指的是人体内各种液体的组成和功能。气血津液的辅助治疗是指在中医治疗中,通过调节气血津液的平衡和功能,以达到治疗疾病的目的。以下是一些常见的气血津液的辅助治疗方法:\n",
      "\n",
      "1. 调整饮食:饮食对于气血津液的平衡有着重要的影响。中医认为,饮食应该多样化,荤素搭配适当,饮食偏向清淡,少吃辛辣油腻之品。\n",
      "\n",
      "2. 运动锻炼:适当的运动可以增强气血津液的运行,缓解疾病症状,促进身体康复。\n",
      "\n",
      "3. 情绪调节:情绪稳定对于气血津液的平衡有着积极的影响。中医认为,情绪波动会对脏腑功能产生影响,因此应该学会调节情绪,保持心情平和。\n",
      "\n",
      "4. 草药治疗:某些草药具有调节气血津液的功效,可以在中医医生的指导下使用。\n",
      "\n",
      "5. 针灸按摩:针灸按摩可以刺激经络穴位,调节气血津液的运行,缓解疾病症状。\n",
      "\n",
      "6. 推拿按摩:推拿按摩可以刺激肌肉\n",
      " label is:\n",
      " 脾胃调理\n",
      "pred is:\n",
      " 腹腔镜通染术是一种常见的手术技术,通过腹腔镜将导管穿刺到腹腔内进行手术,并通过导管将灌洗液或手术器械送入腹腔进行操作。由于腹腔镜通染术是通过手术操作来进行的,因此术后需要进行影像学检查以确定手术效果和有无并发症。常见的腹腔镜通染术影像学检查包括:\n",
      "\n",
      "1. 腹腔镜下腹部平片:术后立即进行,可以观察是否有活动性出血或脏器脱出等并发症。\n",
      "\n",
      "2. 超声检查:可以观察手术部位有无渗血、渗液以及脏器的位置、形态等变化。\n",
      "\n",
      "3. CT或MRI检查:可以更详细地观察手术部位的解剖结构以及手术对周围脏器的影响。\n",
      "\n",
      "4. 实验室检查:包括血液检查、尿液检查和肝功能检查等,可以观察手术对患者身体的影响以及手术是否成功。\n",
      "\n",
      "5. 腹腔镜检查:在手术后再次进行腹腔镜检查,可以观察手术部位有无感染或其他并发症。\n",
      "\n",
      "不同的影像学\n",
      " label is:\n",
      " 超声造影检查\n",
      "pred is:\n",
      " 细胞瘤的转移部位取决于许多因素,包括原发肿瘤的类型、细胞的类型、肿瘤生长的速度、肿瘤扩散到周围组织或器官的程度以及患者的免疫状态等。\n",
      "\n",
      "常见的细胞瘤转移部位包括:\n",
      "\n",
      "1. 淋巴结:淋巴结是肿瘤最常见的转移部位之一,因为它们是免疫系统中的重要组成部分,容易受到肿瘤细胞的攻击。\n",
      "\n",
      "2. 血管:肿瘤细胞可以通过血液或淋巴系统传播到远处的组织和器官,因此肿瘤细胞转移至血管是常见的现象。\n",
      "\n",
      "3. 肝:肝脏是肿瘤转移的常见部位,尤其是肝癌。\n",
      "\n",
      "4. 肺:肺癌是常见的肿瘤之一,肿瘤细胞可以通过血液或淋巴系统传播到肺部。\n",
      "\n",
      "5. 骨骼:骨骼是肿瘤转移的常见部位,尤其是骨肉瘤。\n",
      "\n",
      "6. 脑:脑部肿瘤的转移相对较少,但仍然可能发生。\n",
      "\n",
      "肿瘤的转移部位不仅取决于肿瘤本身,还取决于患者的身体状况、肿瘤生长的速度和其他因素。\n",
      " label is:\n",
      " 肝\n",
      "pred is:\n",
      " 精囊疾病是指精囊腺体本身发生病变或者引起其功能的障碍所引起的疾病。精囊疾病可能会导致以下结果:\n",
      "\n",
      "1. 腰背痛:精囊位于下腹部,与腰背相邻,因此精囊疾病可能会导致腰背疼痛。\n",
      "\n",
      "2. 尿频:精囊疾病可能会导致尿道括约肌失去功能,引起尿频。\n",
      "\n",
      "3. 尿急:精囊疾病可能会导致尿道括约肌失去功能,引起尿急。\n",
      "\n",
      "4. 尿痛:精囊疾病可能会导致尿道括约肌失去功能,引起尿痛。\n",
      "\n",
      "5. 血尿:精囊疾病可能会导致尿道出血,引起血尿。\n",
      "\n",
      "6. 性功能障碍:精囊疾病可能会影响性器官的正常功能,导致性功能障碍。\n",
      "\n",
      "7. 男性不育:精囊疾病可能会影响精子的产生和输送,导致男性不育。\n",
      "\n",
      "精囊疾病可能会导致一系列不适症状,但这些症状并不一定都会出现,具体情况需要根据疾病类型和个体差异来确定。如果怀疑\n",
      " label is:\n",
      " 血精；男性不育症\n",
      "pred is:\n",
      " 血液感染的高危因素包括以下几个方面:\n",
      "\n",
      "1. 基础疾病:肝病、糖尿病、免疫缺陷等基础疾病会增加感染的风险。\n",
      "\n",
      "2. 免疫抑制:使用免疫抑制剂等药物会降低免疫系统的功能,增加感染的风险。\n",
      "\n",
      "3. 医疗操作:输血、器官移植、手术等医疗操作会增加感染的风险。\n",
      "\n",
      "4. 血液制品:使用血液制品会增加感染的风险。\n",
      "\n",
      "5. 性传播:性接触可以传播血液感染。\n",
      "\n",
      "6. 母婴传播:孕妇在怀孕期间或分娩时感染血液病原体,可能会通过胎盘或乳汁传给胎儿或新生儿。\n",
      "\n",
      "7. 不良卫生习惯:共用牙刷、剃须刀等个人用品,不注意个人卫生等不良卫生习惯会增加感染的风险。\n",
      "\n",
      "8. 感染源:血液感染最常见的感染源是细菌、病毒和真菌等微生物。\n",
      "\n",
      "9. 季节性:有些血液感染的病原体在不同季节的发病率不同,例如疟疾病例在热带和亚热带地区较为常见,而在温带地区\n",
      " label is:\n",
      " 侵入性操作；糖尿病；机械通气\n",
      "pred is:\n",
      " 症状性癫痫综合征是指由特定癫痫类型引起的癫痫发作,其特点是反复发作的癫痫发作,伴随有特定的临床表现。对于症状性癫痫综合征的检查,通常需要进行以下步骤:\n",
      "\n",
      "1. 病史检查:了解患者的病史,包括癫痫发作的类型、频率、持续时间、诱发因素等,以及家族病史等。\n",
      "\n",
      "2. 神经系统检查:包括脑电图、脑部CT、MRI等检查,以排除癫痫发作是由脑部病变引起的。\n",
      "\n",
      "3. 血液检查:检查患者的血液常规、电解质、肝肾功能等,以排除其他疾病引起的癫痫发作。\n",
      "\n",
      "4. 癫痫发作的评估:对于癫痫发作进行评估,包括癫痫发作的类型、持续时间、频率、强度等。\n",
      "\n",
      "5. 病因诊断:通过对病史、神经系统检查和癫痫发作的评估,确定癫痫发作的病因。\n",
      " label is:\n",
      " 脑脊液氨基酸；颅脑CT检查；血常规；血液电解质检查；便常规；颅脑MRI检查\n",
      "2023-11-17 11:08:29,833 - mindformers[text_generator.py:434] - INFO - total time: 24.929969787597656 s; generated tokens: 1418 tokens; generate speed: 56.8793308648708 tokens/s\n",
      "2023-11-17 11:08:29,836 - mindformers[causal_language_modeling.py:253] - INFO - Step[2/2], cost time 24.9369s, every example cost time is 3.1171, generate speed: 56.8635 tokens/s, avg speed: 56.8632 tokens/s, remaining time: 0:00:00\n",
      "pred is:\n",
      " 遗传性良性上皮内角化不良（Heritable Urodysplasia of the Interstitial Skin，HUSI）是一种罕见的遗传性皮肤病，主要表现为皮肤和黏膜的良性上皮内角化。该病的症状包括：\n",
      "\n",
      "1. 皮肤症状：患者通常会出现皮肤干燥、粗糙、厚实、脱屑、瘙痒、烧灼感、干燥性皮疹、皮肤干燥脱屑、皮肤干燥斑疹、皮肤角化过度等。\n",
      "\n",
      "2. 口腔黏膜症状：患者可能会出现口腔干燥、口腔溃疡、口腔炎、牙龈炎等。\n",
      "\n",
      "3. 眼睛症状：患者可能会出现眼睛干燥、痒、疼痛、畏光、异物感等症状。\n",
      "\n",
      "4. 头皮症状：患者可能会出现头皮干燥、头皮屑、头皮瘙痒等症状。\n",
      "\n",
      "5. 其他症状：患者还可能出现关节疼痛、肌肉无力、呼吸困难、心动过缓等。\n",
      "\n",
      "HUSI的症状通常在儿童或青少年时期出现，随着病情发展，症状会逐渐加重。治疗HUS\n",
      " label is:\n",
      " 畏光；视力障碍；唇皱褶增多畸形；结膜充血\n",
      "pred is:\n",
      " 高甲基化是一种表观遗传学现象,即某些基因的甲基化水平发生了变化,导致基因表达和表型发生改变。高甲基化通常发生在某些肿瘤、神经退行性疾病、自身免疫性疾病和某些代谢性疾病中。\n",
      "\n",
      "不过,高甲基化并不一定发生在肿瘤中。在某些情况下,高甲基化可能是一种保护性机制,有助于细胞存活和增殖。例如,在一些情况下,高甲基化可能是一种对抗某些有害刺激的反应,或者是一种对某些病原体具有保护作用的机制。\n",
      "\n",
      "高甲基化发生的位置可能因所涉及的基因和疾病类型而异,因此需要具体问题具体分析。\n",
      " label is:\n",
      " 口腔黏膜；OSCC组织\n",
      "pred is:\n",
      " 根据世界卫生组织的数据,全球超重和肥胖的发病率正在不断上升。根据2019年的数据,全球成人的超重和肥胖发病率约为65%至89%。其中,低收入和中等收入国家的发病率更高,而高收入国家的发病率较低。\n",
      "\n",
      "不过,不同地区、不同人群的超重和肥胖发病率可能有所不同。此外,超重和肥胖的定义也可能会影响发病率的数据。因此,具体的超重发病率数据会因地区、人群和定义的不同而有所差异。\n",
      " label is:\n",
      " 41.97％和18.14％；22.8％和7.1％\n",
      "pred is:\n",
      " 前降支MB-MCA(多排螺旋CT血管成像)是一种常用于评估冠状动脉狭窄严重程度和指导冠状动脉旁路移植术(PCI)的影像学检查方法。以下是前降支MB-MCA的常见检查项目:\n",
      "\n",
      "1. 横断位图像:通过拍摄从头部到脚部的横断图像,可以评估心脏和主动脉的形态和大小,以及冠状动脉的狭窄程度。\n",
      "\n",
      "2. 冠状位图像:通过拍摄心脏的冠状位图像,可以评估冠状动脉的狭窄程度和方向,以及心脏的收缩和舒张功能。\n",
      "\n",
      "3. 搏动显示:通过观察心脏的搏动,可以评估心脏的泵血功能和冠状动脉的血液供应情况。\n",
      "\n",
      "4. 血流的显示:通过观察血液在血管内的流速和方向,可以评估冠状动脉的狭窄程度和血液供应情况。\n",
      "\n",
      "5. 血管造影:通过注射造影剂,可以更清晰地显示冠状动脉的狭窄程度和方向,以及心脏的泵血\n",
      " label is:\n",
      " CT冠状动脉图像\n",
      "pred is:\n",
      " \n",
      "\n",
      "遗忘型精神疾病(也称为遗忘综合征)是一种影响记忆、学习和社会交往的神经心理障碍。对于辅助检查,以下是一些可能会有帮助的检查:\n",
      "\n",
      "1. 脑影像学检查:包括CT、MRI和PET扫描等,可以检测到脑部结构和功能的变化,这些变化可能与遗忘型精神疾病有关。\n",
      "\n",
      "2. 心理测量:包括认知测验、注意力测验、记忆测验和学习测验等,可以评估患者的认知和记忆能力,以及排除其他可能的疾病。\n",
      "\n",
      "3. 精神科检查:包括精神症状问卷、精神分裂症症状评估量表等,可以评估患者的症状和排除其他可能的疾病。\n",
      "\n",
      "4. 血液检查:包括血糖、胆固醇、甲状腺功能等指标,可以评估患者的身体健康状况,并排除其他可能的疾病。\n",
      "\n",
      "5. 神经心理学测试:包括瑞文测验、韦氏记忆测验等,可以评估患者的记忆和学习能力,以及排除其他可能的疾病。\n",
      "\n",
      "6. 行为学检查:包括人格测验、社交技能测验等\n",
      " label is:\n",
      " 听觉词语学习测验\n",
      "pred is:\n",
      " 根据《铁路劳动安全》第12章第11节，白眼爆裂性骨折的多发群体是铁路装卸工。\n",
      " label is:\n",
      " 儿童\n",
      "pred is:\n",
      " 难治性感染性休克是一种严重感染导致的休克状态,常常需要使用特定的药物进行治疗。以下是一些常用的药物,可以用于治疗难治性感染性休克:\n",
      "\n",
      "1. 抗生素:抗生素是治疗感染的基础,可以用于治疗难治性感染性休克。常用的抗生素包括青霉素、头孢菌素、碳青霉烯类抗生素和氨基糖苷类抗生素等。\n",
      "\n",
      "2. 液体疗法:在治疗难治性感染性休克时,液体疗法是非常重要的。液体可以提供身体所需的液体和营养,维持血液循环和氧供。常用的液体包括全血、血浆、红细胞、白细胞和能量液等。\n",
      "\n",
      "3. 糖皮质激素:糖皮质激素可以用于治疗难治性感染性休克,尤其是伴有炎症反应的患者。常用药物包括地塞米松、甲泼尼龙和泼尼松等。\n",
      "\n",
      "4. 免疫球蛋白:免疫球蛋白可以增强机体的免疫力,有助于治疗难治性感染性休克。常用药物包括人血免疫\n",
      " label is:\n",
      " 特利加压素；垂体后叶素\n",
      "pred is:\n",
      " 子宫肌层内膜异位囊肿是指囊肿位于子宫肌层内，与子宫肌层组织混合。这种囊肿可能引起不适症状，如痛经、月经不调等。目前，辅助治疗子宫肌层内膜异位囊肿的方法有多种，包括以下几种：\n",
      "\n",
      "1. 药物治疗：常用的药物包括口服避孕药、抗雄激素药物等。这些药物可以调节激素水平，减轻症状，使囊肿缩小。\n",
      "\n",
      "2. 放置宫内节育器：宫内节育器可以抑制卵巢功能，减少月经量，从而缓解症状。同时，它还可以避免子宫内膜异位囊肿的复发。\n",
      "\n",
      "3. 手术治疗：腹腔镜手术是治疗子宫肌层内膜异位囊肿的常见手术方法。通过腹腔镜观察，可以准确地切除囊肿，并检查有无其他病变。\n",
      "\n",
      "4. 消融治疗：局部加热囊肿，使其破坏，达到治疗效果。这种方法适用于小的、无症状的囊肿。\n",
      "\n",
      "5. 中医治疗：中药治疗子宫\n",
      " label is:\n",
      " 保守性治疗\n",
      "metric: ADGENMetric\n",
      "rouge-1: 3.3990\n",
      "rouge-2: 0.0786\n",
      "rouge-l: 1.4537\n",
      "bleu-4:  0.3770\n",
      "2023-11-17 11:08:29,906 - mindformers[causal_language_modeling.py:267] - INFO - ...........Evaluate Over!...............\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/cloud_utils.py:169: ResourceWarning: unclosed <socket.socket fd=121, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 41522), raddr=('127.0.0.1', 5208)>\n",
      "  ak, sk, session_token = _decrypt_by_sidecar(ak, sk, session_token, modelarts_scc_port)\n",
      "INFO:root:List OBS time cost: 0.71 seconds.\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/src/obs/client.py:172: ResourceWarning: unclosed file <_io.BufferedReader name='/cache/ma-user-work/log/rank_0/error.log'>\n",
      "  ret = func(*args, **kwargs)\n",
      "INFO:root:Copy parallel total time cost: 86.11 seconds.\n",
      "2023-11-17 11:10:00,183 - mindformers[cloud_adapter.py:195] - INFO - Pull/Push file obs://lxy-guiyang1-output/glm2-2.2-eval-output success, cost time: 86.107295\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/run_mindformer.py\", line 340, in <module>\n",
      "    main(config_)\n",
      "  File \"/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/mindformers/tools/cloud_adapter/cloud_monitor.py\", line 45, in wrapper\n",
      "    _last_transform(local_id, log)\n",
      "  File \"/home/ma-user/work/chatglm2_6b_lora/mindformers/scripts/mf_standalone/mindformers/tools/cloud_adapter/cloud_monitor.py\", line 72, in _last_transform\n",
      "    os.mknod(LAST_TRANSFORM_LOCK_PATH)\n",
      "FileExistsError: [Errno 17] File exists\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=161, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 36536), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=180, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34630), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=182, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34626), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=160, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34620), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=178, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39244), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=156, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34600), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=181, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34628), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=177, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 36546), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=165, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 36538), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=172, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39240), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=153, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55660), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=159, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 38654), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=163, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46302), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=130, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57510), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=145, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57558), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=176, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46300), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=158, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 46298), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=149, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35820), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=148, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57560), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=157, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 55662), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=143, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35290), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=142, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35292), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=155, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 38650), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=162, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 34606), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=185, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 38656), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=144, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57512), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=150, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 36530), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=137, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 56326), raddr=('100.125.7.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=146, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 39132), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=139, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 57556), raddr=('100.125.6.3', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=136, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 35294), raddr=('100.125.6.131', 443)>\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=122, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.16.0.75', 42966), raddr=('100.125.7.3', 443)>\n"
     ]
    }
   ],
   "source": [
    "!python run_mindformer.py --config=configs/glm2/run_glm2_6b_lora_eval.yaml --use_parallel=False --run_mode=\"eval\" --device_id=0 --load_checkpoint=/cache/glm2_6b.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f79e6-02e5-4222-9087-3e974b41c411",
   "metadata": {},
   "source": [
    "# 模型转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5a0ec-2493-4e62-8367-988f27d35a47",
   "metadata": {},
   "source": [
    "模型效果满足要求，需转换为mindir格式进行推理，进入模型转换工具包路径，准备模型转换环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28d799af-752f-43c4-ae43-72eeca2360b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/chatglm2_6b_lora/mindspore_lite_devserver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd $chatglm2_6b_lora_path/mindspore_lite_devserver/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4cecd3-d716-4583-9d36-1ea46a141d6f",
   "metadata": {},
   "source": [
    "安装模型转换依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42709d22-111a-49d4-ad83-868e481f6e3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Collecting torch==1.11.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/torch/1.11.0/torch-1.11.0-cp39-cp39-manylinux2014_aarch64.whl (50.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.9 MB 52.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/transformers/4.35.2/transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 59.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.97 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 4)) (0.1.99)\n",
      "Requirement already satisfied: ftfy>=6.1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 5)) (6.1.1)\n",
      "Requirement already satisfied: regex>=2022.10.31 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 6)) (2023.10.3)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 7)) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 8)) (6.0.1)\n",
      "Requirement already satisfied: jieba>=0.42.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 9)) (0.42.1)\n",
      "Requirement already satisfied: rouge_chinese>=1.0.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 10)) (1.0.3)\n",
      "Requirement already satisfied: nltk>=2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 11)) (3.8.1)\n",
      "Requirement already satisfied: mindpet==1.0.2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 12)) (1.0.2)\n",
      "Requirement already satisfied: pydantic==1.10.11 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 13)) (1.10.11)\n",
      "Requirement already satisfied: mdtex2html in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 14)) (1.2.0)\n",
      "Requirement already satisfied: gradio in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 15)) (3.50.2)\n",
      "Requirement already satisfied: opencv-python in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from -r mindformers_requirements.txt (line 16)) (4.8.0.76)\n",
      "Collecting werkzeug==2.2.2\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/werkzeug/2.2.2/Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 72.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindpet==1.0.2->-r mindformers_requirements.txt (line 12)) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pydantic==1.10.11->-r mindformers_requirements.txt (line 13)) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from werkzeug==2.2.2->-r mindformers_requirements.txt (line 17)) (2.1.3)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from ftfy>=6.1.1->-r mindformers_requirements.txt (line 5)) (0.2.9)\n",
      "Requirement already satisfied: joblib in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from nltk>=2.0->-r mindformers_requirements.txt (line 11)) (1.3.2)\n",
      "Requirement already satisfied: six in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from rouge_chinese>=1.0.3->-r mindformers_requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (23.2.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (0.18.0)\n",
      "Requirement already satisfied: fastapi in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (0.104.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (3.5.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (10.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (2.27.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (5.1.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (6.1.1)\n",
      "Requirement already satisfied: packaging in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (23.2)\n",
      "Requirement already satisfied: httpx in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (0.25.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (1.2.5)\n",
      "Requirement already satisfied: pydub in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (0.25.1)\n",
      "Requirement already satisfied: gradio-client==0.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (0.6.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (0.24.0.post1)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (3.9.10)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (11.0.3)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (3.1.2)\n",
      "Requirement already satisfied: ffmpy in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (0.3.1)\n",
      "Requirement already satisfied: python-multipart in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio->-r mindformers_requirements.txt (line 15)) (2.10.0)\n",
      "Requirement already satisfied: fsspec in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from gradio-client==0.6.1->gradio->-r mindformers_requirements.txt (line 15)) (2023.10.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->-r mindformers_requirements.txt (line 15)) (4.19.2)\n",
      "Requirement already satisfied: toolz in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->-r mindformers_requirements.txt (line 15)) (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio->-r mindformers_requirements.txt (line 15)) (3.13.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio->-r mindformers_requirements.txt (line 15)) (3.17.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r mindformers_requirements.txt (line 15)) (0.30.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r mindformers_requirements.txt (line 15)) (23.1.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r mindformers_requirements.txt (line 15)) (0.12.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r mindformers_requirements.txt (line 15)) (2023.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r mindformers_requirements.txt (line 15)) (3.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r mindformers_requirements.txt (line 15)) (4.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r mindformers_requirements.txt (line 15)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r mindformers_requirements.txt (line 15)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->-r mindformers_requirements.txt (line 15)) (1.4.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->-r mindformers_requirements.txt (line 15)) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests~=2.0->gradio->-r mindformers_requirements.txt (line 15)) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests~=2.0->gradio->-r mindformers_requirements.txt (line 15)) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests~=2.0->gradio->-r mindformers_requirements.txt (line 15)) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests~=2.0->gradio->-r mindformers_requirements.txt (line 15)) (2.10)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from uvicorn>=0.14.0->gradio->-r mindformers_requirements.txt (line 15)) (0.14.0)\n",
      "Requirement already satisfied: markdown in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mdtex2html->-r mindformers_requirements.txt (line 14)) (3.5.1)\n",
      "Requirement already satisfied: latex2mathml in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mdtex2html->-r mindformers_requirements.txt (line 14)) (3.76.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/safetensors/0.4.0/safetensors-0.4.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 53.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/tokenizers/0.15.0/tokenizers-0.15.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 47.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from fastapi->gradio->-r mindformers_requirements.txt (line 15)) (0.27.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from fastapi->gradio->-r mindformers_requirements.txt (line 15)) (3.7.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio->-r mindformers_requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio->-r mindformers_requirements.txt (line 15)) (1.1.3)\n",
      "Requirement already satisfied: httpcore in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from httpx->gradio->-r mindformers_requirements.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from markdown->mdtex2html->-r mindformers_requirements.txt (line 14)) (6.8.0)\n",
      "Installing collected packages: tokenizers, safetensors, werkzeug, transformers, torch\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: werkzeug 3.0.1\n",
      "    Uninstalling werkzeug-3.0.1:\n",
      "      Successfully uninstalled werkzeug-3.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mindinsight 2.2.0 requires protobuf<=3.20.1,>=3.13.0, but you have protobuf 3.20.2 which is incompatible.\n",
      "flask 2.3.3 requires Werkzeug>=2.3.7, but you have werkzeug 2.2.2 which is incompatible.\u001b[0m\n",
      "Successfully installed safetensors-0.4.0 tokenizers-0.15.0 torch-1.11.0 transformers-4.35.2 werkzeug-2.2.2\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r mindformers_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834724ac-7b30-4b38-834a-2ca097e8f4b4",
   "metadata": {},
   "source": [
    "我们将微调后的模型转换到`/cache/mindir`目录下，注意修改微调模型路径为实际路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60efd075-0070-41d8-af15-d3686c003c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mindir_path = \"/home/ma-user/work/mindir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be862fcf-9afa-4702-b9f1-e3073972fdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/cache/ma-user-work/checkpoint/rank_0/glm2-6b-lora_rank_0-937_4.ckpt': No such file or directory\n",
      "2023-11-17 11:15:18,718 - INFO - tokenizer save to path: /cache/mindir/tokenizer\n",
      "2023-11-17 11:15:18,718 - INFO - convert finish, start export mindir, it takes about 5~10 minutes\n",
      "2023-11-17 11:21:34,349 - INFO - mindir export success, output dir: /cache/mindir\n"
     ]
    }
   ],
   "source": [
    "!python ckpt_to_mindir.py --ckpt_path=$finetuning_model_path --tokenize_model_path=$chatglm2_6b_lora_path/tokenizer.model --output_dir=$mindir_path --model_name=glm2 --train_mode=origin --size=6 --seq_length=2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5ee16-0876-40d1-b59f-fae2b319c8ed",
   "metadata": {},
   "source": [
    "看到`mindir export success`日志说明模型已经导出到`$mindir_path`路径下。如报错，可以查看`$mindir_path/logs/export_mindir.log`路径下的日志确定原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be05b8-eff5-41cf-b565-0aaf2c16cad5",
   "metadata": {},
   "source": [
    "# AI应用开发与验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d4e57-e945-4333-87c5-dd4b58331926",
   "metadata": {},
   "source": [
    "最终的交付件为AI应用，AI应用规范可以查阅[帮助文档](https://support.huaweicloud.com/inference-modelarts/inference-modelarts-0008.html)，我们提供了样例模板，可以在模板上进行个性化修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5570549-6542-4a8d-ac7f-22ded4d00ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载AI应用模板路径\n",
    "chatglm2_model_path = \"/home/ma-user/work/chatglm2_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730d2d7f-61d5-4735-a26c-7c6cd02f8bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v2.2.3.2c7f2141-2c7f2141\n",
      "INFO:root:Using OBS-Python-SDK-3.20.9.1\n",
      "INFO:root:Using OBS-C-SDK-2.23.1\n",
      "INFO:root:An exception occurred in function copy_parallel_c: /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/moxing/framework/file/csrc/obs/lib/aarch64/libobs_cxx.so: cannot open shared object file: No such file or directory\n",
      "INFO:root:load shared library failed, retry with copy parallel python\n",
      "INFO:root:Multiprocessing connection patch for bpo-17560 not applied, not an applicable Python version: 3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:53:27) \n",
      "[GCC 9.4.0]\n",
      "INFO:root:List OBS time cost: 0.81 seconds.\n",
      "INFO:root:Copy parallel total time cost: 1.35 seconds.\n"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "\n",
    "mox.file.copy_parallel(\"obs://dtse-models-guiyang1/competition/glm2/chatglm2_model/\", chatglm2_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760d80b-6ac1-489f-aae2-119bc97fbf78",
   "metadata": {},
   "source": [
    "由于我们上线的AI应用采用的是mindpet 1.0.1版本，因此我们需要将mindpet降为对应版本，**如后续需要再次运行微调步骤**，则需在运行前运行 `!pip install mindpet==1.0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247519b5-4223-4f7d-a410-f26df436018c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Requirement already satisfied: mindpet==1.0.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: click in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindpet==1.0.1) (8.1.7)\n",
      "Requirement already satisfied: pyyaml in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindpet==1.0.1) (6.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mindpet==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f360e-e9e8-4a67-96a7-b553a1681d2d",
   "metadata": {},
   "source": [
    "进入工作路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fcbc1f-ed72-449e-b507-18f16e2cc4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/chatglm2_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd $chatglm2_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509cb7d-1fce-432a-b839-6d795ea94575",
   "metadata": {},
   "source": [
    "可以先在notebook中调试prompt_template与推理参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d920ae93-a6e1-4a3f-b841-ff2335074150",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:30:52,060 - mindformers - INFO - config in the yaml file /home/ma-user/work/chatglm2_model/checkpoint_download/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-11-17 15:30:52,107 - mindformers - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-11-17 15:30:52,108 - mindformers - INFO - build tokenizer class name is: ChatGLM2Tokenizer using args {'bos_token': '<sop>', 'eos_token': '<eop>', 'end_token': '</s>', 'mask_token': '[MASK]', 'gmask_token': '[gMASK]', 'pad_token': '<pad>', 'unk_token': '<unk>', 'vocab_file': '/home/ma-user/work/chatglm2_model/checkpoint_download/tokenizer.model'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:mslite ascendc custom kernel path not found\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "WARNING:root:mslite ascendc custom kernel path not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['helloHM\\n\\n0000000000000000000000000000 at should do. should use.;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n;;;;;;;;...................................................']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from mindformers import ChatGLM2Tokenizer\n",
    "from mindformers.infer.infers import InferTask\n",
    "from mindformers.infer.infer_config import InferConfig\n",
    "from model_service.model_service import SingleNodeService\n",
    "\n",
    "# 基础镜像环境变量配置不支持推理，移除相关配置\n",
    "os.unsetenv('MS_ENABLE_GE')\n",
    "os.unsetenv('MS_GE_TRAIN')\n",
    "\n",
    "\n",
    "# 为了方便部署AI应用，我们在模型转换时将权重文件进行了切分，在实际推理时，需要先将权重合并\n",
    "\n",
    "def cat_weight_file(mindir_dir: str):\n",
    "    for filename in os.listdir(mindir_dir):\n",
    "        if '_variables' in filename:\n",
    "            weight_dir = os.path.join(mindir_dir, filename)\n",
    "            if len(list(weight_dir)) > 1 and not os.path.exists(f\"{weight_dir}/data_0\"):\n",
    "                os.system(f\"cat {weight_dir}/data_0_* > {weight_dir}/data_0\")\n",
    "\n",
    "root = mindir_path\n",
    "cat_weight_file(os.path.join(root, 'mindir'))\n",
    "prefill_model_path = os.path.join(root, 'mindir', 'chatglm2_6b_seq2048_bs1_full_graph.mindir')\n",
    "increment_model_path = os.path.join(root, 'mindir', 'chatglm2_6b_seq2048_bs1_inc_graph.mindir')\n",
    "ge_config_path = os.path.join(root, 'config.ini')\n",
    "tokenizer_path = os.path.join(chatglm2_model_path, 'checkpoint_download')\n",
    "\n",
    "lite_config = InferConfig(\n",
    "            prefill_model_path=prefill_model_path,\n",
    "            increment_model_path=increment_model_path,\n",
    "            model_type=\"mindir\",\n",
    "            model_name=\"glm2\",\n",
    "            ge_config_path=ge_config_path,\n",
    "            device_id=0,\n",
    "            infer_seq_length=2048,\n",
    "        )\n",
    "\n",
    "# 模型加载与热备会耗时10-15min\n",
    "tokenizer = ChatGLM2Tokenizer.from_pretrained(tokenizer_path)\n",
    "infer_model = InferTask.get_infer_task(\"text_generation\", lite_config, tokenizer=tokenizer)\n",
    "infer_model.infer('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7049728a-82e5-4688-80e1-e009a544dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 prompt_template 构建输入，此步并不是必须的，也可以使用Jinja2等其他方法渲染模板，Baseline只是提供一种样例。\n",
    "\n",
    "prompt_template = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\n",
    "\n",
    "def build_prompt(text, i=1):\n",
    "    return prompt_template.format(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8147d577-13dd-4b02-a8ce-1137874534cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给定测试 prompt, 最好与实际任务贴切，方便调试效果\n",
    "\n",
    "prompt = \"下肢血管狭窄的影像学检查有什么？ \\n选项：\\nA.直接胆胰管造影 \\nB.彩色多普勒超声 \\nC.支气管镜检查 \\nD.CT检查\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abc12a32-2483-4e96-bb3a-f059fcd96795",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【只需要给出对应选项，不需要给出原因】\n",
      "[Round 1]\n",
      "\n",
      "问：下肢血管狭窄的影像学检查有什么？ \n",
      "选项：\n",
      "A.直接胆胰管造影 \n",
      "B.彩色多普勒超声 \n",
      "C.支气管镜检查 \n",
      "D.CT检查\n",
      "\n",
      "答： CT检查\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(prompt)\n",
    "input_ids = infer_model.preprocess(prompt, add_special_tokens=True)\n",
    "input_token_length = len(input_ids[0])\n",
    "if input_token_length > 2048:\n",
    "    response = 'inputs are too long, please retry.'\n",
    "else:\n",
    "    output_ids, logits = infer_model.generate(input_ids, do_sample=True, top_k=1,\n",
    "                                      repetition_penalty=1, eos_token_id=2, pad_token_id=0,\n",
    "                                      is_sample_acceleration=False, streamer=None,\n",
    "                                      top_p=1, temperature=1, max_length=2048, return_logits=True)\n",
    "    outputs = infer_model.postprocess(output_ids)\n",
    "    response = outputs[0]\n",
    "\n",
    "# 查看解码后的输出结果\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2faf8-d5f4-4044-8318-ce62ac12fdec",
   "metadata": {},
   "source": [
    "我们要求AI应用返回值只包含选项如：A，选手可以通过构建特定形式的微调数据、调整prompt_template、添加后处理等方式规范输出，下面提供一种处理思路："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a45fd12d-43d3-4c72-b6c9-f9f451c1d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "choice_tokens = [tokenizer.encode(choice, add_special_tokens=False)[0] for choice in choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9eea7da-1f77-43ee-9976-68a188aa2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompt = f'【只需要给出对应选项，不需要给出原因】'\n",
    "prompt_template = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\n",
    "\n",
    "def build_prompt(text, i=1):\n",
    "    return extraction_prompt + '\\n' + prompt_template.format(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ff75da9-fd53-491b-8abe-8d1fe49478e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【只需要给出对应选项，不需要给出原因】\n",
      "[Round 1]\n",
      "\n",
      "问：下肢血管狭窄的影像学检查有什么？ \n",
      "选项：\n",
      "A.直接胆胰管造影 \n",
      "B.彩色多普勒超声 \n",
      "C.支气管镜检查 \n",
      "D.CT检查\n",
      "\n",
      "答：\n"
     ]
    }
   ],
   "source": [
    "prompt = \"下肢血管狭窄的影像学检查有什么？ \\n选项：\\nA.直接胆胰管造影 \\nB.彩色多普勒超声 \\nC.支气管镜检查 \\nD.CT检查\"\n",
    "prompt = build_prompt(prompt)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c3b4ee1-c289-4d93-a1e5-67db1a8603ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = infer_model.preprocess(prompt, add_special_tokens=True)\n",
    "output_ids, logits = infer_model.generate(input_ids, do_sample=True, top_k=5,\n",
    "                                          repetition_penalty=1, eos_token_id=2, pad_token_id=0,\n",
    "                                          is_sample_acceleration=False, streamer=None,\n",
    "                                          top_p=1, temperature=1, max_length=2048, return_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d304127-c733-407e-a1bc-af3845f96f0f",
   "metadata": {},
   "source": [
    "读取指定位置logits在choice_tokens上的取值，较大的一个作为选项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d0a9641-5546-44f9-98bc-670cbfdccbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "logits = logits[0][:, choice_tokens]\n",
    "preds = logits.argmax(axis=-1)\n",
    "print(choices[preds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4206c3f7-66ae-4577-98cc-74ff6798b358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【只需要给出对应选项，不需要给出原因】\n",
      "[Round 1]\n",
      "\n",
      "问：下肢血管狭窄的影像学检查有什么？ \n",
      "选项：\n",
      "A.直接胆胰管造影 \n",
      "B.彩色多普勒超声 \n",
      "C.支气管镜检查 \n",
      "D.CT检查\n",
      "\n",
      "答： CT\n"
     ]
    }
   ],
   "source": [
    "outputs = infer_model.postprocess(output_ids)\n",
    "response = outputs[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030a7e6-beeb-44f0-834b-543baf85c124",
   "metadata": {},
   "source": [
    "# 构建AI应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60d5e6-a0de-4cae-bb86-1a49381b82ad",
   "metadata": {},
   "source": [
    "调试好prompt_template与推理逻辑后，我们可以修改AI应用推理代码`$chatglm2_model_path/customize_service.py`，同时，也可以在Notebook中进行调试，当然调试并不是必须的。我们可以查看推理代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09c4f40a-1779-4552-a373-4af533ae5e7d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mmindformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatGLM2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mmindformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferTask\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mmindformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferConfig\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mmodel_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_service\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleNodeService\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;31m# 基础镜像环境变量配置不支持推理，移除相关配置\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MS_ENABLE_GE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MS_GE_TRAIN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mcat_weight_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmindir_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmindir_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m'_variables'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mweight_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmindir_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{weight_dir}/data_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cat {weight_dir}/data_0_* > {weight_dir}/data_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mLlmService\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSingleNodeService\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# 获取程序当前运行路径，即model文件夹所在的路径\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcat_weight_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mindir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefill_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mindir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chatglm2_6b_seq2048_bs1_full_graph.mindir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mindir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chatglm2_6b_seq2048_bs1_inc_graph.mindir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge_config_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'config.ini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint_download'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraction_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'【只需要给出对应选项，不需要给出原因】\\n'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[Round {}]\\n\\n问：{}\\n\\n答：\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# 非阻塞方式加载模型，防止阻塞超时\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_warm_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m     \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mhealth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload_and_warm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load and warm up start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mlite_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInferConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mprefill_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefill_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mincrement_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mindir\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"glm2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mge_config_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge_config_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdevice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minfer_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatGLM2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInferTask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_infer_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlite_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# warm up\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hello'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load and warm up end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_content\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prompt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'top_p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mchoice_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquery_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquery_token_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraction_prompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraction_prompt\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mold_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mprompt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mprompt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minput_token_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0minput_token_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inputs are too long, cut.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0moutput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mis_sample_acceleration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mreturn_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpred_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtotal_token_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'模型编译未完成，请等待约10至20分钟'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minput_token_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtotal_token_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquery_token_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpred_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minference_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcreated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minference_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcreated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"choice\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_choice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"prompt_tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery_token_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"total_tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtotal_token_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"response\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minference_result\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pycat $chatglm2_model_path/customize_service.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8e4bd-a9ff-4c91-a699-0bb3e93cc47b",
   "metadata": {},
   "source": [
    "拷贝代码到工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "045dae5e-cbb4-4b3e-8056-5a54b67e4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -rf $chatglm2_model_path/* /home/ma-user/work/mindir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e081ee-5311-4705-950f-e7a185e4c6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cache/mindir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 60\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:55 \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
      "-rw-r-----  1 ma-user  287 Nov 17 15:18 args.json\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:55 \u001b[01;34mcheckpoint_download\u001b[0m/\n",
      "-rw-r-----  1 ma-user  118 Nov 17 15:55 config.ini\n",
      "-rw-r-----  1 ma-user 1613 Nov 17 15:55 config.json\n",
      "-rw-r-----  1 ma-user 5783 Nov 17 15:55 customize_service.py\n",
      "drwxr-x---  3 ma-user 4096 Nov 17 15:55 \u001b[01;34mkernel_meta_11263499095870700066\u001b[0m/\n",
      "drwxr-x---  3 ma-user 4096 Nov 17 15:55 \u001b[01;34mkernel_meta_14679507254913862661\u001b[0m/\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:55 \u001b[01;34mkernel_meta_temp_14205105289487976846\u001b[0m/\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:55 \u001b[01;34mkernel_meta_temp_3881696564625442621\u001b[0m/\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:18 \u001b[01;34mlogs\u001b[0m/\n",
      "drwxr-x--- 14 ma-user 4096 Nov 17 15:55 \u001b[01;34mmindformers\u001b[0m/\n",
      "drwxr-x---  4 ma-user 4096 Nov 17 15:18 \u001b[01;34mmindir\u001b[0m/\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:18 \u001b[01;34mtokenizer\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd /cache/mindir/\n",
    "%ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11892a0-85bd-46fc-9136-960591c19eea",
   "metadata": {},
   "source": [
    "初始化推理实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5de0b8-d6c8-448a-b21a-5cb9625e45b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load and warm up start\n",
      "2023-11-17 15:56:57,378 - mindformers - INFO - config in the yaml file /cache/mindir/checkpoint_download/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-11-17 15:56:57,423 - mindformers - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-11-17 15:56:57,424 - mindformers - INFO - build tokenizer class name is: ChatGLM2Tokenizer using args {'bos_token': '<sop>', 'eos_token': '<eop>', 'end_token': '</s>', 'mask_token': '[MASK]', 'gmask_token': '[gMASK]', 'pad_token': '<pad>', 'unk_token': '<unk>', 'vocab_file': '/cache/mindir/checkpoint_download/tokenizer.model'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:mslite ascendc custom kernel path not found\n"
     ]
    }
   ],
   "source": [
    "from customize_service import LlmService\n",
    "\n",
    "llm = LlmService(\"/cache/mindir/\", \"/cache/mindir/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81facb-ce92-4429-85fc-910d5a6ae3f7",
   "metadata": {},
   "source": [
    "由于采用非阻塞的方式加载模型与热备，因此初始化实例后并不代表模型已经加载完成，可以通过health()函数判断，若热备完成，返回200，否则，返回404。热备同样需要10~15min。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d34c07b-778f-420c-952d-b820cc1706c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 200)\n"
     ]
    }
   ],
   "source": [
    "print(llm.health())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d3190-1205-4424-ada9-0f015c269051",
   "metadata": {},
   "source": [
    "等待模型热备完成，可以准备数据进行推理测试，由于我们的判分采用文件进行推理，在测试时我们可以模仿BytesIO输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6b78afe-c1a9-498b-935b-889dd235ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_prompt = {\n",
    "    \"prompt\": \"请问下面描述的是什么类型的临床实验筛选标准？\\n试验前3个月每日吸烟量多于5支者或使用其他含有等量尼古丁的烟草制品。\\n选项：\\nA.实验室检查 \\nB.受体状态 \\nC.特殊病人特征 \\nD.吸烟状况\",\n",
    "    \"choices\": [\"A\", \"B\", \"C\", \"D\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edc83bd5-04af-41de-969e-5961e022fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "\n",
    "post_data = {\"file_name\":{\"input_txt\": io.BytesIO(json.dumps(post_prompt).encode())}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9ce58-3e57-4068-be1f-bf8388ca3d97",
   "metadata": {},
   "source": [
    "**预处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "684191ae-3a7c-4b05-9fab-a312d5c6c260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '请问下面描述的是什么类型的临床实验筛选标准？\\n试验前3个月每日吸烟量多于5支者或使用其他含有等量尼古丁的烟草制品。\\n选项：\\nA.实验室检查 \\nB.受体状态 \\nC.特殊病人特征 \\nD.吸烟状况',\n",
       " 'choices': ['A', 'B', 'C', 'D']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = llm._preprocess(post_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527b265-293c-4334-b581-61d4795958fd",
   "metadata": {},
   "source": [
    "**推理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d2c2aff-2187-4f4a-9c25-47e82ec223e3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': {'id': '6fa06e75-322c-494f-a4b1-36fb245ec7bd',\n",
       "  'created': 1700209120,\n",
       "  'choice': 'D',\n",
       "  'prompt_tokens': 66,\n",
       "  'total_tokens': 95,\n",
       "  'history': [('请问下面描述的是什么类型的临床实验筛选标准？\\n试验前3个月每日吸烟量多于5支者或使用其他含有等量尼古丁的烟草制品。\\n选项：\\nA.实验室检查 \\nB.受体状态 \\nC.特殊病人特征 \\nD.吸烟状况',\n",
       "    '【只需要给出对应选项，不需要给出原因】\\n[Round 1]\\n\\n问：请问下面描述的是什么类型的临床实验筛选标准？\\n试验前3个月每日吸烟量多于5支者或使用其他含有等量尼古丁的烟草制品。\\n选项：\\nA.实验室检查 \\nB.受体状态 \\nC.特殊病人特征 \\nD.吸烟状况\\n\\n答： D.吸烟状况')],\n",
       "  'response': '【只需要给出对应选项，不需要给出原因】\\n[Round 1]\\n\\n问：请问下面描述的是什么类型的临床实验筛选标准？\\n试验前3个月每日吸烟量多于5支者或使用其他含有等量尼古丁的烟草制品。\\n选项：\\nA.实验室检查 \\nB.受体状态 \\nC.特殊病人特征 \\nD.吸烟状况\\n\\n答： D.吸烟状况'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = llm._inference(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb3071-03b0-4cf3-b684-70a71e7ea5d1",
   "metadata": {},
   "source": [
    "调试好应用推理代码，最后，我们删除一些构建AI应用时不需要的文件，将调试好的AI应用保存到obs中，推理时合并的文件不需要拷贝，之后就可以在ModelArts控制台中创建AI应用，具体方法可以参考指导手册。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b70724b2-c1f8-4d43-9f83-3d87c884b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf __pycache__/ kernel_meta_* mindir/chatglm2_6b_seq2048_bs1_full_variables/data_0_* mindir/chatglm2_6b_seq2048_bs1_inc_variables/data_0_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0da35e23-600b-48d4-9213-d31b1c487d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "-rw-r-----  1 ma-user  287 Nov 17 15:18 args.json\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:55 \u001b[0m\u001b[01;34mcheckpoint_download\u001b[0m/\n",
      "-rw-r-----  1 ma-user  118 Nov 17 15:55 config.ini\n",
      "-rw-r-----  1 ma-user 1613 Nov 17 15:55 config.json\n",
      "-rw-r-----  1 ma-user 5783 Nov 17 15:55 customize_service.py\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:18 \u001b[01;34mlogs\u001b[0m/\n",
      "drwxr-x--- 14 ma-user 4096 Nov 17 15:55 \u001b[01;34mmindformers\u001b[0m/\n",
      "drwxr-x---  4 ma-user 4096 Nov 17 15:18 \u001b[01;34mmindir\u001b[0m/\n",
      "drwxr-x---  3 ma-user 4096 Nov 17 15:56 \u001b[01;34moutput\u001b[0m/\n",
      "drwxr-x---  2 ma-user 4096 Nov 17 15:18 \u001b[01;34mtokenizer\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97351255-86a5-43de-bc26-ede8f990f7b6",
   "metadata": {},
   "source": [
    "将文件夹保存到obs桶中，记得修改路径为实际路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73389609-ac6a-4557-8d51-a724855ee070",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'moxing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20900\\1870870359.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmoxing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/consonnm/Desktop/华为云百模千态\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"obs://cheems-dog/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'moxing'"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "\n",
    "mox.file.copy_parallel(\"/home/ma-user/work/mindir\", \"obs://zzaa/model_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84da255f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 驱动器 C 中的卷是 Windows-SSD\n",
      " 卷的序列号是 C828-3D4D\n",
      "\n",
      " C:\\Users\\consonnm\\Desktop\\华为云百模千态 的目录\n",
      "\n",
      "2023/11/28  15:47    <DIR>          .\n",
      "2023/11/28  14:23    <DIR>          ..\n",
      "2023/11/28  15:47    <DIR>          .ipynb_checkpoints\n",
      "2023/11/27  17:31               612 a.jsonl\n",
      "2023/11/27  17:55             5,783 customize_service.py\n",
      "2023/11/26  15:23           870,949 glm2-baseline-v1(1).ipynb\n",
      "2023/11/25  21:27             7,455 train.jsonl\n",
      "2023/11/26  16:04            12,916 配置文件.md\n",
      "               5 个文件        897,715 字节\n",
      "               3 个目录 51,451,584,512 可用字节\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2ee27-a13d-4bf3-8626-8cb22a4549b8",
   "metadata": {},
   "source": [
    "接下来，按照指导手册的介绍构建AI应用即可。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
